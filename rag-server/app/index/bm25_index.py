from typing import List, Dict, Any, Optional, Union
from llama_index.retrievers.bm25 import BM25Retriever
from llama_index.core.schema import TextNode, NodeWithScore
from llama_index.core import Document
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import re
import json
import pickle
from pathlib import Path
import uuid
import logging
from datetime import datetime

from .base_index import BaseIndex, IndexedDocument
from app.retriever.document_builder import EnhancedDocument

logger = logging.getLogger(__name__)

# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ ì‹¤í–‰ ì‹œ)
def _download_nltk_data():
    """NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ë©±ë“±ì„± ë³´ì¥)"""
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        logger.info("NLTK punkt í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ ì¤‘...")
        nltk.download('punkt', quiet=True)

    try:
        nltk.data.find('corpora/stopwords')
    except LookupError:
        logger.info("NLTK stopwords ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...")
        nltk.download('stopwords', quiet=True)

# ì´ˆê¸° ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
_download_nltk_data()


class CodeTokenizer:
    """ì½”ë“œ íŠ¹í™” í† í¬ë‚˜ì´ì €"""
    
    def __init__(self, language: str = "english"):
        self.language = language
        self.stemmer = PorterStemmer()
        self.stop_words = set(stopwords.words(language))
        
        # ì½”ë“œ íŠ¹í™” ë¶ˆìš©ì–´ ì¶”ê°€ (ìµœì†Œí•œìœ¼ë¡œ ì œí•œ)
        self.code_stop_words = {
            'public', 'private', 'protected', 'static', 'final', 'void',
            'extends', 'implements', 'import', 'package', 
            'if', 'else', 'for', 'while', 'try', 'catch', 'throw', 'throws',
            'new', 'this', 'super', 'return',
            'const', 'let', 'var', 'async', 'await',
            'true', 'false', 'null', 'undefined', 'none'
        }
        # ì¤‘ìš”í•œ í‚¤ì›Œë“œë“¤ì€ ë¶ˆìš©ì–´ì—ì„œ ì œì™¸: class, function, def, interface, controller ë“±
        self.stop_words.update(self.code_stop_words)
    
    def tokenize(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ í† í°í™”"""
        if not text or not text.strip():
            return []
        
        try:
            # ì½”ë“œ íŒ¨í„´ ì „ì²˜ë¦¬
            text = self._preprocess_code(text)
            
            # ê¸°ë³¸ í† í°í™”
            tokens = word_tokenize(text.lower())
            
            # í•„í„°ë§ ë° ìŠ¤í…Œë°
            filtered_tokens = []
            for token in tokens:
                # ë¶ˆìš©ì–´ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°, ìµœì†Œ ê¸¸ì´ í™•ì¸
                if (len(token) > 1 and 
                    token not in self.stop_words and
                    token.isalnum()):
                    
                    # ìŠ¤í…Œë° ì ìš©
                    stemmed = self.stemmer.stem(token)
                    filtered_tokens.append(stemmed)
            
            return filtered_tokens
            
        except Exception as e:
            logger.warning(f"í† í°í™” ì‹¤íŒ¨: {e}, ì›ë³¸ í…ìŠ¤íŠ¸: {text[:100]}...")
            # ì‹¤íŒ¨ ì‹œ ê°„ë‹¨í•œ fallback í† í°í™”
            return self._fallback_tokenize(text)
    
    def _preprocess_code(self, text: str) -> str:
        """ì½”ë“œ íŠ¹í™” ì „ì²˜ë¦¬"""
        # CamelCase ë¶„ë¦¬ (getUserById -> get User By Id)
        text = re.sub(r'([a-z])([A-Z])', r'\1 \2', text)
        
        # snake_case ë¶„ë¦¬ (get_user_by_id -> get user by id)
        text = re.sub(r'_', ' ', text)
        
        # íŠ¹ìˆ˜ ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜ (ì¤‘ê´„í˜¸, ê´„í˜¸, ì„¸ë¯¸ì½œë¡  ë“±)
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # ì—°ì† ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    def _fallback_tokenize(self, text: str) -> List[str]:
        """Fallback í† í°í™” (NLTK ì‹¤íŒ¨ ì‹œ)"""
        # ê°„ë‹¨í•œ ì •ê·œí‘œí˜„ì‹ ê¸°ë°˜ í† í°í™”
        text = self._preprocess_code(text)
        tokens = re.findall(r'\b\w+\b', text.lower())
        
        # ê¸°ë³¸ í•„í„°ë§
        filtered = [
            token for token in tokens 
            if len(token) > 1 and token not in self.code_stop_words
        ]
        
        return filtered[:50]  # ìµœëŒ€ 50ê°œë¡œ ì œí•œ


class BM25IndexConfig:
    """BM25 Index ì„¤ì •"""
    
    def __init__(
        self,
        k1: float = 1.2,
        b: float = 0.75,
        top_k: int = 10,
        language: str = "english",
        index_path: str = "data/bm25_index",
        use_stemming: bool = True,
        include_metadata: bool = True,
        metadata_weight: float = 0.3  # ë©”íƒ€ë°ì´í„° ê°€ì¤‘ì¹˜
    ):
        self.k1 = k1
        self.b = b
        self.top_k = top_k
        self.language = language
        self.index_path = Path(index_path)
        self.use_stemming = use_stemming
        self.include_metadata = include_metadata
        self.metadata_weight = metadata_weight


class CodeBM25Index(BaseIndex):
    """ì½”ë“œ BM25 ì¸ë±ìŠ¤"""
    
    def __init__(self, config: BM25IndexConfig = None):
        self.config = config or BM25IndexConfig()
        self.tokenizer = CodeTokenizer(self.config.language)
        self.retriever = None
        self.nodes = []
        self.documents_map = {}  # ID -> EnhancedDocument ë§¤í•‘
        
        # ì¸ë±ìŠ¤ ì €ì¥ ê²½ë¡œ ìƒì„±
        self.config.index_path.mkdir(parents=True, exist_ok=True)
    
    async def setup(self):
        """ì¸ë±ìŠ¤ ì´ˆê¸°í™”"""
        try:
            # ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„
            if await self._load_existing_index():
                logger.info(f"ê¸°ì¡´ BM25 ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {len(self.nodes)}ê°œ ë¬¸ì„œ")
                return
            
            # ìƒˆ ì¸ë±ìŠ¤ ì´ˆê¸°í™”
            self.nodes = []
            self.documents_map = {}
            self._build_retriever()
            logger.info("ìƒˆë¡œìš´ BM25 ì¸ë±ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"BM25 ì¸ë±ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    async def add_documents(self, documents: List[Union[EnhancedDocument, Dict[str, Any]]]) -> List[str]:
        """ë¬¸ì„œ ì¶”ê°€"""
        if not documents:
            return []
        
        added_ids = []
        new_nodes = []
        
        try:
            for doc in documents:
                if isinstance(doc, EnhancedDocument):
                    # EnhancedDocument ì²˜ë¦¬
                    enhanced_text = self._create_enhanced_text(doc)
                    text_node = TextNode(
                        text=enhanced_text,
                        metadata=doc.metadata.model_dump(),
                        id_=doc.text_node.id_
                    )
                    
                    new_nodes.append(text_node)
                    self.documents_map[text_node.id_] = doc
                    added_ids.append(text_node.id_)
                    
                else:
                    # Dict í˜•íƒœì˜ ë¬¸ì„œ ì²˜ë¦¬
                    node = await self._create_text_node_from_dict(doc)
                    new_nodes.append(node)
                    added_ids.append(node.id_)
            
            # ê¸°ì¡´ ë…¸ë“œì— ì¶”ê°€
            self.nodes.extend(new_nodes)
            
            # Retriever ì¬êµ¬ì„±
            self._build_retriever()
            
            # ì¸ë±ìŠ¤ ì €ì¥
            await self._save_index()
            
            logger.info(f"BM25 ì¸ë±ìŠ¤ì— {len(added_ids)}ê°œ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ")
            return added_ids
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            raise
    
    def _create_enhanced_text(self, doc: EnhancedDocument) -> str:
        """ê°•í™”ëœ í…ìŠ¤íŠ¸ ìƒì„±"""
        text_parts = []
        
        # ê¸°ë³¸ ì½”ë“œ ë‚´ìš©
        text_parts.append(doc.text_node.text)
        
        # ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í…ìŠ¤íŠ¸ ì¶”ê°€ (ê°€ì¤‘ì¹˜ ì ìš©)
        if self.config.include_metadata:
            metadata = doc.metadata
            
            # í•¨ìˆ˜/í´ë˜ìŠ¤ëª… ê°•ì¡° (ë†’ì€ ê°€ì¤‘ì¹˜)
            for _ in range(3):  # 3ë²ˆ ë°˜ë³µìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì¦ê°€
                text_parts.append(metadata.name)
            
            # í‚¤ì›Œë“œ ì¶”ê°€
            if metadata.keywords:
                text_parts.extend(metadata.keywords)
            
            # íŒŒë¼ë¯¸í„° íƒ€ì… ì¶”ê°€
            for param in metadata.parameters:
                if isinstance(param, dict) and param.get('type'):
                    text_parts.append(param['type'])
                elif isinstance(param, str):
                    text_parts.append(param)
            
            # ë°˜í™˜ íƒ€ì… ì¶”ê°€
            if metadata.return_type:
                text_parts.append(metadata.return_type)
            
            # ìƒì†/êµ¬í˜„ ê´€ê³„
            if metadata.extends:
                text_parts.append(metadata.extends)
            
            if metadata.implements:
                text_parts.extend(metadata.implements)
            
            # ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ê°€
            if hasattr(doc, 'search_keywords') and doc.search_keywords:
                text_parts.extend(doc.search_keywords)
            
            # ì˜ë¯¸ì  íƒœê·¸ ì¶”ê°€
            if hasattr(doc, 'semantic_tags') and doc.semantic_tags:
                text_parts.extend(doc.semantic_tags)
        
        return ' '.join(filter(None, text_parts))
    
    async def _create_text_node_from_dict(self, doc_dict: Dict[str, Any]) -> TextNode:
        """ë”•ì…”ë„ˆë¦¬ì—ì„œ TextNode ìƒì„±"""
        node_id = doc_dict.get('id', str(uuid.uuid4()))
        text = doc_dict.get('content', doc_dict.get('text', ''))
        metadata = doc_dict.get('metadata', {})
        
        # ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í…ìŠ¤íŠ¸ ê°•í™”
        if self.config.include_metadata and metadata:
            enhanced_parts = [text]
            
            # ë©”íƒ€ë°ì´í„°ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ê°€
            for key in ['name', 'function_name', 'keywords']:
                if key in metadata:
                    value = metadata[key]
                    if isinstance(value, list):
                        enhanced_parts.extend(value)
                    elif isinstance(value, str):
                        enhanced_parts.append(value)
            
            text = ' '.join(filter(None, enhanced_parts))
        
        # ì¸ë±ì‹± ì‹œê°„ ì¶”ê°€
        metadata['indexed_at'] = datetime.now().isoformat()
        
        return TextNode(
            text=text,
            metadata=metadata,
            id_=node_id
        )
    
    def _build_retriever(self):
        """BM25 Retriever êµ¬ì„± (í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹: ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ + ê¸°ë³¸ í† í¬ë‚˜ì´ì €)"""
        if not self.nodes:
            self.retriever = None
            return
        
        try:
            # ğŸ“Œ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹: ì»¤ìŠ¤í…€ í† í¬ë‚˜ì´ì €ì˜ ì „ì²˜ë¦¬ ê¸°ëŠ¥ì„ í™œìš©í•œ ë…¸ë“œ ìƒì„±
            enhanced_nodes = self._create_enhanced_nodes_for_bm25()
            
            # ê¸°ë³¸ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ BM25Retriever ìƒì„± (ì•ˆì •ì„± í™•ë³´)
            self.retriever = BM25Retriever.from_defaults(
                nodes=enhanced_nodes,
                similarity_top_k=self.config.top_k
            )
            
            # BM25 íŒŒë¼ë¯¸í„° ê°•ì œ ì„¤ì •
            if hasattr(self.retriever, 'bm25') and self.retriever.bm25:
                self.retriever.bm25.k1 = self.config.k1
                self.retriever.bm25.b = self.config.b
                logger.debug(f"BM25 íŒŒë¼ë¯¸í„° ì„¤ì •: k1={self.config.k1}, b={self.config.b}")
            
            logger.debug(f"BM25 Retriever êµ¬ì„± ì™„ë£Œ: {len(enhanced_nodes)}ê°œ ë…¸ë“œ (í•˜ì´ë¸Œë¦¬ë“œ ì „ì²˜ë¦¬ ì ìš©)")
            
        except Exception as e:
            logger.error(f"BM25 Retriever êµ¬ì„± ì‹¤íŒ¨: {e}")
            self.retriever = None
    
    def _create_enhanced_nodes_for_bm25(self) -> List[TextNode]:
        """BM25ë¥¼ ìœ„í•œ í–¥ìƒëœ ë…¸ë“œ ìƒì„± (ì»¤ìŠ¤í…€ í† í¬ë‚˜ì´ì €ì˜ ì¥ì  í™œìš©)"""
        enhanced_nodes = []
        
        for node in self.nodes:
            # âœ¨ ì»¤ìŠ¤í…€ í† í¬ë‚˜ì´ì €ì˜ í•µì‹¬ ê¸°ëŠ¥ë“¤ì„ í™œìš©í•˜ì—¬ ê²€ìƒ‰ ì¹œí™”ì  í…ìŠ¤íŠ¸ ìƒì„±
            enhanced_text = self._enhance_text_for_search(node.text)
            
            # í–¥ìƒëœ í…ìŠ¤íŠ¸ë¡œ ìƒˆ ë…¸ë“œ ìƒì„±
            enhanced_node = TextNode(
                text=enhanced_text,
                metadata=node.metadata,
                id_=node.id_
            )
            enhanced_nodes.append(enhanced_node)
        
        return enhanced_nodes
    
    def _enhance_text_for_search(self, original_text: str) -> str:
        """ê²€ìƒ‰ì„ ìœ„í•œ í…ìŠ¤íŠ¸ í–¥ìƒ (ì»¤ìŠ¤í…€ í† í¬ë‚˜ì´ì € ê¸°ëŠ¥ í™œìš©)"""
        # 1. ì›ë³¸ í…ìŠ¤íŠ¸ ìœ ì§€
        enhanced_parts = [original_text]
        
        # 2. ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ ì ìš© (CamelCase ë¶„ë¦¬, snake_case ë¶„ë¦¬ ë“±)
        preprocessed = self.tokenizer._preprocess_code(original_text)
        enhanced_parts.append(preprocessed)
        
        # 3. ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ ë° ê°•ì¡° (ê°€ì¤‘ì¹˜ ì¦ê°€)
        try:
            important_tokens = self._extract_important_keywords(original_text)
            if important_tokens:
                # ì¤‘ìš” í‚¤ì›Œë“œë“¤ì„ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µí•˜ì—¬ ê°€ì¤‘ì¹˜ ì¦ê°€
                enhanced_parts.extend(important_tokens * 2)  # 2ë²ˆ ë°˜ë³µìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì¦ê°€
        except Exception as e:
            logger.debug(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        # 4. ëª¨ë“  ë¶€ë¶„ì„ ê³µë°±ìœ¼ë¡œ ì—°ê²°
        return " ".join(enhanced_parts)
    
    def _extract_important_keywords(self, text: str) -> List[str]:
        """ì¤‘ìš” í‚¤ì›Œë“œ ì¶”ì¶œ (í´ë˜ìŠ¤ëª…, ë©”ì„œë“œëª…, ì–´ë…¸í…Œì´ì…˜ ë“±)"""
        import re
        keywords = []
        
        # í´ë˜ìŠ¤ëª… ì¶”ì¶œ (class ë‹¤ìŒì˜ ë‹¨ì–´)
        class_matches = re.findall(r'\bclass\s+(\w+)', text, re.IGNORECASE)
        keywords.extend(class_matches)
        
        # ë©”ì„œë“œëª… ì¶”ì¶œ (í•¨ìˆ˜ëª…())
        method_matches = re.findall(r'\b(\w+)\s*\(', text)
        keywords.extend(method_matches)
        
        # ì–´ë…¸í…Œì´ì…˜ ì¶”ì¶œ (@RestController ë“±)
        annotation_matches = re.findall(r'@(\w+)', text)
        keywords.extend(annotation_matches)
        
        # Controller, Service ë“± ì¤‘ìš” ì ‘ë¯¸ì‚¬
        important_suffixes = re.findall(r'\b(\w*(?:Controller|Service|Repository|Component|Entity|DTO|Interface))\b', text, re.IGNORECASE)
        keywords.extend(important_suffixes)
        
        return list(set(keywords))  # ì¤‘ë³µ ì œê±°
    
    async def search(self, query: str, limit: int = 10, filters: Dict[str, Any] = None) -> List[IndexedDocument]:
        """BM25 ê²€ìƒ‰"""
        if not self.retriever or not query.strip():
            return []
        
        try:
            # ê²€ìƒ‰ ì‹¤í–‰
            nodes_with_scores = self.retriever.retrieve(query)
            
            # ê²°ê³¼ ë³€í™˜
            results = []
            for node_with_score in nodes_with_scores:
                if len(results) >= limit:
                    break
                    
                node = node_with_score.node
                
                # í•„í„° ì ìš©
                if filters and not self._apply_filters(node.metadata, filters):
                    continue
                
                indexed_doc = IndexedDocument(
                    id=node.id_,
                    content=node.text,
                    metadata=node.metadata,
                    indexed_at=node.metadata.get('indexed_at', '')
                )
                results.append(indexed_doc)
            
            logger.debug(f"BM25 ê²€ìƒ‰ ì™„ë£Œ: ì¿¼ë¦¬='{query}', ê²°ê³¼={len(results)}ê°œ")
            return results
            
        except Exception as e:
            logger.error(f"BM25 ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    async def search_with_scores(self, query: str, limit: int = 10, filters: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """ì ìˆ˜ì™€ í•¨ê»˜ BM25 ê²€ìƒ‰"""
        if not self.retriever or not query.strip():
            logger.warning(f"BM25 ê²€ìƒ‰ ì¤‘ë‹¨: retriever={self.retriever is not None}, query='{query.strip()}'")
            return []
        
        try:
            logger.debug(f"BM25 ê²€ìƒ‰ ì‹œì‘: query='{query}', limit={limit}")
            nodes_with_scores = self.retriever.retrieve(query)
            logger.debug(f"BM25 ì›ì‹œ ê²°ê³¼: {len(nodes_with_scores)}ê°œ")
            
            results = []
            for i, node_with_score in enumerate(nodes_with_scores):
                if len(results) >= limit:
                    break
                    
                node = node_with_score.node
                score = node_with_score.score
                
                logger.debug(f"ê²°ê³¼ #{i}: id={node.id_}, score={score}, content_length={len(node.text)}")
                
                # í•„í„° ì ìš©
                if filters and not self._apply_filters(node.metadata, filters):
                    logger.debug(f"í•„í„°ë¡œ ì œì™¸ëœ ê²°ê³¼: {node.id_}")
                    continue
                
                result = {
                    'id': node.id_,
                    'content': node.text,
                    'metadata': node.metadata,
                    'score': max(0.0, float(score)) if score is not None else 0.0,
                    'source': 'bm25'
                }
                results.append(result)
            
            logger.debug(f"BM25 ì ìˆ˜ ê²€ìƒ‰ ì™„ë£Œ: ì¿¼ë¦¬='{query}', ê²°ê³¼={len(results)}ê°œ")
            return results
            
        except Exception as e:
            logger.error(f"ì ìˆ˜ë³„ BM25 ê²€ìƒ‰ ì‹¤íŒ¨: {e}", exc_info=True)
            return []
    
    def _apply_filters(self, metadata: Dict[str, Any], filters: Dict[str, Any]) -> bool:
        """í•„í„° ì ìš©"""
        try:
            for key, value in filters.items():
                if key not in metadata:
                    return False
                
                metadata_value = metadata[key]
                
                # ë¦¬ìŠ¤íŠ¸ íƒ€ì… ì²˜ë¦¬
                if isinstance(metadata_value, list):
                    if value not in metadata_value:
                        return False
                else:
                    if metadata_value != value:
                        return False
            
            return True
            
        except Exception as e:
            logger.warning(f"í•„í„° ì ìš© ì‹¤íŒ¨: {e}")
            return False
    
    async def update_document(self, doc_id: str, document: Dict[str, Any]) -> bool:
        """ë¬¸ì„œ ì—…ë°ì´íŠ¸"""
        try:
            # ê¸°ì¡´ ë¬¸ì„œ ì œê±°
            await self.delete_document(doc_id)
            
            # ìƒˆ ë¬¸ì„œ ì¶”ê°€
            updated_doc = document.copy()
            updated_doc['id'] = doc_id
            added_ids = await self.add_documents([updated_doc])
            
            success = len(added_ids) > 0
            if success:
                logger.info(f"ë¬¸ì„œ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {doc_id}")
            
            return success
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ ({doc_id}): {e}")
            return False
    
    async def delete_document(self, doc_id: str) -> bool:
        """ë¬¸ì„œ ì‚­ì œ"""
        try:
            # ë…¸ë“œ ëª©ë¡ì—ì„œ ì œê±°
            original_count = len(self.nodes)
            self.nodes = [node for node in self.nodes if node.id_ != doc_id]
            removed_count = original_count - len(self.nodes)
            
            # ë¬¸ì„œ ë§µì—ì„œ ì œê±°
            if doc_id in self.documents_map:
                del self.documents_map[doc_id]
            
            # Retriever ì¬êµ¬ì„±
            self._build_retriever()
            
            # ì¸ë±ìŠ¤ ì €ì¥
            await self._save_index()
            
            if removed_count > 0:
                logger.info(f"ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ: {doc_id}")
            
            return removed_count > 0
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨ ({doc_id}): {e}")
            return False
    
    async def get_stats(self) -> Dict[str, Any]:
        """ì¸ë±ìŠ¤ í†µê³„ ì •ë³´"""
        try:
            total_docs = len(self.nodes)
            
            if total_docs == 0:
                return {
                    "total_documents": 0,
                    "total_tokens": 0,
                    "average_tokens_per_doc": 0.0,
                    "language_distribution": {},
                    "bm25_parameters": {
                        "k1": self.config.k1,
                        "b": self.config.b
                    },
                    "index_path": str(self.config.index_path)
                }
            
            # í† í° ìˆ˜ ê³„ì‚°
            total_tokens = 0
            for node in self.nodes:
                try:
                    tokens = self.tokenizer.tokenize(node.text)
                    total_tokens += len(tokens)
                except:
                    # í† í°í™” ì‹¤íŒ¨ ì‹œ ëŒ€ëµì ì¸ ì¶”ì •
                    total_tokens += len(node.text.split())
            
            avg_tokens = total_tokens / total_docs if total_docs > 0 else 0
            
            # ì–¸ì–´ë³„ ë¬¸ì„œ ìˆ˜ ê³„ì‚°
            language_stats = {}
            for node in self.nodes:
                lang = node.metadata.get('language', 'unknown')
                language_stats[lang] = language_stats.get(lang, 0) + 1
            
            return {
                "total_documents": total_docs,
                "total_tokens": total_tokens,
                "average_tokens_per_doc": round(avg_tokens, 2),
                "language_distribution": language_stats,
                "bm25_parameters": {
                    "k1": self.config.k1,
                    "b": self.config.b
                },
                "index_path": str(self.config.index_path)
            }
            
        except Exception as e:
            logger.error(f"í†µê³„ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                "total_documents": 0,
                "total_tokens": 0,
                "error": str(e)
            }
    
    async def _save_index(self):
        """ì¸ë±ìŠ¤ ì €ì¥"""
        try:
            # ë…¸ë“œ ë°ì´í„° ì €ì¥
            nodes_data = []
            for node in self.nodes:
                nodes_data.append({
                    'id': node.id_,
                    'text': node.text,
                    'metadata': node.metadata
                })
            
            nodes_file = self.config.index_path / "nodes.json"
            with open(nodes_file, 'w', encoding='utf-8') as f:
                json.dump(nodes_data, f, ensure_ascii=False, indent=2)
            
            # ë¬¸ì„œ ë§µ ì €ì¥ (pickleì€ ì„ íƒì )
            if self.documents_map:
                docs_map_file = self.config.index_path / "documents_map.pkl"
                with open(docs_map_file, 'wb') as f:
                    pickle.dump(self.documents_map, f)
            
            logger.debug(f"BM25 ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {len(self.nodes)}ê°œ ë…¸ë“œ")
            
        except Exception as e:
            logger.error(f"ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    async def _load_existing_index(self) -> bool:
        """ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ"""
        try:
            nodes_file = self.config.index_path / "nodes.json"
            docs_map_file = self.config.index_path / "documents_map.pkl"
            
            if not nodes_file.exists():
                return False
            
            # ë…¸ë“œ ë°ì´í„° ë¡œë“œ
            with open(nodes_file, 'r', encoding='utf-8') as f:
                nodes_data = json.load(f)
            
            self.nodes = []
            for node_data in nodes_data:
                node = TextNode(
                    text=node_data['text'],
                    metadata=node_data['metadata'],
                    id_=node_data['id']
                )
                self.nodes.append(node)
            
            # ë¬¸ì„œ ë§µ ë¡œë“œ (ì„ íƒì )
            self.documents_map = {}
            if docs_map_file.exists():
                try:
                    with open(docs_map_file, 'rb') as f:
                        self.documents_map = pickle.load(f)
                except Exception as e:
                    logger.warning(f"ë¬¸ì„œ ë§µ ë¡œë“œ ì‹¤íŒ¨: {e}")
            
            # Retriever êµ¬ì„±
            self._build_retriever()
            
            return True
            
        except Exception as e:
            logger.warning(f"ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False 