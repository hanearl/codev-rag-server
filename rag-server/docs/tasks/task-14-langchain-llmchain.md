# Task 14: LangChain LLMChain êµ¬í˜„

## ğŸ“‹ ì‘ì—… ê°œìš”
LangChain LLMChainì„ í™œìš©í•˜ì—¬ PromptTemplateê³¼ LLMì„ ì—°ê²°í•˜ê³ , ê²€ìƒ‰ëœ ì½”ë“œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ê³ í’ˆì§ˆ ì‘ë‹µ ìƒì„± ì‹œìŠ¤í…œì„ êµ¬í˜„í•©ë‹ˆë‹¤.

## ğŸ¯ ì‘ì—… ëª©í‘œ
- LangChain LLMChain ê¸°ë°˜ ì‘ë‹µ ìƒì„± ì‹œìŠ¤í…œ êµ¬ì¶•
- ë‹¤ì–‘í•œ LLM ì œê³µì ì§€ì› (OpenAI, Anthropic ë“±)
- ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ë° ë¹„ë™ê¸° ì²˜ë¦¬ ì§€ì›
- ì‘ë‹µ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ë° ìºì‹± ê¸°ëŠ¥

## ğŸ”— ì˜ì¡´ì„±
- **ì„ í–‰ Task**: Task 13 (LangChain PromptTemplate êµ¬í˜„)
- **í™œìš©í•  ê¸°ì¡´ ì½”ë“œ**: `app/features/generation/service.py`

## ğŸ”§ êµ¬í˜„ ì‚¬í•­

### 1. LLMChain í•µì‹¬ êµ¬í˜„

```python
# app/llm/langchain_llm.py
from typing import List, Dict, Any, Optional, Union, AsyncGenerator
from langchain.llms.base import BaseLLM
from langchain.chat_models import ChatOpenAI, ChatAnthropic
from langchain.chains import LLMChain
from langchain.schema import BaseMessage, HumanMessage, SystemMessage
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.callbacks.base import BaseCallbackHandler
from pydantic import BaseModel, Field
from enum import Enum
import asyncio
import time
import logging

logger = logging.getLogger(__name__)

class LLMProvider(str, Enum):
    """LLM ì œê³µì"""
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    LOCAL = "local"

class LLMConfig(BaseModel):
    """LLM ì„¤ì •"""
    provider: LLMProvider = LLMProvider.OPENAI
    model_name: str = "gpt-4"
    temperature: float = Field(default=0.1, ge=0.0, le=2.0)
    max_tokens: int = Field(default=2048, gt=0)
    streaming: bool = True
    timeout: int = Field(default=30, gt=0)
    max_retries: int = Field(default=3, ge=0)
    
    # OpenAI íŠ¹í™” ì„¤ì •
    openai_api_key: Optional[str] = None
    openai_organization: Optional[str] = None
    
    # Anthropic íŠ¹í™” ì„¤ì •
    anthropic_api_key: Optional[str] = None
    
    # ë¡œì»¬ LLM ì„¤ì •
    local_model_path: Optional[str] = None
    local_api_url: Optional[str] = None

class ResponseMetadata(BaseModel):
    """ì‘ë‹µ ë©”íƒ€ë°ì´í„°"""
    provider: str
    model_name: str
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int
    response_time_ms: int
    finish_reason: Optional[str] = None
    cached: bool = False

class LLMResponse(BaseModel):
    """LLM ì‘ë‹µ ëª¨ë¸"""
    content: str
    metadata: ResponseMetadata
    error: Optional[str] = None
    is_complete: bool = True

class CustomCallbackHandler(BaseCallbackHandler):
    """ì»¤ìŠ¤í…€ ì½œë°± í•¸ë“¤ëŸ¬"""
    
    def __init__(self):
        self.tokens_used = 0
        self.start_time = None
        self.response_parts = []
    
    def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs) -> None:
        self.start_time = time.time()
        logger.info(f"LLM í˜¸ì¶œ ì‹œì‘: {serialized.get('name', 'unknown')}")
    
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.response_parts.append(token)
        self.tokens_used += 1
    
    def on_llm_end(self, response, **kwargs) -> None:
        end_time = time.time()
        response_time = int((end_time - self.start_time) * 1000) if self.start_time else 0
        logger.info(f"LLM í˜¸ì¶œ ì™„ë£Œ: {response_time}ms, í† í°: {self.tokens_used}")
    
    def on_llm_error(self, error: Exception, **kwargs) -> None:
        logger.error(f"LLM í˜¸ì¶œ ì˜¤ë¥˜: {error}")

class CodeLLMChain:
    """ì½”ë“œ íŠ¹í™” LLM ì²´ì¸"""
    
    def __init__(self, config: LLMConfig = None):
        self.config = config or LLMConfig()
        self.llm = self._create_llm()
        self.callback_handler = CustomCallbackHandler()
        self._response_cache = {}
    
    def _create_llm(self) -> BaseLLM:
        """LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        callback_manager = CallbackManager([self.callback_handler])
        
        if self.config.provider == LLMProvider.OPENAI:
            return ChatOpenAI(
                model_name=self.config.model_name,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
                streaming=self.config.streaming,
                callback_manager=callback_manager,
                openai_api_key=self.config.openai_api_key,
                openai_organization=self.config.openai_organization,
                request_timeout=self.config.timeout,
                max_retries=self.config.max_retries
            )
        
        elif self.config.provider == LLMProvider.ANTHROPIC:
            return ChatAnthropic(
                model=self.config.model_name,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
                streaming=self.config.streaming,
                callback_manager=callback_manager,
                anthropic_api_key=self.config.anthropic_api_key,
                timeout=self.config.timeout,
                max_retries=self.config.max_retries
            )
        
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM ì œê³µì: {self.config.provider}")
    
    async def generate_response(
        self,
        prompt: str,
        use_cache: bool = True,
        **kwargs
    ) -> LLMResponse:
        """ì‘ë‹µ ìƒì„±"""
        # ìºì‹œ í™•ì¸
        cache_key = self._generate_cache_key(prompt, kwargs)
        if use_cache and cache_key in self._response_cache:
            cached_response = self._response_cache[cache_key]
            cached_response.metadata.cached = True
            logger.info(f"ìºì‹œëœ ì‘ë‹µ ë°˜í™˜: {cache_key[:16]}...")
            return cached_response
        
        start_time = time.time()
        
        try:
            # LLM ì²´ì¸ ìƒì„±
            chain = LLMChain(llm=self.llm, prompt=None, verbose=True)
            
            # ì‘ë‹µ ìƒì„±
            self.callback_handler = CustomCallbackHandler()  # ë¦¬ì…‹
            
            if self.config.streaming:
                response_content = await self._generate_streaming_response(prompt)
            else:
                response_content = await self._generate_simple_response(prompt)
            
            end_time = time.time()
            response_time = int((end_time - start_time) * 1000)
            
            # ë©”íƒ€ë°ì´í„° ìƒì„±
            metadata = ResponseMetadata(
                provider=self.config.provider.value,
                model_name=self.config.model_name,
                prompt_tokens=self._estimate_tokens(prompt),
                completion_tokens=self._estimate_tokens(response_content),
                total_tokens=self._estimate_tokens(prompt + response_content),
                response_time_ms=response_time,
                cached=False
            )
            
            response = LLMResponse(
                content=response_content,
                metadata=metadata
            )
            
            # ìºì‹œ ì €ì¥
            if use_cache:
                self._response_cache[cache_key] = response
            
            return response
            
        except Exception as e:
            logger.error(f"LLM ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return LLMResponse(
                content="",
                metadata=ResponseMetadata(
                    provider=self.config.provider.value,
                    model_name=self.config.model_name,
                    prompt_tokens=0,
                    completion_tokens=0,
                    total_tokens=0,
                    response_time_ms=0
                ),
                error=str(e),
                is_complete=False
            )
    
    async def _generate_simple_response(self, prompt: str) -> str:
        """ë‹¨ìˆœ ì‘ë‹µ ìƒì„±"""
        messages = [HumanMessage(content=prompt)]
        response = await self.llm.agenerate([messages])
        return response.generations[0][0].text
    
    async def _generate_streaming_response(self, prompt: str) -> str:
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±"""
        messages = [HumanMessage(content=prompt)]
        response_parts = []
        
        async for chunk in self.llm.astream(messages):
            if hasattr(chunk, 'content'):
                response_parts.append(chunk.content)
            elif isinstance(chunk, str):
                response_parts.append(chunk)
        
        return "".join(response_parts)
    
    async def generate_streaming_response(
        self,
        prompt: str,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì œë„ˆë ˆì´í„°"""
        try:
            messages = [HumanMessage(content=prompt)]
            
            async for chunk in self.llm.astream(messages):
                if hasattr(chunk, 'content') and chunk.content:
                    yield chunk.content
                elif isinstance(chunk, str) and chunk:
                    yield chunk
                    
        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            yield f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def _generate_cache_key(self, prompt: str, kwargs: Dict[str, Any]) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        import hashlib
        
        cache_data = {
            "prompt": prompt,
            "model": self.config.model_name,
            "temperature": self.config.temperature,
            "max_tokens": self.config.max_tokens,
            **kwargs
        }
        
        cache_string = str(sorted(cache_data.items()))
        return hashlib.md5(cache_string.encode()).hexdigest()
    
    def _estimate_tokens(self, text: str) -> int:
        """í† í° ìˆ˜ ì¶”ì • (ëŒ€ëµì )"""
        # ì‹¤ì œë¡œëŠ” tiktoken ë“±ì„ ì‚¬ìš©í•´ì•¼ í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ì¶”ì •
        return int(len(text.split()) * 1.3)
    
    def clear_cache(self):
        """ìºì‹œ í´ë¦¬ì–´"""
        self._response_cache.clear()
        logger.info("LLM ì‘ë‹µ ìºì‹œ í´ë¦¬ì–´ë¨")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„"""
        return {
            "cache_size": len(self._response_cache),
            "cache_keys": list(self._response_cache.keys())[:10]  # ìµœëŒ€ 10ê°œë§Œ
        }
```

### 2. í†µí•© LLM ì„œë¹„ìŠ¤

```python
# app/llm/llm_service.py
from typing import List, Dict, Any, Optional, AsyncGenerator
from .langchain_llm import CodeLLMChain, LLMConfig, LLMResponse, LLMProvider
from .prompt_service import PromptService, PromptType
import logging
import asyncio

logger = logging.getLogger(__name__)

class LLMService:
    """í†µí•© LLM ì„œë¹„ìŠ¤"""
    
    def __init__(self, llm_config: LLMConfig = None, prompt_service: PromptService = None):
        self.llm_config = llm_config or LLMConfig()
        self.prompt_service = prompt_service or PromptService()
        self.llm_chain = CodeLLMChain(self.llm_config)
        self._request_stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_tokens": 0
        }
    
    async def explain_code(
        self,
        code_content: str,
        language: str,
        question: str = "ì´ ì½”ë“œê°€ ë¬´ì—‡ì„ í•˜ëŠ”ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”",
        context_data: Dict[str, Any] = None,
        use_cache: bool = True
    ) -> LLMResponse:
        """ì½”ë“œ ì„¤ëª… ìƒì„±"""
        try:
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = await self.prompt_service.create_code_explanation_prompt(
                code_content, language, question, context_data
            )
            
            # LLM ì‘ë‹µ ìƒì„±
            response = await self.llm_chain.generate_response(prompt, use_cache)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self._update_stats(response)
            
            logger.info(f"ì½”ë“œ ì„¤ëª… ìƒì„± ì™„ë£Œ: {response.metadata.response_time_ms}ms")
            return response
            
        except Exception as e:
            logger.error(f"ì½”ë“œ ì„¤ëª… ìƒì„± ì‹¤íŒ¨: {e}")
            self._request_stats["failed_requests"] += 1
            raise
    
    async def generate_code(
        self,
        requirements: str,
        language: str,
        related_code: List[str] = None,
        context_data: Dict[str, Any] = None,
        use_cache: bool = True
    ) -> LLMResponse:
        """ì½”ë“œ ìƒì„±"""
        try:
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = await self.prompt_service.create_code_generation_prompt(
                requirements, language, related_code, context_data
            )
            
            # LLM ì‘ë‹µ ìƒì„±
            response = await self.llm_chain.generate_response(prompt, use_cache)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self._update_stats(response)
            
            logger.info(f"ì½”ë“œ ìƒì„± ì™„ë£Œ: {response.metadata.response_time_ms}ms")
            return response
            
        except Exception as e:
            logger.error(f"ì½”ë“œ ìƒì„± ì‹¤íŒ¨: {e}")
            self._request_stats["failed_requests"] += 1
            raise
    
    async def review_code(
        self,
        code_content: str,
        language: str,
        context_data: Dict[str, Any] = None,
        use_cache: bool = True
    ) -> LLMResponse:
        """ì½”ë“œ ë¦¬ë·°"""
        try:
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = await self.prompt_service.create_code_review_prompt(
                code_content, language, context_data
            )
            
            # LLM ì‘ë‹µ ìƒì„±
            response = await self.llm_chain.generate_response(prompt, use_cache)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self._update_stats(response)
            
            logger.info(f"ì½”ë“œ ë¦¬ë·° ì™„ë£Œ: {response.metadata.response_time_ms}ms")
            return response
            
        except Exception as e:
            logger.error(f"ì½”ë“œ ë¦¬ë·° ì‹¤íŒ¨: {e}")
            self._request_stats["failed_requests"] += 1
            raise
    
    async def help_debug(
        self,
        code_content: str,
        language: str,
        problem_description: str,
        error_info: str = None,
        context_data: Dict[str, Any] = None,
        use_cache: bool = True
    ) -> LLMResponse:
        """ë””ë²„ê¹… ë„ì›€"""
        try:
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = await self.prompt_service.create_debugging_help_prompt(
                code_content, language, problem_description, error_info, context_data
            )
            
            # LLM ì‘ë‹µ ìƒì„±
            response = await self.llm_chain.generate_response(prompt, use_cache)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self._update_stats(response)
            
            logger.info(f"ë””ë²„ê¹… ë„ì›€ ì™„ë£Œ: {response.metadata.response_time_ms}ms")
            return response
            
        except Exception as e:
            logger.error(f"ë””ë²„ê¹… ë„ì›€ ì‹¤íŒ¨: {e}")
            self._request_stats["failed_requests"] += 1
            raise
    
    async def answer_question(
        self,
        question: str,
        code_contexts: List[Dict[str, Any]],
        language: str = "java",
        use_cache: bool = True
    ) -> LLMResponse:
        """ì§ˆë¬¸ ë‹µë³€"""
        try:
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = await self.prompt_service.create_general_qa_prompt(
                question, code_contexts, language
            )
            
            # LLM ì‘ë‹µ ìƒì„±
            response = await self.llm_chain.generate_response(prompt, use_cache)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self._update_stats(response)
            
            logger.info(f"ì§ˆë¬¸ ë‹µë³€ ì™„ë£Œ: {response.metadata.response_time_ms}ms")
            return response
            
        except Exception as e:
            logger.error(f"ì§ˆë¬¸ ë‹µë³€ ì‹¤íŒ¨: {e}")
            self._request_stats["failed_requests"] += 1
            raise
    
    async def generate_streaming_response(
        self,
        prompt_type: PromptType,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±"""
        try:
            # í”„ë¡¬í”„íŠ¸ íƒ€ì…ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ìƒì„±
            if prompt_type == PromptType.CODE_EXPLANATION:
                prompt = await self.prompt_service.create_code_explanation_prompt(
                    kwargs.get('code_content', ''),
                    kwargs.get('language', 'java'),
                    kwargs.get('question', 'ì½”ë“œë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”'),
                    kwargs.get('context_data')
                )
            elif prompt_type == PromptType.CODE_GENERATION:
                prompt = await self.prompt_service.create_code_generation_prompt(
                    kwargs.get('requirements', ''),
                    kwargs.get('language', 'java'),
                    kwargs.get('related_code'),
                    kwargs.get('context_data')
                )
            elif prompt_type == PromptType.GENERAL_QA:
                prompt = await self.prompt_service.create_general_qa_prompt(
                    kwargs.get('question', ''),
                    kwargs.get('code_contexts', []),
                    kwargs.get('language', 'java')
                )
            else:
                yield "ì§€ì›í•˜ì§€ ì•ŠëŠ” í”„ë¡¬í”„íŠ¸ íƒ€ì…ì…ë‹ˆë‹¤."
                return
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
            async for chunk in self.llm_chain.generate_streaming_response(prompt):
                yield chunk
            
            # í†µê³„ ì—…ë°ì´íŠ¸ (ë¹„ë™ê¸°ë¡œ)
            asyncio.create_task(self._update_streaming_stats())
            
        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            yield f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    async def batch_process(
        self,
        requests: List[Dict[str, Any]],
        max_concurrent: int = 3
    ) -> List[LLMResponse]:
        """ë°°ì¹˜ ì²˜ë¦¬"""
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def process_single_request(request_data: Dict[str, Any]) -> LLMResponse:
            async with semaphore:
                request_type = request_data.get('type')
                
                if request_type == 'explain':
                    return await self.explain_code(**request_data.get('params', {}))
                elif request_type == 'generate':
                    return await self.generate_code(**request_data.get('params', {}))
                elif request_type == 'review':
                    return await self.review_code(**request_data.get('params', {}))
                elif request_type == 'debug':
                    return await self.help_debug(**request_data.get('params', {}))
                elif request_type == 'qa':
                    return await self.answer_question(**request_data.get('params', {}))
                else:
                    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ìš”ì²­ íƒ€ì…: {request_type}")
        
        # ëª¨ë“  ìš”ì²­ ë³‘ë ¬ ì²˜ë¦¬
        tasks = [process_single_request(req) for req in requests]
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ì˜ˆì™¸ë¥¼ LLMResponseë¡œ ë³€í™˜
        processed_responses = []
        for response in responses:
            if isinstance(response, Exception):
                processed_responses.append(LLMResponse(
                    content="",
                    metadata=ResponseMetadata(
                        provider=self.llm_config.provider.value,
                        model_name=self.llm_config.model_name,
                        prompt_tokens=0,
                        completion_tokens=0,
                        total_tokens=0,
                        response_time_ms=0
                    ),
                    error=str(response),
                    is_complete=False
                ))
            else:
                processed_responses.append(response)
        
        return processed_responses
    
    def _update_stats(self, response: LLMResponse):
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        self._request_stats["total_requests"] += 1
        if response.error is None:
            self._request_stats["successful_requests"] += 1
        else:
            self._request_stats["failed_requests"] += 1
        
        self._request_stats["total_tokens"] += response.metadata.total_tokens
    
    async def _update_streaming_stats(self):
        """ìŠ¤íŠ¸ë¦¬ë° í†µê³„ ì—…ë°ì´íŠ¸"""
        self._request_stats["total_requests"] += 1
        self._request_stats["successful_requests"] += 1
    
    async def get_service_stats(self) -> Dict[str, Any]:
        """ì„œë¹„ìŠ¤ í†µê³„"""
        cache_stats = self.llm_chain.get_cache_stats()
        
        return {
            "request_stats": self._request_stats,
            "cache_stats": cache_stats,
            "llm_config": {
                "provider": self.llm_config.provider.value,
                "model_name": self.llm_config.model_name,
                "temperature": self.llm_config.temperature,
                "max_tokens": self.llm_config.max_tokens,
                "streaming": self.llm_config.streaming
            }
        }
    
    async def update_llm_config(self, new_config: Dict[str, Any]):
        """LLM ì„¤ì • ì—…ë°ì´íŠ¸"""
        # ê¸°ì¡´ ì„¤ì • ì—…ë°ì´íŠ¸
        current_config = self.llm_config.dict()
        current_config.update(new_config)
        
        # ìƒˆ ì„¤ì •ìœ¼ë¡œ LLM ì¬ìƒì„±
        self.llm_config = LLMConfig(**current_config)
        self.llm_chain = CodeLLMChain(self.llm_config)
        
        logger.info(f"LLM ì„¤ì • ì—…ë°ì´íŠ¸: {new_config}")
    
    async def health_check(self) -> Dict[str, Any]:
        """í—¬ìŠ¤ ì²´í¬"""
        try:
            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì‘ë‹µ ìƒì„±
            test_response = await self.llm_chain.generate_response(
                "ì•ˆë…•í•˜ì„¸ìš”. ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤.",
                use_cache=False
            )
            
            return {
                "status": "healthy",
                "provider": self.llm_config.provider.value,
                "model": self.llm_config.model_name,
                "test_response_time_ms": test_response.metadata.response_time_ms,
                "test_successful": test_response.error is None
            }
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e)
            }
```

## âœ… ì™„ë£Œ ì¡°ê±´

1. **LLMChain êµ¬í˜„**: LangChain ê¸°ë°˜ LLM ì²´ì¸ ì™„ì „ êµ¬í˜„
2. **ë‹¤ì¤‘ ì œê³µì ì§€ì›**: OpenAI, Anthropic ë“± ë‹¤ì–‘í•œ LLM ì§€ì›
3. **ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ**: ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì§€ì›
4. **ì‘ë‹µ ìºì‹±**: íš¨ìœ¨ì ì¸ ì‘ë‹µ ìºì‹± ì‹œìŠ¤í…œ
5. **ë°°ì¹˜ ì²˜ë¦¬**: ë‹¤ì¤‘ ìš”ì²­ ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›
6. **ëª¨ë‹ˆí„°ë§**: ìƒì„¸í•œ í†µê³„ ë° ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
7. **ì—ëŸ¬ ì²˜ë¦¬**: ëª¨ë“  ì˜ˆì™¸ ìƒí™© ì ì ˆíˆ ì²˜ë¦¬

## ğŸ“‹ ë‹¤ìŒ Taskì™€ì˜ ì—°ê´€ê´€ê³„

- **Task 15**: HybridRAG ì„œë¹„ìŠ¤ì—ì„œ ì´ LLM ì„œë¹„ìŠ¤ë¥¼ í•µì‹¬ êµ¬ì„±ìš”ì†Œë¡œ í™œìš©

## ğŸ§ª í…ŒìŠ¤íŠ¸ ê³„íš

```python
# tests/unit/llm/test_langchain_llm.py
async def test_llm_chain_initialization():
    """LLM ì²´ì¸ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
    config = LLMConfig(provider=LLMProvider.OPENAI)
    chain = CodeLLMChain(config)
    assert chain.llm is not None

async def test_response_generation():
    """ì‘ë‹µ ìƒì„± í…ŒìŠ¤íŠ¸"""
    service = LLMService()
    response = await service.explain_code(
        "public void test() {}", "java", "ì´ ë©”ì„œë“œë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”"
    )
    assert response.content != ""
    assert response.metadata.response_time_ms > 0

async def test_streaming_response():
    """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í…ŒìŠ¤íŠ¸"""
    service = LLMService()
    chunks = []
    async for chunk in service.generate_streaming_response(
        PromptType.CODE_EXPLANATION,
        code_content="public void test() {}",
        language="java"
    ):
        chunks.append(chunk)
    
    assert len(chunks) > 0

async def test_batch_processing():
    """ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
    service = LLMService()
    requests = [
        {"type": "explain", "params": {"code_content": "test", "language": "java"}},
        {"type": "generate", "params": {"requirements": "ê°„ë‹¨í•œ í•¨ìˆ˜", "language": "java"}}
    ]
    responses = await service.batch_process(requests, max_concurrent=2)
    assert len(responses) == 2
```

ì´ TaskëŠ” ê²€ìƒ‰ëœ ì½”ë“œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³ í’ˆì§ˆ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” í•µì‹¬ êµ¬ì„±ìš”ì†Œì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ LLMê³¼ í”„ë¡¬í”„íŠ¸ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì¡°í•©í•˜ì—¬ ìµœì ì˜ ì‚¬ìš©ì ê²½í—˜ì„ ì œê³µí•©ë‹ˆë‹¤. 