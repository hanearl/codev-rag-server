# Task 15: HybridRAG ì„œë¹„ìŠ¤ êµ¬í˜„

## ğŸ“‹ ì‘ì—… ê°œìš”
ëª¨ë“  êµ¬ì„±ìš”ì†Œ(Hybrid Retriever, PromptTemplate, LLMChain)ë¥¼ í†µí•©í•˜ì—¬ ì™„ì „í•œ í•˜ì´ë¸Œë¦¬ë“œ RAG ì‹œìŠ¤í…œì„ êµ¬í˜„í•©ë‹ˆë‹¤. ì´ëŠ” ê¸°ì¡´ ê°œë³„ ëª¨ë“ˆë“¤ì„ ëŒ€ì²´í•˜ëŠ” í†µí•©ëœ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.

## ğŸ¯ ì‘ì—… ëª©í‘œ
- ëª¨ë“  RAG êµ¬ì„±ìš”ì†Œì˜ ì™„ì „í•œ í†µí•©
- í†µí•©ëœ API ì¸í„°í˜ì´ìŠ¤ ì œê³µ
- ì›Œí¬í”Œë¡œìš° ìµœì í™” ë° ì„±ëŠ¥ í–¥ìƒ
- ê¸°ì¡´ features ëª¨ë“ˆë“¤ì˜ ê¸°ëŠ¥ì„ í•˜ë‚˜ë¡œ í†µí•©

## ğŸ”— ì˜ì¡´ì„±
- **ì„ í–‰ Task**: Task 12 (Hybrid Retriever), Task 13 (PromptTemplate), Task 14 (LLMChain)
- **ëŒ€ì²´í•  ê¸°ì¡´ ì½”ë“œ**: `app/features/generation`, `app/features/search`, `app/features/prompts`

## ğŸ”§ êµ¬í˜„ ì‚¬í•­

### 1. í†µí•© HybridRAG ì„œë¹„ìŠ¤

```python
# app/features/hybrid_rag/service.py
from typing import List, Dict, Any, Optional, AsyncGenerator
from pydantic import BaseModel
import asyncio
import time
import logging

from app.retriever.hybrid_retriever import HybridRetrievalService
from app.index.vector_index import VectorIndexService, VectorIndexConfig
from app.index.bm25_index import BM25IndexService, BM25IndexConfig
from app.llm.llm_service import LLMService, LLMConfig
from app.llm.prompt_service import PromptService, PromptConfig, PromptType
from app.retriever.document_builder import DocumentService, EnhancedDocument

logger = logging.getLogger(__name__)

class RAGRequest(BaseModel):
    """RAG ìš”ì²­ ëª¨ë¸"""
    query: str
    language: str = "java"
    task_type: PromptType = PromptType.GENERAL_QA
    max_results: int = 5
    use_cache: bool = True
    include_metadata: bool = True
    
    # ê²€ìƒ‰ ê´€ë ¨ ì„¤ì •
    vector_weight: Optional[float] = None
    bm25_weight: Optional[float] = None
    use_rrf: Optional[bool] = None
    
    # ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨
    additional_context: Optional[Dict[str, Any]] = None
    
    # ìŠ¤íŠ¸ë¦¬ë° ì„¤ì •
    streaming: bool = False

class RAGResponse(BaseModel):
    """RAG ì‘ë‹µ ëª¨ë¸"""
    answer: str
    sources: List[Dict[str, Any]]
    metadata: Dict[str, Any]
    processing_time_ms: int
    search_time_ms: int
    generation_time_ms: int
    cached: bool = False

class HybridRAGService:
    """í†µí•© í•˜ì´ë¸Œë¦¬ë“œ RAG ì„œë¹„ìŠ¤"""
    
    def __init__(
        self,
        vector_config: VectorIndexConfig = None,
        bm25_config: BM25IndexConfig = None,
        llm_config: LLMConfig = None,
        prompt_config: PromptConfig = None
    ):
        # ê°œë³„ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        self.vector_service = VectorIndexService(vector_config)
        self.bm25_service = BM25IndexService(bm25_config)
        self.llm_service = LLMService(llm_config, PromptService(prompt_config))
        self.document_service = DocumentService(None)  # embedding_clientëŠ” ë‚˜ì¤‘ì— ì„¤ì •
        
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„œë¹„ìŠ¤
        self.hybrid_retriever = None
        
        # í†µê³„ ë° ìºì‹±
        self._request_stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "average_processing_time": 0,
            "cache_hits": 0
        }
        self._response_cache = {}
        self._initialized = False
    
    async def initialize(self):
        """ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        if self._initialized:
            return
        
        logger.info("HybridRAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹œì‘...")
        
        try:
            # ê°œë³„ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
            await self.vector_service.initialize()
            await self.bm25_service.initialize()
            
            # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„œë¹„ìŠ¤ ìƒì„±
            self.hybrid_retriever = HybridRetrievalService(
                vector_index=self.vector_service.index,
                bm25_index=self.bm25_service.index
            )
            await self.hybrid_retriever.setup()
            
            self._initialized = True
            logger.info("HybridRAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"HybridRAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    async def process_query(self, request: RAGRequest) -> RAGResponse:
        """ì¿¼ë¦¬ ì²˜ë¦¬ - ë©”ì¸ RAG ì›Œí¬í”Œë¡œìš°"""
        await self.initialize()
        
        start_time = time.time()
        cache_key = self._generate_cache_key(request)
        
        # ìºì‹œ í™•ì¸
        if request.use_cache and cache_key in self._response_cache:
            cached_response = self._response_cache[cache_key]
            cached_response.cached = True
            self._request_stats["cache_hits"] += 1
            logger.info(f"ìºì‹œëœ ì‘ë‹µ ë°˜í™˜: {cache_key[:16]}...")
            return cached_response
        
        try:
            # 1. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰
            search_start = time.time()
            
            if any([request.vector_weight, request.bm25_weight, request.use_rrf]):
                # ìƒì„¸ ê²€ìƒ‰ ì˜µì…˜ì´ ìˆëŠ” ê²½ìš°
                search_results = await self.hybrid_retriever.search_with_detailed_scores(
                    query=request.query,
                    limit=request.max_results,
                    vector_weight=request.vector_weight,
                    bm25_weight=request.bm25_weight,
                    use_rrf=request.use_rrf
                )
                retrieved_docs = search_results['results']
                search_metadata = search_results['search_metadata']
            else:
                # ê¸°ë³¸ ê²€ìƒ‰
                retrieved_docs = await self.hybrid_retriever.retrieve(
                    query=request.query,
                    limit=request.max_results
                )
                search_metadata = {}
            
            search_end = time.time()
            search_time = int((search_end - search_start) * 1000)
            
            # 2. ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
            code_contexts = self._prepare_code_contexts(retrieved_docs, request)
            
            # 3. LLM ì‘ë‹µ ìƒì„±
            generation_start = time.time()
            
            llm_response = await self._generate_llm_response(
                request, code_contexts
            )
            
            generation_end = time.time()
            generation_time = int((generation_end - generation_start) * 1000)
            
            # 4. ì‘ë‹µ êµ¬ì„±
            end_time = time.time()
            total_time = int((end_time - start_time) * 1000)
            
            response = RAGResponse(
                answer=llm_response.content,
                sources=self._format_sources(retrieved_docs),
                metadata={
                    "search_metadata": search_metadata,
                    "llm_metadata": llm_response.metadata.dict(),
                    "request_type": request.task_type.value,
                    "language": request.language,
                    "sources_count": len(retrieved_docs)
                },
                processing_time_ms=total_time,
                search_time_ms=search_time,
                generation_time_ms=generation_time
            )
            
            # ìºì‹œ ì €ì¥
            if request.use_cache:
                self._response_cache[cache_key] = response
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self._update_stats(response, success=True)
            
            logger.info(f"RAG ì¿¼ë¦¬ ì²˜ë¦¬ ì™„ë£Œ: {total_time}ms (ê²€ìƒ‰: {search_time}ms, ìƒì„±: {generation_time}ms)")
            return response
            
        except Exception as e:
            logger.error(f"RAG ì¿¼ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            self._update_stats(None, success=False)
            raise
    
    async def process_streaming_query(
        self, 
        request: RAGRequest
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """ìŠ¤íŠ¸ë¦¬ë° ì¿¼ë¦¬ ì²˜ë¦¬"""
        await self.initialize()
        
        start_time = time.time()
        
        try:
            # 1. ê²€ìƒ‰ ì‹¤í–‰
            search_start = time.time()
            retrieved_docs = await self.hybrid_retriever.retrieve(
                query=request.query,
                limit=request.max_results
            )
            search_end = time.time()
            search_time = int((search_end - search_start) * 1000)
            
            # ê²€ìƒ‰ ê²°ê³¼ ë¨¼ì € ì „ì†¡
            yield {
                "type": "search_results",
                "data": {
                    "sources": self._format_sources(retrieved_docs),
                    "search_time_ms": search_time
                }
            }
            
            # 2. ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
            code_contexts = self._prepare_code_contexts(retrieved_docs, request)
            
            # 3. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
            generation_start = time.time()
            
            async for chunk in self.llm_service.generate_streaming_response(
                request.task_type,
                question=request.query,
                code_contexts=code_contexts,
                language=request.language,
                context_data=request.additional_context
            ):
                yield {
                    "type": "content_chunk",
                    "data": {"content": chunk}
                }
            
            generation_end = time.time()
            generation_time = int((generation_end - generation_start) * 1000)
            
            # ìµœì¢… ë©”íƒ€ë°ì´í„° ì „ì†¡
            end_time = time.time()
            total_time = int((end_time - start_time) * 1000)
            
            yield {
                "type": "completion",
                "data": {
                    "processing_time_ms": total_time,
                    "search_time_ms": search_time,
                    "generation_time_ms": generation_time,
                    "sources_count": len(retrieved_docs)
                }
            }
            
        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° RAG ì¿¼ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            yield {
                "type": "error",
                "data": {"error": str(e)}
            }
    
    async def index_documents(
        self, 
        documents: List[EnhancedDocument]
    ) -> Dict[str, Any]:
        """ë¬¸ì„œ ì¸ë±ì‹±"""
        await self.initialize()
        
        try:
            # ë³‘ë ¬ë¡œ Vectorì™€ BM25 ì¸ë±ìŠ¤ì— ì¶”ê°€
            vector_task = self.vector_service.index_documents(documents)
            bm25_task = self.bm25_service.index_documents(documents)
            
            vector_result, bm25_result = await asyncio.gather(
                vector_task, bm25_task, return_exceptions=True
            )
            
            return {
                "success": True,
                "indexed_count": len(documents),
                "vector_result": vector_result if not isinstance(vector_result, Exception) else str(vector_result),
                "bm25_result": bm25_result if not isinstance(bm25_result, Exception) else str(bm25_result)
            }
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì¸ë±ì‹± ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "indexed_count": 0
            }
    
    async def _generate_llm_response(
        self, 
        request: RAGRequest, 
        code_contexts: List[Dict[str, Any]]
    ):
        """LLM ì‘ë‹µ ìƒì„±"""
        if request.task_type == PromptType.CODE_EXPLANATION:
            # ì²« ë²ˆì§¸ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì£¼ ì½”ë“œë¡œ ì‚¬ìš©
            if code_contexts:
                main_context = code_contexts[0]
                return await self.llm_service.explain_code(
                    code_content=main_context.get('content', ''),
                    language=request.language,
                    question=request.query,
                    context_data={
                        **main_context,
                        'related_code': [ctx.get('content', '') for ctx in code_contexts[1:3]]
                    },
                    use_cache=request.use_cache
                )
        
        elif request.task_type == PromptType.CODE_GENERATION:
            return await self.llm_service.generate_code(
                requirements=request.query,
                language=request.language,
                related_code=[ctx.get('content', '') for ctx in code_contexts[:3]],
                context_data=request.additional_context,
                use_cache=request.use_cache
            )
        
        elif request.task_type == PromptType.CODE_REVIEW:
            if code_contexts:
                main_context = code_contexts[0]
                return await self.llm_service.review_code(
                    code_content=main_context.get('content', ''),
                    language=request.language,
                    context_data=main_context,
                    use_cache=request.use_cache
                )
        
        elif request.task_type == PromptType.DEBUGGING_HELP:
            if code_contexts:
                main_context = code_contexts[0]
                return await self.llm_service.help_debug(
                    code_content=main_context.get('content', ''),
                    language=request.language,
                    problem_description=request.query,
                    context_data=main_context,
                    use_cache=request.use_cache
                )
        
        # ê¸°ë³¸ì ìœ¼ë¡œ ì¼ë°˜ QAë¡œ ì²˜ë¦¬
        return await self.llm_service.answer_question(
            question=request.query,
            code_contexts=code_contexts,
            language=request.language,
            use_cache=request.use_cache
        )
    
    def _prepare_code_contexts(
        self, 
        retrieved_docs: List[Any], 
        request: RAGRequest
    ) -> List[Dict[str, Any]]:
        """ì½”ë“œ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„"""
        code_contexts = []
        
        for doc in retrieved_docs:
            if hasattr(doc, 'content'):
                context = {
                    'content': doc.content,
                    'file_path': doc.metadata.get('file_path', ''),
                    'language': doc.metadata.get('language', request.language),
                    'function_name': doc.metadata.get('name', ''),
                    'class_name': doc.metadata.get('parent_class', ''),
                    'score': getattr(doc, 'score', 0.0)
                }
                
                if request.include_metadata:
                    context['metadata'] = doc.metadata
                
                code_contexts.append(context)
        
        return code_contexts
    
    def _format_sources(self, retrieved_docs: List[Any]) -> List[Dict[str, Any]]:
        """ì†ŒìŠ¤ í¬ë§·íŒ…"""
        sources = []
        
        for i, doc in enumerate(retrieved_docs):
            source = {
                'rank': i + 1,
                'file_path': doc.metadata.get('file_path', f'ë¬¸ì„œ {i+1}'),
                'function_name': doc.metadata.get('name', ''),
                'language': doc.metadata.get('language', ''),
                'score': getattr(doc, 'score', 0.0),
                'content_preview': doc.content[:200] + "..." if len(doc.content) > 200 else doc.content
            }
            sources.append(source)
        
        return sources
    
    def _generate_cache_key(self, request: RAGRequest) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        import hashlib
        
        cache_data = {
            "query": request.query,
            "language": request.language,
            "task_type": request.task_type.value,
            "max_results": request.max_results,
            "vector_weight": request.vector_weight,
            "bm25_weight": request.bm25_weight,
            "use_rrf": request.use_rrf
        }
        
        cache_string = str(sorted(cache_data.items()))
        return hashlib.md5(cache_string.encode()).hexdigest()
    
    def _update_stats(self, response: Optional[RAGResponse], success: bool):
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        self._request_stats["total_requests"] += 1
        
        if success:
            self._request_stats["successful_requests"] += 1
            if response:
                # ì´ë™ í‰ê· ìœ¼ë¡œ í‰ê·  ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
                current_avg = self._request_stats["average_processing_time"]
                new_time = response.processing_time_ms
                total_requests = self._request_stats["total_requests"]
                
                self._request_stats["average_processing_time"] = (
                    (current_avg * (total_requests - 1) + new_time) / total_requests
                )
        else:
            self._request_stats["failed_requests"] += 1
    
    async def get_service_stats(self) -> Dict[str, Any]:
        """ì„œë¹„ìŠ¤ í†µê³„"""
        # ê°œë³„ ì„œë¹„ìŠ¤ í†µê³„ ìˆ˜ì§‘
        vector_stats = await self.vector_service.get_collection_stats()
        bm25_stats = await self.bm25_service.get_index_stats()
        llm_stats = await self.llm_service.get_service_stats()
        
        return {
            "hybrid_rag_stats": self._request_stats,
            "vector_index_stats": vector_stats,
            "bm25_index_stats": bm25_stats,
            "llm_service_stats": llm_stats,
            "cache_stats": {
                "cache_size": len(self._response_cache),
                "cache_hit_rate": (
                    self._request_stats["cache_hits"] / max(self._request_stats["total_requests"], 1)
                ) * 100
            }
        }
    
    async def health_check(self) -> Dict[str, Any]:
        """í—¬ìŠ¤ ì²´í¬"""
        health_status = {
            "status": "healthy",
            "components": {}
        }
        
        try:
            # ê°œë³„ ì»´í¬ë„ŒíŠ¸ í—¬ìŠ¤ ì²´í¬
            vector_health = await self.vector_service.health_check()
            bm25_health = await self.bm25_service.health_check()
            llm_health = await self.llm_service.health_check()
            
            health_status["components"] = {
                "vector_index": vector_health,
                "bm25_index": bm25_health,
                "llm_service": llm_health,
                "hybrid_retriever": {
                    "status": "healthy" if self._initialized else "initializing"
                }
            }
            
            # ì „ì²´ ìƒíƒœ ê²°ì •
            all_healthy = all(
                comp.get("status") == "healthy" 
                for comp in health_status["components"].values()
            )
            
            if not all_healthy:
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["error"] = str(e)
        
        return health_status
    
    async def clear_cache(self):
        """ìºì‹œ í´ë¦¬ì–´"""
        self._response_cache.clear()
        await self.llm_service.llm_chain.clear_cache()
        logger.info("ëª¨ë“  ìºì‹œê°€ í´ë¦¬ì–´ë˜ì—ˆìŠµë‹ˆë‹¤")
```

### 2. RAG ì›Œí¬í”Œë¡œìš° ìµœì í™”

```python
# app/features/hybrid_rag/optimizer.py
from typing import Dict, Any, List, Optional
import asyncio
import time
from .service import HybridRAGService, RAGRequest, PromptType

class RAGWorkflowOptimizer:
    """RAG ì›Œí¬í”Œë¡œìš° ìµœì í™”"""
    
    def __init__(self, rag_service: HybridRAGService):
        self.rag_service = rag_service
        self.optimization_history = []
    
    async def optimize_for_query_type(
        self, 
        query_type: PromptType,
        sample_queries: List[str],
        optimization_rounds: int = 5
    ) -> Dict[str, Any]:
        """ì¿¼ë¦¬ íƒ€ì…ë³„ ìµœì í™”"""
        best_config = None
        best_avg_time = float('inf')
        
        # í…ŒìŠ¤íŠ¸í•  ì„¤ì •ë“¤
        test_configs = [
            {"vector_weight": 0.7, "bm25_weight": 0.3, "use_rrf": False},
            {"vector_weight": 0.6, "bm25_weight": 0.4, "use_rrf": False},
            {"vector_weight": 0.8, "bm25_weight": 0.2, "use_rrf": False},
            {"use_rrf": True, "rrf_k": 60},
            {"use_rrf": True, "rrf_k": 30},
            {"use_rrf": True, "rrf_k": 90},
        ]
        
        results = []
        
        for config in test_configs:
            times = []
            errors = 0
            
            for query in sample_queries[:min(len(sample_queries), 10)]:  # ìµœëŒ€ 10ê°œ ì¿¼ë¦¬ë§Œ
                try:
                    request = RAGRequest(
                        query=query,
                        task_type=query_type,
                        use_cache=False,  # ìºì‹œ ë¹„í™œì„±í™”ë¡œ ì •í™•í•œ ì¸¡ì •
                        **config
                    )
                    
                    start_time = time.time()
                    response = await self.rag_service.process_query(request)
                    end_time = time.time()
                    
                    times.append((end_time - start_time) * 1000)
                    
                except Exception as e:
                    errors += 1
                    print(f"ìµœì í™” ì¤‘ ì˜¤ë¥˜: {e}")
            
            if times:
                avg_time = sum(times) / len(times)
                config_result = {
                    "config": config,
                    "avg_time_ms": avg_time,
                    "min_time_ms": min(times),
                    "max_time_ms": max(times),
                    "error_count": errors,
                    "success_rate": (len(times) / (len(times) + errors)) * 100
                }
                results.append(config_result)
                
                if avg_time < best_avg_time and errors == 0:
                    best_avg_time = avg_time
                    best_config = config
        
        return {
            "query_type": query_type.value,
            "best_config": best_config,
            "best_avg_time_ms": best_avg_time,
            "all_results": results,
            "optimization_summary": {
                "total_configs_tested": len(test_configs),
                "successful_configs": len([r for r in results if r["error_count"] == 0]),
                "improvement_percentage": self._calculate_improvement(results)
            }
        }
    
    def _calculate_improvement(self, results: List[Dict[str, Any]]) -> float:
        """ê°œì„ ìœ¨ ê³„ì‚°"""
        if len(results) < 2:
            return 0.0
        
        times = [r["avg_time_ms"] for r in results if r["error_count"] == 0]
        if len(times) < 2:
            return 0.0
        
        worst_time = max(times)
        best_time = min(times)
        
        return ((worst_time - best_time) / worst_time) * 100
```

## âœ… ì™„ë£Œ ì¡°ê±´

1. **í†µí•© ì„œë¹„ìŠ¤ êµ¬í˜„**: ëª¨ë“  RAG êµ¬ì„±ìš”ì†Œê°€ ì™„ì „íˆ í†µí•©ë¨
2. **ì›Œí¬í”Œë¡œìš° ìµœì í™”**: ê²€ìƒ‰-ìƒì„± íŒŒì´í”„ë¼ì¸ì´ íš¨ìœ¨ì ìœ¼ë¡œ ë™ì‘í•¨
3. **ìŠ¤íŠ¸ë¦¬ë° ì§€ì›**: ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì œê³µ
4. **ìºì‹± ì‹œìŠ¤í…œ**: íš¨ê³¼ì ì¸ ì‘ë‹µ ìºì‹±ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
5. **ëª¨ë‹ˆí„°ë§**: ìƒì„¸í•œ ì„±ëŠ¥ ë° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
6. **í—¬ìŠ¤ ì²´í¬**: ëª¨ë“  êµ¬ì„±ìš”ì†Œì˜ ìƒíƒœ í™•ì¸ ê°€ëŠ¥
7. **ìµœì í™” ë„êµ¬**: ì›Œí¬í”Œë¡œìš° ì„±ëŠ¥ ìµœì í™” ì§€ì›

## ğŸ“‹ ë‹¤ìŒ Taskì™€ì˜ ì—°ê´€ê´€ê³„

- **Task 16**: ì´ í†µí•© ì„œë¹„ìŠ¤ë¥¼ ìœ„í•œ í†µí•© API ì—”ë“œí¬ì¸íŠ¸ êµ¬í˜„

## ğŸ§ª í…ŒìŠ¤íŠ¸ ê³„íš

```python
# tests/unit/features/hybrid_rag/test_service.py
async def test_hybrid_rag_initialization():
    """HybridRAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
    service = HybridRAGService()
    await service.initialize()
    assert service._initialized is True

async def test_process_query():
    """ì¿¼ë¦¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
    service = HybridRAGService()
    request = RAGRequest(
        query="JWT ì¸ì¦ êµ¬í˜„ ë°©ë²•",
        task_type=PromptType.GENERAL_QA
    )
    response = await service.process_query(request)
    assert response.answer != ""
    assert len(response.sources) > 0

async def test_streaming_query():
    """ìŠ¤íŠ¸ë¦¬ë° ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸"""
    service = HybridRAGService()
    request = RAGRequest(
        query="ì½”ë“œ ë¦¬ë·° ìš”ì²­",
        task_type=PromptType.CODE_REVIEW,
        streaming=True
    )
    
    chunks = []
    async for chunk in service.process_streaming_query(request):
        chunks.append(chunk)
    
    assert len(chunks) > 0
    assert any(chunk["type"] == "search_results" for chunk in chunks)

async def test_document_indexing():
    """ë¬¸ì„œ ì¸ë±ì‹± í…ŒìŠ¤íŠ¸"""
    service = HybridRAGService()
    documents = [create_sample_enhanced_document()]
    result = await service.index_documents(documents)
    assert result["success"] is True
```

ì´ TaskëŠ” ì „ì²´ RAG ì‹œìŠ¤í…œì˜ í•µì‹¬ìœ¼ë¡œ, ëª¨ë“  êµ¬ì„±ìš”ì†Œë¥¼ í†µí•©í•˜ì—¬ ì™„ì „í•œ ì½”ë“œ ê²€ìƒ‰ ë° ìƒì„± ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë¶„ì‚°ëœ features ëª¨ë“ˆë“¤ì„ í•˜ë‚˜ì˜ í†µí•©ëœ ì„œë¹„ìŠ¤ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤. 