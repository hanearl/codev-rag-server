# Task 9: Document Builder êµ¬í˜„

## ğŸ“‹ ì‘ì—… ê°œìš”
AST íŒŒì„œì—ì„œ ìƒì„±ëœ ì½”ë“œ êµ¬ì¡°ë¥¼ LlamaIndex Documentì™€ TextNodeë¡œ ë³€í™˜í•˜ê³ , ì„ë² ë”©ê³¼ ê²€ìƒ‰ì— ìµœì í™”ëœ í˜•íƒœë¡œ êµ¬ì¡°í™”í•˜ëŠ” Document Builderë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

## ğŸ¯ ì‘ì—… ëª©í‘œ
- LlamaIndex Document/TextNode ìƒì„± ìµœì í™”
- ì½”ë“œ êµ¬ì¡°í™” ë° ì²­í‚¹ ì „ëµ êµ¬í˜„
- ë©”íƒ€ë°ì´í„° ê°•í™” ë° ê²€ìƒ‰ ìµœì í™”
- ê¸°ì¡´ indexing ëª¨ë“ˆì˜ ê¸°ëŠ¥ì„ LlamaIndex ê¸°ë°˜ìœ¼ë¡œ í†µí•©

## ğŸ”— ì˜ì¡´ì„±
- **ì„ í–‰ Task**: Task 8 (LlamaIndex ê¸°ë°˜ AST íŒŒì„œ êµ¬í˜„)
- **í™œìš©í•  ê¸°ì¡´ ì½”ë“œ**: `app/features/indexing/service.py`ì˜ ë¬¸ì„œ ìƒì„± ë¡œì§

## ğŸ”§ êµ¬í˜„ ì‚¬í•­

### 1. Document Builder ì¸í„°í˜ì´ìŠ¤

```python
# app/retriever/document_builder.py
from typing import List, Dict, Any, Optional, Union
from llama_index.core import Document
from llama_index.core.schema import TextNode, MetadataMode
from llama_index.core.node_parser import SimpleNodeParser, CodeSplitter
from pydantic import BaseModel
from enum import Enum
from .ast_parser import ParseResult, CodeMetadata, Language, CodeType
import hashlib

class ChunkingStrategy(str, Enum):
    """ì²­í‚¹ ì „ëµ"""
    METHOD_LEVEL = "method_level"      # ë©”ì„œë“œ/í•¨ìˆ˜ ë‹¨ìœ„
    CLASS_LEVEL = "class_level"        # í´ë˜ìŠ¤ ë‹¨ìœ„  
    FILE_LEVEL = "file_level"          # íŒŒì¼ ë‹¨ìœ„
    SEMANTIC = "semantic"              # ì˜ë¯¸ì  ë‹¨ìœ„
    HYBRID = "hybrid"                  # í•˜ì´ë¸Œë¦¬ë“œ

class DocumentBuildConfig(BaseModel):
    """Document ë¹Œë“œ ì„¤ì •"""
    chunking_strategy: ChunkingStrategy = ChunkingStrategy.METHOD_LEVEL
    max_chunk_size: int = 1024
    chunk_overlap: int = 100
    include_metadata: bool = True
    enhance_keywords: bool = True
    add_context: bool = True
    preserve_structure: bool = True

class EnhancedDocument(BaseModel):
    """ê°•í™”ëœ ë¬¸ì„œ ëª¨ë¸"""
    document: Document
    text_node: TextNode
    metadata: CodeMetadata
    enhanced_content: str
    search_keywords: List[str]
    semantic_tags: List[str]
    relationships: Dict[str, Any]

class DocumentBuildResult(BaseModel):
    """Document ë¹Œë“œ ê²°ê³¼"""
    documents: List[EnhancedDocument]
    total_documents: int
    build_time_ms: int
    metadata: Dict[str, Any]
    statistics: Dict[str, int]

class DocumentBuilder:
    """LlamaIndex Document Builder"""
    
    def __init__(self, config: DocumentBuildConfig = None):
        self.config = config or DocumentBuildConfig()
        self.node_parser = self._create_node_parser()
    
    async def build_from_parse_result(self, parse_result: ParseResult) -> DocumentBuildResult:
        """íŒŒì‹± ê²°ê³¼ë¡œë¶€í„° ê°•í™”ëœ Document ìƒì„±"""
        import time
        start_time = time.time()
        
        enhanced_docs = []
        statistics = {
            "classes": 0,
            "methods": 0,
            "functions": 0,
            "interfaces": 0,
            "total_lines": 0
        }
        
        for doc in parse_result.documents:
            enhanced_doc = await self._enhance_document(doc)
            enhanced_docs.append(enhanced_doc)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self._update_statistics(statistics, enhanced_doc.metadata)
        
        end_time = time.time()
        
        return DocumentBuildResult(
            documents=enhanced_docs,
            total_documents=len(enhanced_docs),
            build_time_ms=int((end_time - start_time) * 1000),
            metadata=parse_result.metadata,
            statistics=statistics
        )
    
    async def build_from_legacy_chunks(self, chunks: List[Dict[str, Any]]) -> DocumentBuildResult:
        """ê¸°ì¡´ indexing ëª¨ë“ˆì˜ ì²­í¬ë¡œë¶€í„° Document ìƒì„±"""
        import time
        start_time = time.time()
        
        enhanced_docs = []
        statistics = {"legacy_chunks": len(chunks)}
        
        for chunk in chunks:
            # ê¸°ì¡´ ì²­í¬ë¥¼ CodeMetadataë¡œ ë³€í™˜
            metadata = self._convert_legacy_chunk_to_metadata(chunk)
            
            # Document ìƒì„±
            document = Document(
                text=chunk.get("code_content", ""),
                metadata=metadata.dict(),
                id_=self._generate_document_id(metadata)
            )
            
            # TextNode ìƒì„±
            text_node = TextNode(
                text=chunk.get("code_content", ""),
                metadata=metadata.dict(),
                id_=self._generate_document_id(metadata)
            )
            
            # ê°•í™”ëœ ë¬¸ì„œ ìƒì„±
            enhanced_doc = EnhancedDocument(
                document=document,
                text_node=text_node,
                metadata=metadata,
                enhanced_content=await self._enhance_content(chunk.get("code_content", ""), metadata),
                search_keywords=await self._extract_search_keywords(chunk.get("code_content", ""), metadata),
                semantic_tags=await self._generate_semantic_tags(metadata),
                relationships=await self._analyze_relationships(metadata)
            )
            
            enhanced_docs.append(enhanced_doc)
        
        end_time = time.time()
        
        return DocumentBuildResult(
            documents=enhanced_docs,
            total_documents=len(enhanced_docs),
            build_time_ms=int((end_time - start_time) * 1000),
            metadata={"source": "legacy_chunks"},
            statistics=statistics
        )
    
    async def _enhance_document(self, document: Document) -> EnhancedDocument:
        """Document ê°•í™”"""
        # ë©”íƒ€ë°ì´í„°ì—ì„œ CodeMetadata ë³µì›
        metadata = CodeMetadata(**document.metadata)
        
        # ì»¨í…ì¸  ê°•í™”
        enhanced_content = await self._enhance_content(document.text, metadata)
        
        # ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ
        search_keywords = await self._extract_search_keywords(document.text, metadata)
        
        # ì˜ë¯¸ì  íƒœê·¸ ìƒì„±
        semantic_tags = await self._generate_semantic_tags(metadata)
        
        # ê´€ê³„ ë¶„ì„
        relationships = await self._analyze_relationships(metadata)
        
        # TextNode ìƒì„±
        text_node = TextNode(
            text=enhanced_content,
            metadata=document.metadata,
            id_=document.id_
        )
        
        return EnhancedDocument(
            document=document,
            text_node=text_node,
            metadata=metadata,
            enhanced_content=enhanced_content,
            search_keywords=search_keywords,
            semantic_tags=semantic_tags,
            relationships=relationships
        )
    
    async def _enhance_content(self, content: str, metadata: CodeMetadata) -> str:
        """ì»¨í…ì¸  ê°•í™”"""
        enhanced_parts = []
        
        # ê¸°ë³¸ ì •ë³´ ì¶”ê°€
        enhanced_parts.append(f"# {metadata.code_type.value.title()}: {metadata.name}")
        
        # íŒŒì¼ ê²½ë¡œ ì •ë³´
        enhanced_parts.append(f"File: {metadata.file_path}")
        
        # ì–¸ì–´ ì •ë³´
        enhanced_parts.append(f"Language: {metadata.language.value}")
        
        # ë¼ì¸ ì •ë³´
        enhanced_parts.append(f"Lines: {metadata.line_start}-{metadata.line_end}")
        
        # ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        if metadata.parent_class:
            enhanced_parts.append(f"Parent Class: {metadata.parent_class}")
        
        if metadata.namespace:
            enhanced_parts.append(f"Namespace: {metadata.namespace}")
        
        if metadata.extends:
            enhanced_parts.append(f"Extends: {metadata.extends}")
        
        if metadata.implements:
            enhanced_parts.append(f"Implements: {', '.join(metadata.implements)}")
        
        if metadata.parameters:
            params = [f"{p['name']}: {p.get('type', 'unknown')}" for p in metadata.parameters]
            enhanced_parts.append(f"Parameters: {', '.join(params)}")
        
        if metadata.return_type:
            enhanced_parts.append(f"Returns: {metadata.return_type}")
        
        # ì£¼ì„/ë…ìŠ¤íŠ¸ë§ ì¶”ê°€
        if metadata.comments:
            enhanced_parts.append(f"Comments: {metadata.comments}")
        
        if metadata.docstring:
            enhanced_parts.append(f"Documentation: {metadata.docstring}")
        
        # í‚¤ì›Œë“œ ì¶”ê°€
        if metadata.keywords:
            enhanced_parts.append(f"Keywords: {', '.join(metadata.keywords)}")
        
        # ì›ë³¸ ì½”ë“œ
        enhanced_parts.append("\n## Code:")
        enhanced_parts.append(content)
        
        return "\n".join(enhanced_parts)
    
    async def _extract_search_keywords(self, content: str, metadata: CodeMetadata) -> List[str]:
        """ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = set()
        
        # ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í‚¤ì›Œë“œ
        keywords.add(metadata.name)
        keywords.update(metadata.keywords)
        
        if metadata.parent_class:
            keywords.add(metadata.parent_class)
        
        if metadata.namespace:
            keywords.update(metadata.namespace.split('.'))
        
        if metadata.extends:
            keywords.add(metadata.extends)
        
        keywords.update(metadata.implements)
        
        # íŒŒë¼ë¯¸í„° íƒ€ì… í‚¤ì›Œë“œ
        for param in metadata.parameters:
            if param.get('type'):
                keywords.add(param['type'])
        
        if metadata.return_type:
            keywords.add(metadata.return_type)
        
        # ì–´ë…¸í…Œì´ì…˜/ë°ì½”ë ˆì´í„°
        keywords.update(metadata.annotations)
        
        # ìˆ˜ì •ì
        keywords.update(metadata.modifiers)
        
        # ì½”ë“œ ë‚´ìš©ì—ì„œ ì¶”ê°€ í‚¤ì›Œë“œ ì¶”ì¶œ
        content_keywords = await self._extract_keywords_from_content(content, metadata.language)
        keywords.update(content_keywords)
        
        return list(keywords)
    
    async def _extract_keywords_from_content(self, content: str, language: Language) -> List[str]:
        """ì½”ë“œ ë‚´ìš©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = set()
        
        # ì–¸ì–´ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ ì „ëµ
        if language == Language.JAVA:
            # Java í‚¤ì›Œë“œ íŒ¨í„´
            java_patterns = [
                r'\b(String|Integer|Boolean|List|Map|Set)\b',
                r'\b(public|private|protected|static|final)\b',
                r'\b(class|interface|enum|extends|implements)\b',
                r'\b(@\w+)\b'  # ì–´ë…¸í…Œì´ì…˜
            ]
            for pattern in java_patterns:
                import re
                matches = re.findall(pattern, content)
                keywords.update(matches)
        
        elif language == Language.PYTHON:
            # Python í‚¤ì›Œë“œ íŒ¨í„´
            python_patterns = [
                r'\bdef\s+(\w+)',  # í•¨ìˆ˜ëª…
                r'\bclass\s+(\w+)',  # í´ë˜ìŠ¤ëª…
                r'\bimport\s+(\w+)',  # ì„í¬íŠ¸
                r'\bfrom\s+(\w+)',  # from ì„í¬íŠ¸
                r'@(\w+)'  # ë°ì½”ë ˆì´í„°
            ]
            for pattern in python_patterns:
                import re
                matches = re.findall(pattern, content)
                keywords.update(matches)
        
        return list(keywords)
    
    async def _generate_semantic_tags(self, metadata: CodeMetadata) -> List[str]:
        """ì˜ë¯¸ì  íƒœê·¸ ìƒì„±"""
        tags = []
        
        # ì½”ë“œ íƒ€ì… ê¸°ë°˜ íƒœê·¸
        tags.append(f"type:{metadata.code_type.value}")
        tags.append(f"lang:{metadata.language.value}")
        
        # ì ‘ê·¼ ì œí•œì íƒœê·¸
        if 'public' in metadata.modifiers:
            tags.append("access:public")
        elif 'private' in metadata.modifiers:
            tags.append("access:private")
        elif 'protected' in metadata.modifiers:
            tags.append("access:protected")
        
        # ì •ì  ì—¬ë¶€
        if 'static' in metadata.modifiers:
            tags.append("scope:static")
        else:
            tags.append("scope:instance")
        
        # ìƒì† ê´€ê³„
        if metadata.extends:
            tags.append("pattern:inheritance")
        
        if metadata.implements:
            tags.append("pattern:implementation")
        
        # ë³µì¡ë„ ê¸°ë°˜ íƒœê·¸
        if metadata.complexity_score > 0:
            if metadata.complexity_score > 10:
                tags.append("complexity:high")
            elif metadata.complexity_score > 5:
                tags.append("complexity:medium")
            else:
                tags.append("complexity:low")
        
        # íŠ¹ìˆ˜ íŒ¨í„´ ì¸ì‹
        if metadata.name.startswith("test"):
            tags.append("purpose:test")
        elif metadata.name.startswith("get"):
            tags.append("purpose:getter")
        elif metadata.name.startswith("set"):
            tags.append("purpose:setter")
        elif metadata.name.startswith("is") or metadata.name.startswith("has"):
            tags.append("purpose:predicate")
        
        return tags
    
    async def _analyze_relationships(self, metadata: CodeMetadata) -> Dict[str, Any]:
        """ê´€ê³„ ë¶„ì„"""
        relationships = {}
        
        # ìƒìœ„-í•˜ìœ„ ê´€ê³„
        if metadata.parent_class:
            relationships["parent"] = metadata.parent_class
        
        # ìƒì† ê´€ê³„
        if metadata.extends:
            relationships["extends"] = metadata.extends
        
        # êµ¬í˜„ ê´€ê³„
        if metadata.implements:
            relationships["implements"] = metadata.implements
        
        # ì˜ì¡´ì„± ê´€ê³„
        dependencies = []
        for param in metadata.parameters:
            if param.get('type'):
                dependencies.append(param['type'])
        
        if metadata.return_type:
            dependencies.append(metadata.return_type)
        
        if dependencies:
            relationships["dependencies"] = dependencies
        
        # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ê´€ê³„
        if metadata.namespace:
            relationships["namespace"] = metadata.namespace
        
        return relationships
    
    def _convert_legacy_chunk_to_metadata(self, chunk: Dict[str, Any]) -> CodeMetadata:
        """ê¸°ì¡´ ì²­í¬ë¥¼ CodeMetadataë¡œ ë³€í™˜"""
        # ì–¸ì–´ ë§¤í•‘
        language_map = {
            "java": Language.JAVA,
            "python": Language.PYTHON,
            "javascript": Language.JAVASCRIPT
        }
        
        # ì½”ë“œ íƒ€ì… ë§¤í•‘
        code_type_map = {
            "class": CodeType.CLASS,
            "method": CodeType.METHOD,
            "function": CodeType.FUNCTION,
            "interface": CodeType.INTERFACE
        }
        
        return CodeMetadata(
            file_path=chunk.get("file_path", ""),
            language=language_map.get(chunk.get("language", "").lower(), Language.JAVA),
            code_type=code_type_map.get(chunk.get("code_type", "").lower(), CodeType.METHOD),
            name=chunk.get("name", "unknown"),
            line_start=chunk.get("line_start", 1),
            line_end=chunk.get("line_end", 1),
            namespace=chunk.get("namespace"),
            parent_class=chunk.get("parent_class"),
            modifiers=chunk.get("modifiers", []),
            annotations=chunk.get("annotations", []),
            parameters=chunk.get("parameters", []),
            return_type=chunk.get("return_type"),
            extends=chunk.get("extends"),
            implements=chunk.get("implements", []),
            keywords=chunk.get("keywords", [])
        )
    
    def _generate_document_id(self, metadata: CodeMetadata) -> str:
        """Document ID ìƒì„±"""
        id_string = f"{metadata.file_path}:{metadata.name}:{metadata.line_start}:{metadata.line_end}"
        return hashlib.md5(id_string.encode()).hexdigest()
    
    def _create_node_parser(self) -> SimpleNodeParser:
        """Node Parser ìƒì„±"""
        if self.config.chunking_strategy == ChunkingStrategy.SEMANTIC:
            return CodeSplitter(
                language=self.config.chunking_strategy,
                chunk_size=self.config.max_chunk_size,
                chunk_overlap=self.config.chunk_overlap
            )
        else:
            return SimpleNodeParser(
                chunk_size=self.config.max_chunk_size,
                chunk_overlap=self.config.chunk_overlap
            )
    
    def _update_statistics(self, statistics: Dict[str, int], metadata: CodeMetadata):
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        if metadata.code_type == CodeType.CLASS:
            statistics["classes"] += 1
        elif metadata.code_type == CodeType.METHOD:
            statistics["methods"] += 1
        elif metadata.code_type == CodeType.FUNCTION:
            statistics["functions"] += 1
        elif metadata.code_type == CodeType.INTERFACE:
            statistics["interfaces"] += 1
        
        statistics["total_lines"] += (metadata.line_end - metadata.line_start + 1)
```

### 2. Document Builder ì„œë¹„ìŠ¤

```python
# app/retriever/document_service.py
from typing import List, Dict, Any, Optional
from .document_builder import DocumentBuilder, DocumentBuildConfig, DocumentBuildResult, EnhancedDocument
from .ast_parser import ParseResult
from app.core.clients import EmbeddingClient

class DocumentService:
    """Document ìƒì„± ì„œë¹„ìŠ¤"""
    
    def __init__(self, embedding_client: EmbeddingClient):
        self.embedding_client = embedding_client
        self.builder = DocumentBuilder()
    
    async def create_documents_from_parse_result(
        self, 
        parse_result: ParseResult,
        config: Optional[DocumentBuildConfig] = None
    ) -> DocumentBuildResult:
        """íŒŒì‹± ê²°ê³¼ë¡œë¶€í„° Document ìƒì„±"""
        if config:
            self.builder.config = config
        
        result = await self.builder.build_from_parse_result(parse_result)
        
        # ì„ë² ë”© ìƒì„± (ì˜µì…˜)
        if self.embedding_client:
            await self._generate_embeddings(result.documents)
        
        return result
    
    async def create_documents_from_legacy_chunks(
        self,
        chunks: List[Dict[str, Any]],
        config: Optional[DocumentBuildConfig] = None
    ) -> DocumentBuildResult:
        """ê¸°ì¡´ ì²­í¬ë¡œë¶€í„° Document ìƒì„±"""
        if config:
            self.builder.config = config
        
        result = await self.builder.build_from_legacy_chunks(chunks)
        
        # ì„ë² ë”© ìƒì„± (ì˜µì…˜)
        if self.embedding_client:
            await self._generate_embeddings(result.documents)
        
        return result
    
    async def _generate_embeddings(self, documents: List[EnhancedDocument]):
        """Documentë“¤ì— ëŒ€í•œ ì„ë² ë”© ìƒì„±"""
        texts = [doc.enhanced_content for doc in documents]
        
        try:
            embedding_response = await self.embedding_client.embed_bulk({"texts": texts})
            embeddings = [emb["embedding"] for emb in embedding_response["embeddings"]]
            
            # Documentì— ì„ë² ë”© ì¶”ê°€
            for doc, embedding in zip(documents, embeddings):
                doc.document.embedding = embedding
                doc.text_node.embedding = embedding
                
        except Exception as e:
            # ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ ì‹œ ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ê³„ì† ì§„í–‰
            import logging
            logger = logging.getLogger(__name__)
            logger.warning(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
```

## âœ… ì™„ë£Œ ì¡°ê±´

1. **Document Builder êµ¬í˜„**: ëª¨ë“  í•µì‹¬ ê¸°ëŠ¥ì´ êµ¬í˜„ë˜ê³  í…ŒìŠ¤íŠ¸ í†µê³¼
2. **ì»¨í…ì¸  ê°•í™”**: ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì»¨í…ì¸  ê°•í™”ê°€ ì •ìƒ ë™ì‘
3. **í‚¤ì›Œë“œ ì¶”ì¶œ**: ê²€ìƒ‰ ìµœì í™”ë¥¼ ìœ„í•œ í‚¤ì›Œë“œ ì¶”ì¶œì´ ì •í™•í•¨
4. **ì˜ë¯¸ì  íƒœê·¸ ìƒì„±**: ì˜ë¯¸ì  ë¶„ë¥˜ë¥¼ ìœ„í•œ íƒœê·¸ê°€ ìƒì„±ë¨
5. **ê´€ê³„ ë¶„ì„**: ì½”ë“œ ê°„ ê´€ê³„ê°€ ì •í™•íˆ ë¶„ì„ë¨
6. **ê¸°ì¡´ ì½”ë“œ í˜¸í™˜**: ê¸°ì¡´ indexing ëª¨ë“ˆì˜ ì²­í¬ì™€ í˜¸í™˜ë¨
7. **ì„±ëŠ¥ ìµœì í™”**: ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ ì‹œ ì„±ëŠ¥ì´ ì–‘í˜¸í•¨

## ğŸ“‹ ë‹¤ìŒ Taskì™€ì˜ ì—°ê´€ê´€ê³„

- **Task 10**: ì´ Taskì—ì„œ ìƒì„±í•œ EnhancedDocumentë“¤ì„ Vector Indexì— ì €ì¥
- **Task 11**: BM25 Indexì— í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ì„ ìœ„í•œ ë°ì´í„° ì €ì¥
- **Task 12**: Hybrid Retrieverì—ì„œ ê°•í™”ëœ ë©”íƒ€ë°ì´í„° í™œìš©

## ğŸ§ª í…ŒìŠ¤íŠ¸ ê³„íš

```python
# tests/unit/retriever/test_document_builder.py
async def test_build_from_parse_result():
    """íŒŒì‹± ê²°ê³¼ë¡œë¶€í„° Document ë¹Œë“œ í…ŒìŠ¤íŠ¸"""
    builder = DocumentBuilder()
    result = await builder.build_from_parse_result(sample_parse_result)
    assert len(result.documents) > 0
    assert result.total_documents > 0

async def test_enhance_content():
    """ì»¨í…ì¸  ê°•í™” í…ŒìŠ¤íŠ¸"""
    builder = DocumentBuilder()
    enhanced = await builder._enhance_content(sample_code, sample_metadata)
    assert "File:" in enhanced
    assert "Language:" in enhanced

async def test_extract_search_keywords():
    """ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸"""
    builder = DocumentBuilder()
    keywords = await builder._extract_search_keywords(sample_code, sample_metadata)
    assert len(keywords) > 0
    assert sample_metadata.name in keywords

async def test_legacy_chunk_conversion():
    """ê¸°ì¡´ ì²­í¬ ë³€í™˜ í…ŒìŠ¤íŠ¸"""
    builder = DocumentBuilder()
    result = await builder.build_from_legacy_chunks(sample_legacy_chunks)
    assert len(result.documents) == len(sample_legacy_chunks)
```

ì´ TaskëŠ” ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒì„ ìœ„í•œ í•µì‹¬ êµ¬ì„±ìš”ì†Œì…ë‹ˆë‹¤. í’ë¶€í•œ ë©”íƒ€ë°ì´í„°ì™€ ê°•í™”ëœ ì»¨í…ì¸ ë¥¼ í†µí•´ ë” ì •í™•í•œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 