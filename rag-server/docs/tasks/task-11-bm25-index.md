# Task 11: BM25 Index êµ¬í˜„

## ğŸ“‹ ì‘ì—… ê°œìš”
LlamaIndex BM25Retrieverë¥¼ í™œìš©í•˜ì—¬ í‚¤ì›Œë“œ ê¸°ë°˜ ì •ë°€ ê²€ìƒ‰ì„ ì§€ì›í•˜ëŠ” BM25 Indexë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤. ê¸°ì¡´ BM25 ìŠ¤ì½”ì–´ë§ ë¡œì§ì„ LlamaIndex ì•„í‚¤í…ì²˜ë¡œ í†µí•©í•©ë‹ˆë‹¤.

## ğŸ¯ ì‘ì—… ëª©í‘œ
- LlamaIndex BM25Retriever ê¸°ë°˜ í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬ì¶•
- ì½”ë“œ íŠ¹í™” í† í°í™” ë° ì „ì²˜ë¦¬ ìµœì í™”
- ê¸°ì¡´ BM25 ìŠ¤ì½”ì–´ë§ ë¡œì§ê³¼ì˜ í˜¸í™˜ì„± ë³´ì¥
- ë‹¤êµ­ì–´ ì½”ë“œ ì§€ì› ë° ì„±ëŠ¥ ìµœì í™”

## ğŸ”— ì˜ì¡´ì„±
- **ì„ í–‰ Task**: Task 9 (Document Builder êµ¬í˜„)
- **í™œìš©í•  ê¸°ì¡´ ì½”ë“œ**: `app/features/search/bm25_scorer.py`
- **ë³‘ë ¬ ê°œë°œ**: Task 10 (Vector Index êµ¬í˜„)

## ğŸ”§ êµ¬í˜„ ì‚¬í•­

### 1. BM25 Index ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„

```python
# app/index/bm25_index.py
from typing import List, Dict, Any, Optional, Union
from llama_index.retrievers.bm25 import BM25Retriever
from llama_index.core.schema import TextNode, NodeWithScore
from llama_index.core import Document
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import re
import json
import pickle
from pathlib import Path
from .base_index import BaseIndex, IndexedDocument
from app.retriever.document_builder import EnhancedDocument

# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ ì‹¤í–‰ ì‹œ)
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

class CodeTokenizer:
    """ì½”ë“œ íŠ¹í™” í† í¬ë‚˜ì´ì €"""
    
    def __init__(self, language: str = "english"):
        self.language = language
        self.stemmer = PorterStemmer()
        self.stop_words = set(stopwords.words(language))
        
        # ì½”ë“œ íŠ¹í™” ë¶ˆìš©ì–´ ì¶”ê°€
        self.code_stop_words = {
            'public', 'private', 'protected', 'static', 'final', 'void',
            'class', 'interface', 'extends', 'implements', 'import',
            'package', 'return', 'if', 'else', 'for', 'while', 'try',
            'catch', 'throw', 'throws', 'new', 'this', 'super'
        }
        self.stop_words.update(self.code_stop_words)
    
    def tokenize(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ í† í°í™”"""
        # ì½”ë“œ íŒ¨í„´ ì „ì²˜ë¦¬
        text = self._preprocess_code(text)
        
        # ê¸°ë³¸ í† í°í™”
        tokens = word_tokenize(text.lower())
        
        # í•„í„°ë§ ë° ìŠ¤í…Œë°
        filtered_tokens = []
        for token in tokens:
            # ë¶ˆìš©ì–´ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°
            if (len(token) > 1 and 
                token not in self.stop_words and
                token.isalnum()):
                
                # ìŠ¤í…Œë° ì ìš©
                stemmed = self.stemmer.stem(token)
                filtered_tokens.append(stemmed)
        
        return filtered_tokens
    
    def _preprocess_code(self, text: str) -> str:
        """ì½”ë“œ íŠ¹í™” ì „ì²˜ë¦¬"""
        # CamelCase ë¶„ë¦¬
        text = re.sub(r'([a-z])([A-Z])', r'\1 \2', text)
        
        # snake_case ë¶„ë¦¬
        text = re.sub(r'_', ' ', text)
        
        # íŠ¹ìˆ˜ ë¬¸ì ê³µë°±ìœ¼ë¡œ ì¹˜í™˜
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # ìˆ«ì ì œê±° (ì„ íƒì )
        # text = re.sub(r'\d+', '', text)
        
        # ì—°ì† ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()

class BM25IndexConfig:
    """BM25 Index ì„¤ì •"""
    def __init__(
        self,
        k1: float = 1.2,
        b: float = 0.75,
        top_k: int = 10,
        language: str = "english",
        index_path: str = "data/bm25_index",
        use_stemming: bool = True,
        include_metadata: bool = True,
        metadata_weight: float = 0.3  # ë©”íƒ€ë°ì´í„° ê°€ì¤‘ì¹˜
    ):
        self.k1 = k1
        self.b = b
        self.top_k = top_k
        self.language = language
        self.index_path = Path(index_path)
        self.use_stemming = use_stemming
        self.include_metadata = include_metadata
        self.metadata_weight = metadata_weight

class CodeBM25Index(BaseIndex):
    """ì½”ë“œ BM25 ì¸ë±ìŠ¤"""
    
    def __init__(self, config: BM25IndexConfig = None):
        self.config = config or BM25IndexConfig()
        self.tokenizer = CodeTokenizer(self.config.language)
        self.retriever = None
        self.nodes = []
        self.documents_map = {}  # ID -> EnhancedDocument ë§¤í•‘
        
        # ì¸ë±ìŠ¤ ì €ì¥ ê²½ë¡œ ìƒì„±
        self.config.index_path.mkdir(parents=True, exist_ok=True)
    
    async def setup(self):
        """ì¸ë±ìŠ¤ ì´ˆê¸°í™”"""
        # ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„
        if await self._load_existing_index():
            return
        
        # ìƒˆ ì¸ë±ìŠ¤ ì´ˆê¸°í™”
        self.nodes = []
        self.documents_map = {}
        self._build_retriever()
    
    async def add_documents(self, documents: List[Union[EnhancedDocument, Dict[str, Any]]]) -> List[str]:
        """ë¬¸ì„œ ì¶”ê°€"""
        added_ids = []
        new_nodes = []
        
        for doc in documents:
            if isinstance(doc, EnhancedDocument):
                # EnhancedDocument ì²˜ë¦¬
                enhanced_text = self._create_enhanced_text(doc)
                text_node = TextNode(
                    text=enhanced_text,
                    metadata=doc.metadata.dict(),
                    id_=doc.text_node.id_
                )
                
                new_nodes.append(text_node)
                self.documents_map[text_node.id_] = doc
                added_ids.append(text_node.id_)
                
            else:
                # Dict í˜•íƒœì˜ ë¬¸ì„œ ì²˜ë¦¬
                node = await self._create_text_node_from_dict(doc)
                new_nodes.append(node)
                added_ids.append(node.id_)
        
        # ê¸°ì¡´ ë…¸ë“œì— ì¶”ê°€
        self.nodes.extend(new_nodes)
        
        # Retriever ì¬êµ¬ì„±
        self._build_retriever()
        
        # ì¸ë±ìŠ¤ ì €ì¥
        await self._save_index()
        
        return added_ids
    
    def _create_enhanced_text(self, doc: EnhancedDocument) -> str:
        """ê°•í™”ëœ í…ìŠ¤íŠ¸ ìƒì„±"""
        text_parts = []
        
        # ê¸°ë³¸ ì½”ë“œ ë‚´ìš©
        text_parts.append(doc.text_node.text)
        
        # ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í…ìŠ¤íŠ¸ ì¶”ê°€ (ê°€ì¤‘ì¹˜ ì ìš©)
        if self.config.include_metadata:
            metadata = doc.metadata
            
            # í•¨ìˆ˜/í´ë˜ìŠ¤ëª… ê°•ì¡° (ë†’ì€ ê°€ì¤‘ì¹˜)
            for _ in range(3):  # 3ë²ˆ ë°˜ë³µìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì¦ê°€
                text_parts.append(metadata.name)
            
            # í‚¤ì›Œë“œ ì¶”ê°€
            if metadata.keywords:
                text_parts.extend(metadata.keywords)
            
            # íŒŒë¼ë¯¸í„° íƒ€ì… ì¶”ê°€
            for param in metadata.parameters:
                if param.get('type'):
                    text_parts.append(param['type'])
            
            # ë°˜í™˜ íƒ€ì… ì¶”ê°€
            if metadata.return_type:
                text_parts.append(metadata.return_type)
            
            # ìƒì†/êµ¬í˜„ ê´€ê³„
            if metadata.extends:
                text_parts.append(metadata.extends)
            
            text_parts.extend(metadata.implements)
            
            # ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ê°€
            text_parts.extend(doc.search_keywords)
            
            # ì˜ë¯¸ì  íƒœê·¸ ì¶”ê°€
            text_parts.extend(doc.semantic_tags)
        
        return ' '.join(text_parts)
    
    async def _create_text_node_from_dict(self, doc_dict: Dict[str, Any]) -> TextNode:
        """ë”•ì…”ë„ˆë¦¬ì—ì„œ TextNode ìƒì„±"""
        import uuid
        
        node_id = doc_dict.get('id', str(uuid.uuid4()))
        text = doc_dict.get('content', doc_dict.get('text', ''))
        metadata = doc_dict.get('metadata', {})
        
        # ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í…ìŠ¤íŠ¸ ê°•í™”
        if self.config.include_metadata and metadata:
            enhanced_parts = [text]
            
            # ë©”íƒ€ë°ì´í„°ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ê°€
            for key in ['name', 'function_name', 'keywords']:
                if key in metadata:
                    value = metadata[key]
                    if isinstance(value, list):
                        enhanced_parts.extend(value)
                    elif isinstance(value, str):
                        enhanced_parts.append(value)
            
            text = ' '.join(enhanced_parts)
        
        return TextNode(
            text=text,
            metadata=metadata,
            id_=node_id
        )
    
    def _build_retriever(self):
        """BM25 Retriever êµ¬ì„±"""
        if not self.nodes:
            self.retriever = None
            return
        
        # ì»¤ìŠ¤í…€ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ BM25Retriever ìƒì„±
        self.retriever = BM25Retriever.from_defaults(
            nodes=self.nodes,
            tokenizer=self.tokenizer.tokenize,
            similarity_top_k=self.config.top_k
        )
        
        # BM25 íŒŒë¼ë¯¸í„° ì„¤ì •
        if hasattr(self.retriever, 'bm25'):
            self.retriever.bm25.k1 = self.config.k1
            self.retriever.bm25.b = self.config.b
    
    async def search(self, query: str, limit: int = 10, filters: Dict[str, Any] = None) -> List[IndexedDocument]:
        """BM25 ê²€ìƒ‰"""
        if not self.retriever:
            return []
        
        try:
            # ê²€ìƒ‰ ì‹¤í–‰
            nodes_with_scores = self.retriever.retrieve(query)
            
            # ê²°ê³¼ ë³€í™˜
            results = []
            for node_with_score in nodes_with_scores[:limit]:
                node = node_with_score.node
                
                # í•„í„° ì ìš©
                if filters and not self._apply_filters(node.metadata, filters):
                    continue
                
                indexed_doc = IndexedDocument(
                    id=node.id_,
                    content=node.text,
                    metadata=node.metadata,
                    indexed_at=node.metadata.get('indexed_at', '')
                )
                results.append(indexed_doc)
            
            return results
        except Exception as e:
            import logging
            logger = logging.getLogger(__name__)
            logger.error(f"BM25 ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    async def search_with_scores(self, query: str, limit: int = 10, filters: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """ì ìˆ˜ì™€ í•¨ê»˜ BM25 ê²€ìƒ‰"""
        if not self.retriever:
            return []
        
        try:
            nodes_with_scores = self.retriever.retrieve(query)
            
            results = []
            for node_with_score in nodes_with_scores:
                node = node_with_score.node
                
                # í•„í„° ì ìš©
                if filters and not self._apply_filters(node.metadata, filters):
                    continue
                
                result = {
                    'id': node.id_,
                    'content': node.text,
                    'metadata': node.metadata,
                    'score': node_with_score.score,
                    'source': 'bm25'
                }
                results.append(result)
                
                if len(results) >= limit:
                    break
            
            return results
        except Exception as e:
            import logging
            logger = logging.getLogger(__name__)
            logger.error(f"ì ìˆ˜ë³„ BM25 ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _apply_filters(self, metadata: Dict[str, Any], filters: Dict[str, Any]) -> bool:
        """í•„í„° ì ìš©"""
        for key, value in filters.items():
            if key not in metadata:
                return False
            
            metadata_value = metadata[key]
            
            # ë¦¬ìŠ¤íŠ¸ íƒ€ì… ì²˜ë¦¬
            if isinstance(metadata_value, list):
                if value not in metadata_value:
                    return False
            else:
                if metadata_value != value:
                    return False
        
        return True
    
    async def update_document(self, doc_id: str, document: Dict[str, Any]) -> bool:
        """ë¬¸ì„œ ì—…ë°ì´íŠ¸"""
        try:
            # ê¸°ì¡´ ë¬¸ì„œ ì œê±°
            await self.delete_document(doc_id)
            
            # ìƒˆ ë¬¸ì„œ ì¶”ê°€
            updated_doc = document.copy()
            updated_doc['id'] = doc_id
            added_ids = await self.add_documents([updated_doc])
            
            return len(added_ids) > 0
        except Exception as e:
            import logging
            logger = logging.getLogger(__name__)
            logger.error(f"ë¬¸ì„œ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ ({doc_id}): {e}")
            return False
    
    async def delete_document(self, doc_id: str) -> bool:
        """ë¬¸ì„œ ì‚­ì œ"""
        try:
            # ë…¸ë“œ ëª©ë¡ì—ì„œ ì œê±°
            self.nodes = [node for node in self.nodes if node.id_ != doc_id]
            
            # ë¬¸ì„œ ë§µì—ì„œ ì œê±°
            if doc_id in self.documents_map:
                del self.documents_map[doc_id]
            
            # Retriever ì¬êµ¬ì„±
            self._build_retriever()
            
            # ì¸ë±ìŠ¤ ì €ì¥
            await self._save_index()
            
            return True
        except Exception as e:
            import logging
            logger = logging.getLogger(__name__)
            logger.error(f"ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨ ({doc_id}): {e}")
            return False
    
    async def get_stats(self) -> Dict[str, Any]:
        """ì¸ë±ìŠ¤ í†µê³„ ì •ë³´"""
        total_docs = len(self.nodes)
        total_tokens = sum(len(self.tokenizer.tokenize(node.text)) for node in self.nodes)
        avg_tokens = total_tokens / total_docs if total_docs > 0 else 0
        
        # ì–¸ì–´ë³„ ë¬¸ì„œ ìˆ˜ ê³„ì‚°
        language_stats = {}
        for node in self.nodes:
            lang = node.metadata.get('language', 'unknown')
            language_stats[lang] = language_stats.get(lang, 0) + 1
        
        return {
            "total_documents": total_docs,
            "total_tokens": total_tokens,
            "average_tokens_per_doc": round(avg_tokens, 2),
            "language_distribution": language_stats,
            "bm25_parameters": {
                "k1": self.config.k1,
                "b": self.config.b
            },
            "index_path": str(self.config.index_path)
        }
    
    async def _save_index(self):
        """ì¸ë±ìŠ¤ ì €ì¥"""
        try:
            # ë…¸ë“œ ë°ì´í„° ì €ì¥
            nodes_data = []
            for node in self.nodes:
                nodes_data.append({
                    'id': node.id_,
                    'text': node.text,
                    'metadata': node.metadata
                })
            
            nodes_file = self.config.index_path / "nodes.json"
            with open(nodes_file, 'w', encoding='utf-8') as f:
                json.dump(nodes_data, f, ensure_ascii=False, indent=2)
            
            # ë¬¸ì„œ ë§µ ì €ì¥
            docs_map_file = self.config.index_path / "documents_map.pkl"
            with open(docs_map_file, 'wb') as f:
                pickle.dump(self.documents_map, f)
            
        except Exception as e:
            import logging
            logger = logging.getLogger(__name__)
            logger.error(f"ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    async def _load_existing_index(self) -> bool:
        """ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ"""
        try:
            nodes_file = self.config.index_path / "nodes.json"
            docs_map_file = self.config.index_path / "documents_map.pkl"
            
            if not (nodes_file.exists() and docs_map_file.exists()):
                return False
            
            # ë…¸ë“œ ë°ì´í„° ë¡œë“œ
            with open(nodes_file, 'r', encoding='utf-8') as f:
                nodes_data = json.load(f)
            
            self.nodes = []
            for node_data in nodes_data:
                node = TextNode(
                    text=node_data['text'],
                    metadata=node_data['metadata'],
                    id_=node_data['id']
                )
                self.nodes.append(node)
            
            # ë¬¸ì„œ ë§µ ë¡œë“œ
            with open(docs_map_file, 'rb') as f:
                self.documents_map = pickle.load(f)
            
            # Retriever êµ¬ì„±
            self._build_retriever()
            
            return True
        except Exception as e:
            import logging
            logger = logging.getLogger(__name__)
            logger.warning(f"ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
```

### 2. BM25 Index ì„œë¹„ìŠ¤

```python
# app/index/bm25_service.py
from typing import List, Dict, Any, Optional
from .bm25_index import CodeBM25Index, BM25IndexConfig
from app.retriever.document_builder import EnhancedDocument
import logging

logger = logging.getLogger(__name__)

class BM25IndexService:
    """BM25 Index ì„œë¹„ìŠ¤"""
    
    def __init__(self, config: BM25IndexConfig = None):
        self.config = config or BM25IndexConfig()
        self.index = CodeBM25Index(self.config)
        self._initialized = False
    
    async def initialize(self):
        """ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        if not self._initialized:
            await self.index.setup()
            self._initialized = True
            logger.info(f"BM25 Index ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def index_documents(self, documents: List[EnhancedDocument]) -> Dict[str, Any]:
        """ë¬¸ì„œë“¤ ì¸ë±ì‹±"""
        await self.initialize()
        
        try:
            added_ids = await self.index.add_documents(documents)
            
            return {
                "success": True,
                "indexed_count": len(added_ids),
                "document_ids": added_ids,
                "index_type": "bm25"
            }
        except Exception as e:
            logger.error(f"BM25 ë¬¸ì„œ ì¸ë±ì‹± ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "indexed_count": 0
            }
    
    async def search_keywords(
        self, 
        query: str, 
        limit: int = 10,
        filters: Dict[str, Any] = None
    ) -> List[Dict[str, Any]]:
        """í‚¤ì›Œë“œ ê²€ìƒ‰"""
        await self.initialize()
        return await self.index.search_with_scores(query, limit, filters)
    
    async def update_document(self, doc_id: str, document: Dict[str, Any]) -> bool:
        """ë¬¸ì„œ ì—…ë°ì´íŠ¸"""
        await self.initialize()
        return await self.index.update_document(doc_id, document)
    
    async def delete_document(self, doc_id: str) -> bool:
        """ë¬¸ì„œ ì‚­ì œ"""
        await self.initialize()
        return await self.index.delete_document(doc_id)
    
    async def get_index_stats(self) -> Dict[str, Any]:
        """ì¸ë±ìŠ¤ í†µê³„"""
        await self.initialize()
        return await self.index.get_stats()
    
    async def rebuild_index(self, documents: List[EnhancedDocument]) -> Dict[str, Any]:
        """ì¸ë±ìŠ¤ ì¬êµ¬ì„±"""
        try:
            # ê¸°ì¡´ ì¸ë±ìŠ¤ ì´ˆê¸°í™”
            self.index.nodes = []
            self.index.documents_map = {}
            
            # ìƒˆ ë¬¸ì„œë“¤ë¡œ ì¸ë±ìŠ¤ êµ¬ì„±
            result = await self.index_documents(documents)
            
            logger.info(f"BM25 ì¸ë±ìŠ¤ ì¬êµ¬ì„± ì™„ë£Œ: {result['indexed_count']}ê°œ ë¬¸ì„œ")
            return result
            
        except Exception as e:
            logger.error(f"BM25 ì¸ë±ìŠ¤ ì¬êµ¬ì„± ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    async def health_check(self) -> Dict[str, Any]:
        """í—¬ìŠ¤ ì²´í¬"""
        try:
            await self.initialize()
            stats = await self.get_index_stats()
            
            return {
                "status": "healthy",
                "index_type": "bm25",
                "document_count": stats.get("total_documents", 0),
                "total_tokens": stats.get("total_tokens", 0)
            }
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e)
            }
```

## âœ… ì™„ë£Œ ì¡°ê±´

1. **BM25 Index êµ¬í˜„**: LlamaIndex BM25Retriever ê¸°ë°˜ìœ¼ë¡œ ì™„ì „íˆ êµ¬í˜„ë¨
2. **ì½”ë“œ íŠ¹í™” í† í°í™”**: ì½”ë“œ íŠ¹ì„±ì„ ê³ ë ¤í•œ í† í°í™”ê°€ ì •ìƒ ë™ì‘í•¨
3. **ë¬¸ì„œ ê´€ë¦¬**: ì¶”ê°€, ì—…ë°ì´íŠ¸, ì‚­ì œê°€ ì •ìƒ ë™ì‘í•¨
4. **í‚¤ì›Œë“œ ê²€ìƒ‰**: ì •ë°€í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ì´ ì§€ì›ë¨
5. **í•„í„°ë§**: ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•„í„°ë§ì´ ì •ìƒ ë™ì‘í•¨
6. **ì„±ëŠ¥ ìµœì í™”**: ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ì²˜ë¦¬ ì‹œ ì„±ëŠ¥ì´ ì–‘í˜¸í•¨
7. **ì¸ë±ìŠ¤ ì§€ì†ì„±**: ì¸ë±ìŠ¤ ì €ì¥/ë¡œë“œê°€ ì •ìƒ ë™ì‘í•¨

## ğŸ“‹ ë‹¤ìŒ Taskì™€ì˜ ì—°ê´€ê´€ê³„

- **Task 12**: Vector Indexì™€ í•¨ê»˜ Hybrid Retriever êµ¬ì„±
- **Task 15**: HybridRAG ì„œë¹„ìŠ¤ì—ì„œ BM25 ê²€ìƒ‰ ê²°ê³¼ í™œìš©

## ğŸ§ª í…ŒìŠ¤íŠ¸ ê³„íš

```python
# tests/unit/index/test_bm25_index.py
async def test_bm25_index_setup():
    """BM25 Index ì„¤ì • í…ŒìŠ¤íŠ¸"""
    index = CodeBM25Index()
    await index.setup()
    assert index.tokenizer is not None

async def test_code_tokenizer():
    """ì½”ë“œ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸"""
    tokenizer = CodeTokenizer()
    tokens = tokenizer.tokenize("getUserById")
    assert "get" in tokens
    assert "user" in tokens
    assert "id" in tokens

async def test_bm25_search():
    """BM25 ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
    service = BM25IndexService()
    results = await service.search_keywords("authentication method", limit=5)
    assert isinstance(results, list)

async def test_enhanced_text_creation():
    """ê°•í™”ëœ í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸"""
    index = CodeBM25Index()
    enhanced_text = index._create_enhanced_text(sample_enhanced_document)
    assert sample_enhanced_document.metadata.name in enhanced_text
```

ì´ TaskëŠ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œì—ì„œ ì •ë°€í•œ í‚¤ì›Œë“œ ë§¤ì¹­ì„ ë‹´ë‹¹í•˜ëŠ” í•µì‹¬ êµ¬ì„±ìš”ì†Œì…ë‹ˆë‹¤. ì½”ë“œ íŠ¹í™” í† í°í™”ë¥¼ í†µí•´ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì˜ íŠ¹ì„±ì„ ê³ ë ¤í•œ ì •í™•í•œ ê²€ìƒ‰ì„ ì œê³µí•©ë‹ˆë‹¤. 