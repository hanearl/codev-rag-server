# Phase 3: ê³ ê¸‰ ê¸°ëŠ¥ ê°œë°œ íƒœìŠ¤í¬

## ğŸ“‹ ê°œìš”
ì‹œìŠ¤í…œ ê°„ ì„±ëŠ¥ ë¹„êµ, ë™ì  ì‹œìŠ¤í…œ ë“±ë¡, ë°°ì¹˜ í‰ê°€ ë“± ê³ ê¸‰ ê¸°ëŠ¥ì„ êµ¬í˜„í•˜ì—¬ RAG í‰ê°€ ì„œë¹„ìŠ¤ë¥¼ ì™„ì„±í•©ë‹ˆë‹¤.

## ğŸ¯ ëª©í‘œ
- ì‹œìŠ¤í…œ ê°„ ì„±ëŠ¥ ë¹„êµ ë° ìˆœìœ„ ë§¤ê¸°ê¸°
- APIë¥¼ í†µí•œ ë™ì  ì‹œìŠ¤í…œ ë“±ë¡
- ë°°ì¹˜ í‰ê°€ ë° ìŠ¤ì¼€ì¤„ë§ ê¸°ëŠ¥
- ê³ ê¸‰ ë¶„ì„ ë° ì‹œê°í™” ì§€ì›

## ğŸ“ ìƒì„¸ íƒœìŠ¤í¬

### Task 3.1: ì‹œìŠ¤í…œ ê°„ ì„±ëŠ¥ ë¹„êµ êµ¬í˜„
**ë‹´ë‹¹ì**: ê°œë°œì  
**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 5ì‹œê°„  
**ìš°ì„ ìˆœìœ„**: High  

#### ì‘ì—… ë‚´ìš©
- [ ] ë‹¤ì¤‘ ì‹œìŠ¤í…œ ë¹„êµ ë¡œì§ êµ¬í˜„
- [ ] ì‹œìŠ¤í…œ ìˆœìœ„ ë§¤ê¸°ê¸° ì•Œê³ ë¦¬ì¦˜
- [ ] í†µê³„ì  ìœ ì˜ì„± ê²€ì • ì¶”ê°€

#### ì„¸ë¶€ ì‘ì—…
1. **ì‹œìŠ¤í…œ ë¹„êµ ì„œë¹„ìŠ¤**
   ```python
   # app/features/comparisons/service.py
   from typing import List, Dict
   from scipy import stats
   
   class SystemComparisonService:
       def __init__(self, evaluation_repository: EvaluationRepository):
           self.evaluation_repository = evaluation_repository
       
       async def compare_systems(
           self,
           system_ids: List[int],
           dataset_id: str,
           metric_name: str,
           k_value: int = 5
       ) -> SystemComparison:
           """ì—¬ëŸ¬ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ë¹„êµ"""
           
           # ê° ì‹œìŠ¤í…œì˜ ìµœì‹  í‰ê°€ ê²°ê³¼ ìˆ˜ì§‘
           system_results = {}
           for system_id in system_ids:
               latest_result = await self.evaluation_repository.get_latest_by_system_and_dataset(
                   system_id, dataset_id
               )
               if latest_result:
                   metric_key = f"{metric_name}@{k_value}"
                   system_results[system_id] = {
                       'value': latest_result.metrics.get(metric_key, 0.0),
                       'evaluation_id': latest_result.id,
                       'created_at': latest_result.created_at
                   }
           
           # ìˆœìœ„ ê³„ì‚°
           ranking = self._calculate_ranking(system_results, metric_name)
           
           # í†µê³„ì  ìœ ì˜ì„± ê²€ì • (í˜ì–´ì™€ì´ì¦ˆ ë¹„êµ)
           significance_tests = self._perform_significance_tests(system_results)
           
           return SystemComparison(
               dataset_id=dataset_id,
               metric=metric_name,
               k_value=k_value,
               systems=system_results,
               ranking=ranking,
               significance_tests=significance_tests,
               comparison_timestamp=datetime.utcnow()
           )
       
       def _calculate_ranking(self, system_results: Dict, metric_name: str) -> List[Dict]:
           """ì‹œìŠ¤í…œ ìˆœìœ„ ê³„ì‚°"""
           # ë©”íŠ¸ë¦­ ê°’ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ë†’ì€ ê°’ì´ ì¢‹ì€ ë©”íŠ¸ë¦­ì´ë¼ê³  ê°€ì •)
           sorted_systems = sorted(
               system_results.items(),
               key=lambda x: x[1]['value'],
               reverse=True
           )
           
           ranking = []
           for rank, (system_id, result) in enumerate(sorted_systems, 1):
               ranking.append({
                   'rank': rank,
                   'system_id': system_id,
                   'value': result['value'],
                   'evaluation_id': result['evaluation_id']
               })
           
           return ranking
       
       def _perform_significance_tests(self, system_results: Dict) -> Dict:
           """í†µê³„ì  ìœ ì˜ì„± ê²€ì • (t-test)"""
           # ì‹¤ì œë¡œëŠ” ì¿¼ë¦¬ë³„ ìƒì„¸ ê²°ê³¼ê°€ í•„ìš”í•˜ì§€ë§Œ, 
           # ì—¬ê¸°ì„œëŠ” ì‹œìŠ¤í…œ ë ˆë²¨ ë¹„êµë¡œ ë‹¨ìˆœí™”
           significance_tests = {}
           
           system_ids = list(system_results.keys())
           for i, system_a in enumerate(system_ids):
               for system_b in system_ids[i+1:]:
                   # ìŒë³„ ë¹„êµ
                   value_a = system_results[system_a]['value']
                   value_b = system_results[system_b]['value']
                   
                   # ë‹¨ìˆœí•œ ì°¨ì´ ê³„ì‚° (ì‹¤ì œë¡œëŠ” ë¶„í¬ ê¸°ë°˜ ê²€ì • í•„ìš”)
                   difference = abs(value_a - value_b)
                   p_value = 0.05 if difference > 0.1 else 0.5  # ì„ì˜ì˜ ê¸°ì¤€
                   
                   significance_tests[f"{system_a}_vs_{system_b}"] = {
                       'difference': value_a - value_b,
                       'p_value': p_value,
                       'is_significant': p_value < 0.05
                   }
           
           return significance_tests
   ```

2. **ì‹œìŠ¤í…œ ë¹„êµ ìŠ¤í‚¤ë§ˆ**
   ```python
   # app/features/comparisons/schema.py
   class SystemComparisonRequest(BaseModel):
       system_ids: List[int]
       dataset_id: str
       metric_name: str = "recall"
       k_value: int = 5
   
   class SystemComparison(BaseModel):
       dataset_id: str
       metric: str
       k_value: int
       systems: Dict[int, Dict[str, Any]]
       ranking: List[Dict[str, Any]]
       significance_tests: Dict[str, Dict[str, float]]
       comparison_timestamp: datetime
       
       class Config:
           schema_extra = {
               "example": {
                   "dataset_id": "ms-marco",
                   "metric": "recall",
                   "k_value": 5,
                   "ranking": [
                       {"rank": 1, "system_id": 2, "value": 0.85},
                       {"rank": 2, "system_id": 1, "value": 0.78},
                       {"rank": 3, "system_id": 3, "value": 0.72}
                   ]
               }
           }
   ```

3. **ì‹œìŠ¤í…œ ë¹„êµ API**
   ```python
   # app/features/comparisons/router.py
   router = APIRouter(prefix="/api/v1/comparisons", tags=["comparisons"])
   
   @router.post("/systems", response_model=SystemComparison)
   async def compare_systems(request: SystemComparisonRequest):
       """ì—¬ëŸ¬ ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¹„êµ"""
       pass
   
   @router.get("/leaderboard/{dataset_id}", response_model=List[Dict])
   async def get_leaderboard(dataset_id: str, metric: str = "recall", k: int = 5):
       """ë°ì´í„°ì…‹ë³„ ë¦¬ë”ë³´ë“œ"""
       pass
   ```

#### ì™„ë£Œ ì¡°ê±´
- [ ] ë‹¤ì¤‘ ì‹œìŠ¤í…œ ë¹„êµ ê¸°ëŠ¥ ë™ì‘
- [ ] ìˆœìœ„ ê³„ì‚° ì •í™•ì„± ê²€ì¦
- [ ] í†µê³„ì  ìœ ì˜ì„± ê²€ì • êµ¬í˜„

---

### Task 3.2: ë™ì  ì‹œìŠ¤í…œ ë“±ë¡ ë° ê²€ì¦
**ë‹´ë‹¹ì**: ê°œë°œì  
**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 4ì‹œê°„  
**ìš°ì„ ìˆœìœ„**: High  

#### ì‘ì—… ë‚´ìš©
- [ ] ì‹œìŠ¤í…œ ë“±ë¡ ì‹œ ìë™ ê²€ì¦
- [ ] API í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸
- [ ] ì‹œìŠ¤í…œ í—¬ìŠ¤ì²´í¬ ê¸°ëŠ¥

#### ì„¸ë¶€ ì‘ì—…
1. **ì‹œìŠ¤í…œ ê²€ì¦ ì„œë¹„ìŠ¤**
   ```python
   # app/features/systems/validator.py
   class SystemValidator:
       def __init__(self, http_client: httpx.AsyncClient):
           self.http_client = http_client
       
       async def validate_system(self, system_config: dict) -> ValidationResult:
           """ì‹œìŠ¤í…œ ë“±ë¡ ì „ ê²€ì¦"""
           result = ValidationResult()
           
           # 1. ê¸°ë³¸ ì—°ê²° í…ŒìŠ¤íŠ¸
           result.connectivity = await self._test_connectivity(system_config['base_url'])
           
           # 2. API ì—”ë“œí¬ì¸íŠ¸ ì¡´ì¬ í™•ì¸
           result.api_endpoints = await self._test_api_endpoints(system_config)
           
           # 3. ì‘ë‹µ í˜•ì‹ ê²€ì¦
           result.response_format = await self._test_response_format(system_config)
           
           # 4. ì„±ëŠ¥ ê¸°ë³¸ í…ŒìŠ¤íŠ¸
           result.performance = await self._test_basic_performance(system_config)
           
           result.is_valid = all([
               result.connectivity,
               result.api_endpoints['embed_query'],
               result.api_endpoints['retrieve'],
               result.response_format
           ])
           
           return result
       
       async def _test_connectivity(self, base_url: str) -> bool:
           """ê¸°ë³¸ ì—°ê²° í…ŒìŠ¤íŠ¸"""
           try:
               response = await self.http_client.get(f"{base_url}/health", timeout=5.0)
               return response.status_code == 200
           except:
               return False
       
       async def _test_api_endpoints(self, system_config: dict) -> dict:
           """í•„ìˆ˜ API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸"""
           base_url = system_config['base_url']
           
           endpoints = {
               'embed_query': f"{base_url}/api/v1/embed",
               'retrieve': f"{base_url}/api/v1/retrieve"
           }
           
           results = {}
           for name, url in endpoints.items():
               try:
                   # OPTIONS ìš”ì²­ìœ¼ë¡œ ì—”ë“œí¬ì¸íŠ¸ ì¡´ì¬ í™•ì¸
                   response = await self.http_client.options(url, timeout=5.0)
                   results[name] = response.status_code in [200, 405]  # 405ë„ ì¡´ì¬í•¨ì„ ì˜ë¯¸
               except:
                   results[name] = False
           
           return results
       
       async def _test_response_format(self, system_config: dict) -> bool:
           """ì‘ë‹µ í˜•ì‹ ê²€ì¦"""
           try:
               # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë¡œ ì‹¤ì œ API í˜¸ì¶œ
               test_query = "test query"
               rag_system = HTTPRAGSystem(
                   system_config['base_url'],
                   system_config.get('api_key')
               )
               
               # ì„ë² ë”© í…ŒìŠ¤íŠ¸
               embedding = await rag_system.embed_query(test_query)
               if not isinstance(embedding, list) or len(embedding) == 0:
                   return False
               
               # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
               results = await rag_system.retrieve(test_query, top_k=5)
               if not isinstance(results, list):
                   return False
               
               return True
           except:
               return False
   ```

2. **ì‹œìŠ¤í…œ ë“±ë¡ ì„œë¹„ìŠ¤ í™•ì¥**
   ```python
   # app/features/systems/service.py (í™•ì¥)
   class SystemService:
       def __init__(self, validator: SystemValidator, repository: SystemRepository):
           self.validator = validator
           self.repository = repository
       
       async def register_system(self, system_data: SystemCreateRequest) -> SystemResponse:
           """ì‹œìŠ¤í…œ ë“±ë¡ (ê²€ì¦ í¬í•¨)"""
           
           # 1. ì‹œìŠ¤í…œ ê²€ì¦
           validation_result = await self.validator.validate_system(system_data.dict())
           
           if not validation_result.is_valid:
               raise SystemValidationError(
                   "ì‹œìŠ¤í…œ ê²€ì¦ ì‹¤íŒ¨",
                   details=validation_result.dict()
               )
           
           # 2. ì‹œìŠ¤í…œ ë“±ë¡
           system = await self.repository.create(system_data.dict())
           
           # 3. ì´ˆê¸° í—¬ìŠ¤ì²´í¬ ìŠ¤ì¼€ì¤„ë§
           await self._schedule_health_check(system.id)
           
           return SystemResponse.from_orm(system)
       
       async def health_check(self, system_id: int) -> HealthCheckResult:
           """ì‹œìŠ¤í…œ í—¬ìŠ¤ì²´í¬"""
           system = await self.repository.get_by_id(system_id)
           if not system:
               raise SystemNotFoundException(f"System {system_id} not found")
           
           validation_result = await self.validator.validate_system({
               'base_url': system.base_url,
               'api_key': system.api_key
           })
           
           # í—¬ìŠ¤ì²´í¬ ê²°ê³¼ ì €ì¥
           health_check = SystemHealthCheck(
               system_id=system_id,
               is_healthy=validation_result.is_valid,
               response_time=validation_result.performance.get('response_time', 0),
               details=validation_result.dict(),
               checked_at=datetime.utcnow()
           )
           
           await self.repository.save_health_check(health_check)
           
           return HealthCheckResult.from_orm(health_check)
   ```

3. **ì‹œìŠ¤í…œ ê´€ë¦¬ API í™•ì¥**
   ```python
   # app/features/systems/router.py (í™•ì¥)
   @router.post("/validate", response_model=ValidationResult)
   async def validate_system(request: SystemCreateRequest):
       """ì‹œìŠ¤í…œ ë“±ë¡ ì „ ê²€ì¦"""
       pass
   
   @router.get("/{system_id}/health", response_model=HealthCheckResult)
   async def system_health_check(system_id: int):
       """ì‹œìŠ¤í…œ í—¬ìŠ¤ì²´í¬"""
       pass
   
   @router.get("/{system_id}/health/history", response_model=List[HealthCheckResult])
   async def system_health_history(system_id: int, limit: int = 10):
       """ì‹œìŠ¤í…œ í—¬ìŠ¤ì²´í¬ ì´ë ¥"""
       pass
   ```

#### ì™„ë£Œ ì¡°ê±´
- [ ] ì‹œìŠ¤í…œ ë“±ë¡ ì‹œ ìë™ ê²€ì¦ ë™ì‘
- [ ] API í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ í†µê³¼
- [ ] í—¬ìŠ¤ì²´í¬ ê¸°ëŠ¥ ì •ìƒ ë™ì‘

---

### Task 3.3: ë°°ì¹˜ í‰ê°€ ë° ìŠ¤ì¼€ì¤„ë§ ì‹œìŠ¤í…œ
**ë‹´ë‹¹ì**: ê°œë°œì  
**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 6ì‹œê°„  
**ìš°ì„ ìˆœìœ„**: Medium  

#### ì‘ì—… ë‚´ìš©
- [ ] ë¹„ë™ê¸° ë°°ì¹˜ í‰ê°€ ì‹œìŠ¤í…œ
- [ ] í‰ê°€ ì‘ì—… í ë° ìŠ¤ì¼€ì¤„ëŸ¬
- [ ] ì§„í–‰ ìƒí™© ì¶”ì  ë° ëª¨ë‹ˆí„°ë§

#### ì„¸ë¶€ ì‘ì—…
1. **ë°°ì¹˜ í‰ê°€ ì‘ì—… ì‹œìŠ¤í…œ**
   ```python
   # app/features/batch/models.py
   class BatchEvaluationJob(Base):
       __tablename__ = "batch_evaluation_jobs"
       
       id = Column(Integer, primary_key=True)
       name = Column(String, nullable=False)
       description = Column(Text)
       
       # ì‘ì—… ì„¤ì •
       system_ids = Column(JSON)  # List[int]
       dataset_ids = Column(JSON)  # List[str]
       evaluation_config = Column(JSON)
       
       # ì‘ì—… ìƒíƒœ
       status = Column(String, default="pending")  # pending, running, completed, failed
       progress = Column(Integer, default=0)  # 0-100
       
       # ì‹¤í–‰ ì •ë³´
       started_at = Column(DateTime)
       completed_at = Column(DateTime)
       error_message = Column(Text)
       
       # ê²°ê³¼
       evaluation_result_ids = Column(JSON)  # List[int]
       
       created_at = Column(DateTime(timezone=True), server_default=func.now())
   
   class BatchEvaluationTask(Base):
       __tablename__ = "batch_evaluation_tasks"
       
       id = Column(Integer, primary_key=True)
       job_id = Column(Integer, ForeignKey("batch_evaluation_jobs.id"))
       system_id = Column(Integer)
       dataset_id = Column(String)
       
       status = Column(String, default="pending")
       started_at = Column(DateTime)
       completed_at = Column(DateTime)
       evaluation_result_id = Column(Integer, ForeignKey("evaluation_results.id"))
       error_message = Column(Text)
   ```

2. **ë°°ì¹˜ í‰ê°€ ì„œë¹„ìŠ¤**
   ```python
   # app/features/batch/service.py
   import asyncio
   from celery import Celery
   
   class BatchEvaluationService:
       def __init__(self):
           self.celery_app = Celery('evaluation-server')
       
       async def create_batch_job(
           self,
           name: str,
           system_ids: List[int],
           dataset_ids: List[str],
           evaluation_config: EvaluationConfig
       ) -> BatchEvaluationJob:
           """ë°°ì¹˜ í‰ê°€ ì‘ì—… ìƒì„±"""
           
           # 1. ì‘ì—… ìƒì„±
           job = BatchEvaluationJob(
               name=name,
               system_ids=system_ids,
               dataset_ids=dataset_ids,
               evaluation_config=evaluation_config.dict(),
               status="pending"
           )
           
           # 2. ê°œë³„ íƒœìŠ¤í¬ ìƒì„±
           tasks = []
           for system_id in system_ids:
               for dataset_id in dataset_ids:
                   task = BatchEvaluationTask(
                       job_id=job.id,
                       system_id=system_id,
                       dataset_id=dataset_id
                   )
                   tasks.append(task)
           
           # 3. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
           await self.repository.save_job_and_tasks(job, tasks)
           
           # 4. ë¹„ë™ê¸° ì‹¤í–‰ ì‹œì‘
           self._start_batch_execution.delay(job.id)
           
           return job
       
       @celery_app.task
       def _start_batch_execution(self, job_id: int):
           """ë°°ì¹˜ ì‘ì—… ì‹¤í–‰ (Celery íƒœìŠ¤í¬)"""
           asyncio.run(self._execute_batch_job(job_id))
       
       async def _execute_batch_job(self, job_id: int):
           """ì‹¤ì œ ë°°ì¹˜ ì‘ì—… ì‹¤í–‰"""
           job = await self.repository.get_job(job_id)
           tasks = await self.repository.get_job_tasks(job_id)
           
           # ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸
           job.status = "running"
           job.started_at = datetime.utcnow()
           await self.repository.update_job(job)
           
           completed_tasks = 0
           total_tasks = len(tasks)
           
           try:
               # ë³‘ë ¬ ì²˜ë¦¬ (ì œí•œëœ ë™ì‹œì„±)
               semaphore = asyncio.Semaphore(3)  # ìµœëŒ€ 3ê°œ ë™ì‹œ ì‹¤í–‰
               
               async def execute_single_task(task):
                   async with semaphore:
                       await self._execute_single_task(task)
                       nonlocal completed_tasks
                       completed_tasks += 1
                       
                       # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                       progress = int((completed_tasks / total_tasks) * 100)
                       await self.repository.update_job_progress(job_id, progress)
               
               # ëª¨ë“  íƒœìŠ¤í¬ ë³‘ë ¬ ì‹¤í–‰
               await asyncio.gather(*[execute_single_task(task) for task in tasks])
               
               # ì‘ì—… ì™„ë£Œ
               job.status = "completed"
               job.completed_at = datetime.utcnow()
               job.progress = 100
               
           except Exception as e:
               job.status = "failed"
               job.error_message = str(e)
               job.completed_at = datetime.utcnow()
           
           await self.repository.update_job(job)
       
       async def _execute_single_task(self, task: BatchEvaluationTask):
           """ê°œë³„ íƒœìŠ¤í¬ ì‹¤í–‰"""
           try:
               task.status = "running"
               task.started_at = datetime.utcnow()
               await self.repository.update_task(task)
               
               # í‰ê°€ ì‹¤í–‰
               evaluation_result = await self.evaluation_service.run_evaluation(
                   task.system_id,
                   task.dataset_id,
                   EvaluationConfig(**task.job.evaluation_config)
               )
               
               task.status = "completed"
               task.evaluation_result_id = evaluation_result.id
               task.completed_at = datetime.utcnow()
               
           except Exception as e:
               task.status = "failed"
               task.error_message = str(e)
               task.completed_at = datetime.utcnow()
           
           await self.repository.update_task(task)
   ```

3. **ë°°ì¹˜ í‰ê°€ API**
   ```python
   # app/features/batch/router.py
   @router.post("/jobs", response_model=BatchJobResponse)
   async def create_batch_job(request: BatchJobCreateRequest):
       """ë°°ì¹˜ í‰ê°€ ì‘ì—… ìƒì„±"""
       pass
   
   @router.get("/jobs/{job_id}", response_model=BatchJobResponse)
   async def get_batch_job(job_id: int):
       """ë°°ì¹˜ ì‘ì—… ìƒíƒœ ì¡°íšŒ"""
       pass
   
   @router.get("/jobs/{job_id}/progress", response_model=BatchJobProgress)
   async def get_batch_job_progress(job_id: int):
       """ë°°ì¹˜ ì‘ì—… ì§„í–‰ ìƒí™©"""
       pass
   
   @router.post("/jobs/{job_id}/cancel")
   async def cancel_batch_job(job_id: int):
       """ë°°ì¹˜ ì‘ì—… ì·¨ì†Œ"""
       pass
   ```

#### ì™„ë£Œ ì¡°ê±´
- [ ] ë¹„ë™ê¸° ë°°ì¹˜ í‰ê°€ ì‹œìŠ¤í…œ ë™ì‘
- [ ] ì‘ì—… í ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì •ìƒ ì‘ë™
- [ ] ì§„í–‰ ìƒí™© ì¶”ì  ê¸°ëŠ¥ êµ¬í˜„

---

### Task 3.4: ê³ ê¸‰ ë¶„ì„ ë° ë¦¬í¬íŒ…
**ë‹´ë‹¹ì**: ê°œë°œì  
**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 4ì‹œê°„  
**ìš°ì„ ìˆœìœ„**: Medium  

#### ì‘ì—… ë‚´ìš©
- [ ] ìƒì„¸ ì„±ëŠ¥ ë¶„ì„ ë¦¬í¬íŠ¸
- [ ] íŠ¸ë Œë“œ ë¶„ì„ ê¸°ëŠ¥
- [ ] ë°ì´í„° ë‚´ë³´ë‚´ê¸° ê¸°ëŠ¥

#### ì„¸ë¶€ ì‘ì—…
1. **ì„±ëŠ¥ ë¶„ì„ ì„œë¹„ìŠ¤**
   ```python
   # app/features/analytics/service.py
   class AnalyticsService:
       async def generate_performance_report(
           self,
           system_id: int,
           date_range: DateRange
       ) -> PerformanceReport:
           """ì‹œìŠ¤í…œ ì„±ëŠ¥ ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸"""
           
           evaluations = await self.repository.get_evaluations_by_date_range(
               system_id, date_range.start_date, date_range.end_date
           )
           
           # ë©”íŠ¸ë¦­ë³„ íŠ¸ë Œë“œ ë¶„ì„
           trends = self._analyze_trends(evaluations)
           
           # ë°ì´í„°ì…‹ë³„ ì„±ëŠ¥ ë¹„êµ
           dataset_performance = self._analyze_dataset_performance(evaluations)
           
           # ì„±ëŠ¥ ë³€í™” í¬ì¸íŠ¸ íƒì§€
           change_points = self._detect_change_points(evaluations)
           
           return PerformanceReport(
               system_id=system_id,
               date_range=date_range,
               trends=trends,
               dataset_performance=dataset_performance,
               change_points=change_points,
               summary_statistics=self._calculate_summary_stats(evaluations)
           )
       
       def _analyze_trends(self, evaluations: List[EvaluationResult]) -> Dict:
           """ë©”íŠ¸ë¦­ë³„ íŠ¸ë Œë“œ ë¶„ì„"""
           trends = {}
           
           # ì‹œê°„ìˆœ ì •ë ¬
           evaluations.sort(key=lambda x: x.created_at)
           
           # ê° ë©”íŠ¸ë¦­ë³„ íŠ¸ë Œë“œ ê³„ì‚°
           for metric_name in ['recall@5', 'precision@5', 'ndcg@5']:
               values = [e.metrics.get(metric_name, 0) for e in evaluations]
               dates = [e.created_at for e in evaluations]
               
               # ì„ í˜• íšŒê·€ë¡œ íŠ¸ë Œë“œ ê³„ì‚°
               trend_slope = self._calculate_trend_slope(dates, values)
               
               trends[metric_name] = {
                   'slope': trend_slope,
                   'direction': 'improving' if trend_slope > 0 else 'declining',
                   'values': values,
                   'dates': dates
               }
           
           return trends
       
       async def generate_comparison_report(
           self,
           system_ids: List[int],
           dataset_id: str
       ) -> ComparisonReport:
           """ì‹œìŠ¤í…œ ê°„ ë¹„êµ ë¦¬í¬íŠ¸"""
           
           systems_data = {}
           for system_id in system_ids:
               evaluations = await self.repository.get_evaluations_by_system_and_dataset(
                   system_id, dataset_id
               )
               systems_data[system_id] = evaluations
           
           # ë©”íŠ¸ë¦­ë³„ ë¹„êµ ë¶„ì„
           metric_comparisons = self._compare_metrics(systems_data)
           
           # ì•ˆì •ì„± ë¶„ì„ (ì„±ëŠ¥ ë³€ë™ì„±)
           stability_analysis = self._analyze_stability(systems_data)
           
           return ComparisonReport(
               dataset_id=dataset_id,
               systems=system_ids,
               metric_comparisons=metric_comparisons,
               stability_analysis=stability_analysis
           )
   ```

2. **ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì„œë¹„ìŠ¤**
   ```python
   # app/features/export/service.py
   import pandas as pd
   import io
   
   class ExportService:
       async def export_evaluations_csv(
           self,
           filters: EvaluationFilters
       ) -> io.BytesIO:
           """í‰ê°€ ê²°ê³¼ CSV ë‚´ë³´ë‚´ê¸°"""
           
           evaluations = await self.repository.get_filtered_evaluations(filters)
           
           # ë°ì´í„° í”Œë«ë‹
           rows = []
           for eval_result in evaluations:
               base_data = {
                   'evaluation_id': eval_result.id,
                   'system_id': eval_result.system_id,
                   'dataset_id': eval_result.dataset_id,
                   'created_at': eval_result.created_at,
                   'execution_time': eval_result.execution_time
               }
               
               # ë©”íŠ¸ë¦­ ë°ì´í„° ì¶”ê°€
               for metric_name, value in eval_result.metrics.items():
                   base_data[metric_name] = value
               
               rows.append(base_data)
           
           # DataFrame ìƒì„± ë° CSV ë³€í™˜
           df = pd.DataFrame(rows)
           csv_buffer = io.BytesIO()
           df.to_csv(csv_buffer, index=False, encoding='utf-8')
           csv_buffer.seek(0)
           
           return csv_buffer
       
       async def export_comparison_json(
           self,
           comparison_id: int
       ) -> dict:
           """ë¹„êµ ê²°ê³¼ JSON ë‚´ë³´ë‚´ê¸°"""
           comparison = await self.repository.get_comparison(comparison_id)
           
           return {
               'metadata': {
                   'export_date': datetime.utcnow().isoformat(),
                   'comparison_id': comparison_id,
                   'format_version': '1.0'
               },
               'comparison_data': comparison.dict()
           }
   ```

3. **ë¶„ì„ ë° ë‚´ë³´ë‚´ê¸° API**
   ```python
   # app/features/analytics/router.py
   @router.get("/systems/{system_id}/report", response_model=PerformanceReport)
   async def get_performance_report(
       system_id: int,
       start_date: datetime,
       end_date: datetime
   ):
       """ì„±ëŠ¥ ë¶„ì„ ë¦¬í¬íŠ¸"""
       pass
   
   @router.get("/comparisons/{comparison_id}/report", response_model=ComparisonReport)
   async def get_comparison_report(comparison_id: int):
       """ë¹„êµ ë¶„ì„ ë¦¬í¬íŠ¸"""
       pass
   
   @router.get("/export/evaluations")
   async def export_evaluations(
       format: str = "csv",
       system_id: Optional[int] = None,
       dataset_id: Optional[str] = None
   ):
       """í‰ê°€ ê²°ê³¼ ë‚´ë³´ë‚´ê¸°"""
       pass
   ```

#### ì™„ë£Œ ì¡°ê±´
- [ ] ìƒì„¸ ì„±ëŠ¥ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
- [ ] íŠ¸ë Œë“œ ë¶„ì„ ì •í™•ì„± ê²€ì¦
- [ ] CSV/JSON ë‚´ë³´ë‚´ê¸° ê¸°ëŠ¥ ë™ì‘

---

### Task 3.5: ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ì‹œìŠ¤í…œ
**ë‹´ë‹¹ì**: ê°œë°œì  
**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 3ì‹œê°„  
**ìš°ì„ ìˆœìœ„**: Low  

#### ì‘ì—… ë‚´ìš©
- [ ] ì„±ëŠ¥ ì„ê³„ê°’ ëª¨ë‹ˆí„°ë§
- [ ] ì•Œë¦¼ ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] ëŒ€ì‹œë³´ë“œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘

#### ì„¸ë¶€ ì‘ì—…
1. **ëª¨ë‹ˆí„°ë§ ì„œë¹„ìŠ¤**
   ```python
   # app/features/monitoring/service.py
   class MonitoringService:
       def __init__(self):
           self.alert_thresholds = {
               'recall@5': {'min': 0.7, 'max': 1.0},
               'precision@5': {'min': 0.6, 'max': 1.0},
               'ndcg@5': {'min': 0.65, 'max': 1.0}
           }
       
       async def check_performance_alerts(self, evaluation_result: EvaluationResult):
           """ì„±ëŠ¥ ì•Œë¦¼ ì²´í¬"""
           alerts = []
           
           for metric_name, thresholds in self.alert_thresholds.items():
               if metric_name in evaluation_result.metrics:
                   value = evaluation_result.metrics[metric_name]
                   
                   if value < thresholds['min']:
                       alerts.append(PerformanceAlert(
                           type='performance_degradation',
                           metric=metric_name,
                           value=value,
                           threshold=thresholds['min'],
                           evaluation_id=evaluation_result.id,
                           severity='high' if value < thresholds['min'] * 0.9 else 'medium'
                       ))
           
           # ì•Œë¦¼ ë°œì†¡
           for alert in alerts:
               await self.send_alert(alert)
           
           return alerts
       
       async def send_alert(self, alert: PerformanceAlert):
           """ì•Œë¦¼ ë°œì†¡ (ì´ë©”ì¼, ìŠ¬ë™ ë“±)"""
           # ì‹¤ì œë¡œëŠ” ì´ë©”ì¼, ìŠ¬ë™, ì›¹í›… ë“±ìœ¼ë¡œ ë°œì†¡
           logger.warning(f"Performance Alert: {alert.dict()}")
   ```

2. **ëª¨ë‹ˆí„°ë§ API**
   ```python
   # app/features/monitoring/router.py
   @router.get("/alerts", response_model=List[PerformanceAlert])
   async def get_alerts(limit: int = 50):
       """ìµœê·¼ ì•Œë¦¼ ëª©ë¡"""
       pass
   
   @router.get("/metrics/dashboard")
   async def get_dashboard_metrics():
       """ëŒ€ì‹œë³´ë“œìš© ë©”íŠ¸ë¦­"""
       pass
   ```

#### ì™„ë£Œ ì¡°ê±´
- [ ] ì„±ëŠ¥ ì„ê³„ê°’ ëª¨ë‹ˆí„°ë§ ë™ì‘
- [ ] ì•Œë¦¼ ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] ëŒ€ì‹œë³´ë“œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ê¸°ëŠ¥

## ğŸ¯ Phase 3 ì™„ë£Œ ì¡°ê±´

### ê¸°ëŠ¥ì  ìš”êµ¬ì‚¬í•­
- [ ] ì‹œìŠ¤í…œ ê°„ ì„±ëŠ¥ ë¹„êµ ë° ìˆœìœ„ ë§¤ê¸°ê¸°
- [ ] ë™ì  ì‹œìŠ¤í…œ ë“±ë¡ ë° ê²€ì¦
- [ ] ë°°ì¹˜ í‰ê°€ ë° ìŠ¤ì¼€ì¤„ë§ ì‹œìŠ¤í…œ
- [ ] ê³ ê¸‰ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
- [ ] ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ì‹œìŠ¤í…œ

### ê¸°ìˆ ì  ìš”êµ¬ì‚¬í•­
- [ ] ë¹„ë™ê¸° ì²˜ë¦¬ ìµœì í™”
- [ ] í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜
- [ ] ì™„ì „í•œ API ë¬¸ì„œ
- [ ] ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í†µê³¼

### ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­
- [ ] ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›
- [ ] ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì„±ëŠ¥
- [ ] ë™ì‹œ ì‚¬ìš©ì ì§€ì› (10ëª… ì´ìƒ)

## ğŸ“… ì˜ˆìƒ ì¼ì •
**ì´ ì†Œìš” ì‹œê°„**: 22ì‹œê°„ (ì•½ 3ì¼)

1. **Day 1**: Task 3.1, 3.2 (ì‹œìŠ¤í…œ ë¹„êµ, ë™ì  ë“±ë¡)
2. **Day 2**: Task 3.3 (ë°°ì¹˜ í‰ê°€)
3. **Day 3**: Task 3.4, 3.5 (ë¶„ì„, ëª¨ë‹ˆí„°ë§)

## ğŸš€ ì „ì²´ í”„ë¡œì íŠ¸ ì™„ë£Œ í›„ ì¶”ê°€ ê³ ë ¤ì‚¬í•­

### ìš´ì˜ ë° ë°°í¬
- [ ] Docker Compose ì„¤ì • ì—…ë°ì´íŠ¸
- [ ] CI/CD íŒŒì´í”„ë¼ì¸ êµ¬ì„±
- [ ] í”„ë¡œë•ì…˜ í™˜ê²½ ì„¤ì •
- [ ] ë¡œê·¸ ì§‘ê³„ ë° ëª¨ë‹ˆí„°ë§ ì„¤ì •

### ë¬¸ì„œí™”
- [ ] API ì‚¬ìš© ê°€ì´ë“œ ì‘ì„±
- [ ] ê°œë°œì ë¬¸ì„œ ì™„ì„±
- [ ] ì‚¬ìš©ì ë§¤ë‰´ì–¼ ì‘ì„±
- [ ] ì•„í‚¤í…ì²˜ ë¬¸ì„œ ì—…ë°ì´íŠ¸ 