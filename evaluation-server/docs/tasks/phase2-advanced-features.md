# Phase 2: í™•ì¥ ê¸°ëŠ¥ ê°œë°œ íƒœìŠ¤í¬

## ğŸ“‹ ê°œìš”
Phase 1ì˜ ê¸°ë³¸ í‰ê°€ ê¸°ëŠ¥ì„ í™•ì¥í•˜ì—¬ NDCG ë©”íŠ¸ë¦­, ë‹¤ì¤‘ ë°ì´í„°ì…‹ ì§€ì›, ë² ì´ìŠ¤ë¼ì¸ ë¹„êµ ê¸°ëŠ¥ì„ ì¶”ê°€í•©ë‹ˆë‹¤.

## ğŸ¯ ëª©í‘œ
- NDCG ë©”íŠ¸ë¦­ ì¶”ê°€
- ë‹¤ì¤‘ ë°ì´í„°ì…‹ ì§€ì›
- ë² ì´ìŠ¤ë¼ì¸ ë“±ë¡ ë° ë¹„êµ ê¸°ëŠ¥

## ğŸ“ ìƒì„¸ íƒœìŠ¤í¬

### Task 2.1: NDCG ë©”íŠ¸ë¦­ êµ¬í˜„
**ë‹´ë‹¹ì**: ê°œë°œì  
**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 4ì‹œê°„  
**ìš°ì„ ìˆœìœ„**: High  

#### ì‘ì—… ë‚´ìš©
- [ ] NDCG (Normalized Discounted Cumulative Gain) ë©”íŠ¸ë¦­ êµ¬í˜„
- [ ] ê´€ë ¨ì„± ì ìˆ˜ ì²˜ë¦¬ ë¡œì§ ì¶”ê°€
- [ ] NDCG ê²€ì¦ í…ŒìŠ¤íŠ¸ ì‘ì„±

#### ì„¸ë¶€ ì‘ì—…
1. **NDCG ë©”íŠ¸ë¦­ êµ¬í˜„**
   ```python
   # app/features/metrics/ndcg.py
   import numpy as np
   from typing import List, Dict
   
   class NDCGAtK(MetricCalculator):
       def calculate(self, predictions: List[str], ground_truth: Dict[str, float], k: int) -> float:
           """
           NDCG@K ê³„ì‚°
           
           Args:
               predictions: ê²€ìƒ‰ ê²°ê³¼ ë¬¸ì„œ ID ë¦¬ìŠ¤íŠ¸ (ìˆœì„œëŒ€ë¡œ)
               ground_truth: {doc_id: relevance_score} í˜•íƒœì˜ ì •ë‹µ
               k: ìƒìœ„ kê°œ ê²°ê³¼ë§Œ ê³ ë ¤
           
           Returns:
               NDCG@K ê°’ (0.0 ~ 1.0)
           """
           # DCG ê³„ì‚°
           dcg = self._calculate_dcg(predictions[:k], ground_truth)
           
           # IDCG ê³„ì‚°
           ideal_ranking = sorted(ground_truth.items(), key=lambda x: x[1], reverse=True)
           ideal_docs = [doc_id for doc_id, _ in ideal_ranking]
           idcg = self._calculate_dcg(ideal_docs[:k], ground_truth)
           
           # NDCG ê³„ì‚°
           if idcg == 0:
               return 0.0
           return dcg / idcg
       
       def _calculate_dcg(self, doc_ids: List[str], relevance: Dict[str, float]) -> float:
           """DCG (Discounted Cumulative Gain) ê³„ì‚°"""
           dcg = 0.0
           for i, doc_id in enumerate(doc_ids):
               rel_score = relevance.get(doc_id, 0.0)
               # DCG ê³µì‹: rel_i / log2(i+2)
               dcg += rel_score / np.log2(i + 2)
           return dcg
   ```

2. **ê´€ë ¨ì„± ì ìˆ˜ ë°ì´í„° í˜•ì‹ í™•ì¥**
   ```python
   # app/features/datasets/schema.py
   class GroundTruthWithRelevance(BaseModel):
       query_id: str
       relevant_docs: List[str]  # ê¸°ì¡´ ì´ì§„ ê´€ë ¨ì„±
       relevance_scores: Optional[Dict[str, float]] = None  # NDCGìš© ì ìˆ˜
   ```

3. **ë©”íŠ¸ë¦­ ë§¤ë‹ˆì € ì—…ë°ì´íŠ¸**
   ```python
   # app/features/metrics/manager.py (ì—…ë°ì´íŠ¸)
   class MetricsManager:
       def __init__(self):
           self.metrics = {
               'recall': RecallAtK(),
               'precision': PrecisionAtK(),
               'hit': HitAtK(),
               'ndcg': NDCGAtK()  # ìƒˆë¡œ ì¶”ê°€
           }
   ```

#### ì™„ë£Œ ì¡°ê±´
- [ ] NDCG@K ë©”íŠ¸ë¦­ ì •í™•í•œ êµ¬í˜„
- [ ] ê´€ë ¨ì„± ì ìˆ˜ ì²˜ë¦¬ ë¡œì§ ì™„ì„±
- [ ] í‘œì¤€ ë°ì´í„°ì…‹ìœ¼ë¡œ NDCG ê°’ ê²€ì¦

---

### Task 2.2: ë‹¤ì¤‘ ë°ì´í„°ì…‹ ì§€ì› êµ¬í˜„
**ë‹´ë‹¹ì**: ê°œë°œì  
**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 3ì‹œê°„  
**ìš°ì„ ìˆœìœ„**: High  

#### ì‘ì—… ë‚´ìš©
- [ ] ë°°ì¹˜ í‰ê°€ ê¸°ëŠ¥ êµ¬í˜„
- [ ] ë°ì´í„°ì…‹ ìë™ ë°œê²¬ ë¡œì§
- [ ] ë‹¤ì¤‘ ë°ì´í„°ì…‹ ê²°ê³¼ ì§‘ê³„

#### ì„¸ë¶€ ì‘ì—…
1. **ë°°ì¹˜ í‰ê°€ ì„œë¹„ìŠ¤**
   ```python
   # app/features/evaluations/batch_service.py
   class BatchEvaluationService:
       def __init__(self, evaluation_service: EvaluationService):
           self.evaluation_service = evaluation_service
       
       async def run_batch_evaluation(
           self,
           system_id: int,
           dataset_ids: List[str],
           config: EvaluationConfig
       ) -> List[EvaluationResult]:
           """ì—¬ëŸ¬ ë°ì´í„°ì…‹ì— ëŒ€í•´ ë°°ì¹˜ í‰ê°€ ì‹¤í–‰"""
           results = []
           
           for dataset_id in dataset_ids:
               try:
                   result = await self.evaluation_service.run_evaluation(
                       system_id, dataset_id, config
                   )
                   results.append(result)
               except Exception as e:
                   # ê°œë³„ ë°ì´í„°ì…‹ ì‹¤íŒ¨ ì‹œ ë¡œê·¸ ê¸°ë¡í•˜ê³  ê³„ì† ì§„í–‰
                   logger.error(f"Dataset {dataset_id} evaluation failed: {e}")
           
           return results
       
       def aggregate_results(self, results: List[EvaluationResult]) -> Dict[str, float]:
           """ë‹¤ì¤‘ ë°ì´í„°ì…‹ ê²°ê³¼ ì§‘ê³„ (í‰ê· , ê°€ì¤‘í‰ê·  ë“±)"""
           pass
   ```

2. **ë°ì´í„°ì…‹ ìë™ ë°œê²¬**
   ```python
   # app/features/datasets/discovery.py
   class DatasetDiscovery:
       def __init__(self, datasets_path: str):
           self.datasets_path = datasets_path
       
       def discover_datasets(self) -> List[str]:
           """datasets í´ë”ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ìë™ ë°œê²¬"""
           datasets = []
           for item in os.listdir(self.datasets_path):
               dataset_path = os.path.join(self.datasets_path, item)
               if self._is_valid_dataset(dataset_path):
                   datasets.append(item)
           return datasets
       
       def _is_valid_dataset(self, path: str) -> bool:
           """ë°ì´í„°ì…‹ í´ë”ê°€ ìœ íš¨í•œì§€ ê²€ì¦"""
           required_files = ['queries.jsonl', 'ground_truth.jsonl', 'metadata.json']
           return all(os.path.exists(os.path.join(path, f)) for f in required_files)
   ```

3. **ë°°ì¹˜ í‰ê°€ API**
   ```python
   # app/features/evaluations/router.py (í™•ì¥)
   @router.post("/batch", response_model=BatchEvaluationResponse)
   async def run_batch_evaluation(request: BatchEvaluationRequest):
       """ë‹¤ì¤‘ ë°ì´í„°ì…‹ ë°°ì¹˜ í‰ê°€"""
       pass
   
   @router.get("/batch/{batch_id}", response_model=BatchEvaluationResponse)
   async def get_batch_evaluation(batch_id: int):
       """ë°°ì¹˜ í‰ê°€ ê²°ê³¼ ì¡°íšŒ"""
       pass
   ```

#### ì™„ë£Œ ì¡°ê±´
- [ ] ë‹¤ì¤‘ ë°ì´í„°ì…‹ ë°°ì¹˜ í‰ê°€ ë™ì‘
- [ ] ë°ì´í„°ì…‹ ìë™ ë°œê²¬ ê¸°ëŠ¥ ë™ì‘
- [ ] ê²°ê³¼ ì§‘ê³„ ë¡œì§ êµ¬í˜„

---

### Task 2.3: ë² ì´ìŠ¤ë¼ì¸ ê´€ë¦¬ ì‹œìŠ¤í…œ êµ¬í˜„
**ë‹´ë‹¹ì**: ê°œë°œì  
**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 5ì‹œê°„  
**ìš°ì„ ìˆœìœ„**: High  

#### ì‘ì—… ë‚´ìš©
- [ ] ë² ì´ìŠ¤ë¼ì¸ ë°ì´í„° ëª¨ë¸ êµ¬í˜„
- [ ] ë² ì´ìŠ¤ë¼ì¸ ë“±ë¡ ì„œë¹„ìŠ¤
- [ ] ë² ì´ìŠ¤ë¼ì¸ ë¹„êµ ë¡œì§

#### ì„¸ë¶€ ì‘ì—…
1. **ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ êµ¬í˜„**
   ```python
   # app/features/baselines/model.py
   class Baseline(Base):
       __tablename__ = "baselines"
       
       id = Column(Integer, primary_key=True, index=True)
       name = Column(String, nullable=False)
       description = Column(Text)
       system_id = Column(Integer, ForeignKey("rag_systems.id"))
       dataset_id = Column(String, nullable=False)
       evaluation_result_id = Column(Integer, ForeignKey("evaluation_results.id"))
       is_active = Column(Boolean, default=True)
       created_at = Column(DateTime(timezone=True), server_default=func.now())
       
       # ê´€ê³„ ì„¤ì •
       system = relationship("RAGSystem")
       evaluation_result = relationship("EvaluationResult")
   ```

2. **ë² ì´ìŠ¤ë¼ì¸ ì„œë¹„ìŠ¤**
   ```python
   # app/features/baselines/service.py
   class BaselineService:
       def __init__(self, baseline_repository: BaselineRepository):
           self.baseline_repository = baseline_repository
       
       async def register_baseline(
           self,
           name: str,
           description: str,
           evaluation_result_id: int
       ) -> Baseline:
           """í‰ê°€ ê²°ê³¼ë¥¼ ë² ì´ìŠ¤ë¼ì¸ìœ¼ë¡œ ë“±ë¡"""
           pass
       
       async def compare_with_baseline(
           self,
           baseline_id: int,
           current_result: EvaluationResult
       ) -> BaselineComparison:
           """í˜„ì¬ ê²°ê³¼ë¥¼ ë² ì´ìŠ¤ë¼ì¸ê³¼ ë¹„êµ"""
           baseline = await self.baseline_repository.get_by_id(baseline_id)
           baseline_result = baseline.evaluation_result
           
           comparison = self._calculate_comparison(baseline_result, current_result)
           return comparison
       
       def _calculate_comparison(
           self,
           baseline_result: EvaluationResult,
           current_result: EvaluationResult
       ) -> BaselineComparison:
           """ë©”íŠ¸ë¦­ë³„ ë³€í™”ìœ¨ ê³„ì‚°"""
           comparison = {}
           
           for metric_name in baseline_result.metrics:
               if metric_name in current_result.metrics:
                   baseline_value = baseline_result.metrics[metric_name]
                   current_value = current_result.metrics[metric_name]
                   
                   # ë³€í™”ìœ¨ ê³„ì‚° (%)
                   if baseline_value != 0:
                       change_rate = ((current_value - baseline_value) / baseline_value) * 100
                   else:
                       change_rate = 0.0
                   
                   comparison[metric_name] = {
                       'baseline': baseline_value,
                       'current': current_value,
                       'change': current_value - baseline_value,
                       'change_rate': change_rate
                   }
           
           return BaselineComparison(
               baseline_id=baseline_result.id,
               current_result_id=current_result.id,
               metrics_comparison=comparison
           )
   ```

3. **ë² ì´ìŠ¤ë¼ì¸ ë¹„êµ ìŠ¤í‚¤ë§ˆ**
   ```python
   # app/features/baselines/schema.py
   class BaselineCreateRequest(BaseModel):
       name: str
       description: Optional[str] = None
       evaluation_result_id: int
   
   class BaselineComparison(BaseModel):
       baseline_id: int
       current_result_id: int
       metrics_comparison: Dict[str, Dict[str, float]]
       
       class Config:
           schema_extra = {
               "example": {
                   "baseline_id": 1,
                   "current_result_id": 5,
                   "metrics_comparison": {
                       "recall@5": {
                           "baseline": 0.75,
                           "current": 0.80,
                           "change": 0.05,
                           "change_rate": 6.67
                       }
                   }
               }
           }
   ```

4. **ë² ì´ìŠ¤ë¼ì¸ API**
   ```python
   # app/features/baselines/router.py
   router = APIRouter(prefix="/api/v1/baselines", tags=["baselines"])
   
   @router.post("/", response_model=BaselineResponse)
   async def create_baseline(request: BaselineCreateRequest):
       """ë² ì´ìŠ¤ë¼ì¸ ë“±ë¡"""
       pass
   
   @router.get("/", response_model=List[BaselineResponse])
   async def list_baselines(system_id: Optional[int] = None, dataset_id: Optional[str] = None):
       """ë² ì´ìŠ¤ë¼ì¸ ëª©ë¡ ì¡°íšŒ"""
       pass
   
   @router.get("/{baseline_id}/compare", response_model=BaselineComparison)
   async def compare_baseline(baseline_id: int, evaluation_result_id: int):
       """ë² ì´ìŠ¤ë¼ì¸ê³¼ í‰ê°€ ê²°ê³¼ ë¹„êµ"""
       pass
   ```

#### ì™„ë£Œ ì¡°ê±´
- [ ] ë² ì´ìŠ¤ë¼ì¸ ë“±ë¡ ë° ì¡°íšŒ ê¸°ëŠ¥ ë™ì‘
- [ ] ë² ì´ìŠ¤ë¼ì¸ ë¹„êµ ë¡œì§ ì •í™•ì„± ê²€ì¦
- [ ] API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸ í†µê³¼

---

### Task 2.4: í‰ê°€ ê²°ê³¼ í™•ì¥ ë° í†µê³„ ê¸°ëŠ¥
**ë‹´ë‹¹ì**: ê°œë°œì  
**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 3ì‹œê°„  
**ìš°ì„ ìˆœìœ„**: Medium  

#### ì‘ì—… ë‚´ìš©
- [ ] í‰ê°€ ê²°ê³¼ ìƒì„¸ í†µê³„ ì¶”ê°€
- [ ] ì‹¤í–‰ ë©”íƒ€ë°ì´í„° ì €ì¥
- [ ] ê²°ê³¼ í•„í„°ë§ ë° ì •ë ¬ ê¸°ëŠ¥

#### ì„¸ë¶€ ì‘ì—…
1. **í‰ê°€ ê²°ê³¼ ëª¨ë¸ í™•ì¥**
   ```python
   # app/features/evaluations/model.py (í™•ì¥)
   class EvaluationResult(Base):
       # ê¸°ì¡´ í•„ë“œë“¤...
       
       # ìƒˆë¡œ ì¶”ê°€ë˜ëŠ” í•„ë“œë“¤
       query_count = Column(Integer)  # í‰ê°€ëœ ì¿¼ë¦¬ ìˆ˜
       failed_queries = Column(Integer, default=0)  # ì‹¤íŒ¨í•œ ì¿¼ë¦¬ ìˆ˜
       average_response_time = Column(Float)  # í‰ê·  ì‘ë‹µ ì‹œê°„
       median_response_time = Column(Float)  # ì¤‘ì•™ê°’ ì‘ë‹µ ì‹œê°„
       std_response_time = Column(Float)  # ì‘ë‹µ ì‹œê°„ í‘œì¤€í¸ì°¨
       
       # ë©”íƒ€ë°ì´í„°
       environment_info = Column(JSON)  # ì‹¤í–‰ í™˜ê²½ ì •ë³´
       version = Column(String)  # í‰ê°€ ì‹œìŠ¤í…œ ë²„ì „
   ```

2. **í†µê³„ ê³„ì‚° ì„œë¹„ìŠ¤**
   ```python
   # app/features/evaluations/statistics.py
   class EvaluationStatistics:
       @staticmethod
       def calculate_query_level_stats(query_results: List[QueryResult]) -> Dict[str, float]:
           """ì¿¼ë¦¬ë³„ ê²°ê³¼ì˜ í†µê³„ ê³„ì‚°"""
           response_times = [result.response_time for result in query_results]
           
           return {
               'query_count': len(query_results),
               'failed_queries': sum(1 for r in query_results if r.failed),
               'average_response_time': np.mean(response_times),
               'median_response_time': np.median(response_times),
               'std_response_time': np.std(response_times),
               'min_response_time': np.min(response_times),
               'max_response_time': np.max(response_times)
           }
   ```

3. **ê²°ê³¼ í•„í„°ë§ API**
   ```python
   # app/features/evaluations/router.py (í™•ì¥)
   @router.get("/", response_model=List[EvaluationResponse])
   async def list_evaluations(
       system_id: Optional[int] = None,
       dataset_id: Optional[str] = None,
       date_from: Optional[datetime] = None,
       date_to: Optional[datetime] = None,
       sort_by: str = "created_at",
       sort_order: str = "desc",
       limit: int = 50,
       offset: int = 0
   ):
       """í‰ê°€ ê²°ê³¼ ëª©ë¡ (í•„í„°ë§, ì •ë ¬, í˜ì´ì§€ë„¤ì´ì…˜)"""
       pass
   ```

#### ì™„ë£Œ ì¡°ê±´
- [ ] ìƒì„¸ í†µê³„ ê³„ì‚° ë° ì €ì¥
- [ ] ê²°ê³¼ í•„í„°ë§ ê¸°ëŠ¥ ë™ì‘
- [ ] ì„±ëŠ¥ í†µê³„ ì •í™•ì„± ê²€ì¦

---

### Task 2.5: ë°ì´í„°ì…‹ í’ˆì§ˆ ê²€ì¦ ê¸°ëŠ¥
**ë‹´ë‹¹ì**: ê°œë°œì  
**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 2ì‹œê°„  
**ìš°ì„ ìˆœìœ„**: Medium  

#### ì‘ì—… ë‚´ìš©
- [ ] ë°ì´í„°ì…‹ ìœ íš¨ì„± ê²€ì¦
- [ ] í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„±
- [ ] ë¬¸ì œ ë°ì´í„° ì‹ë³„ ë° ë³´ê³ 

#### ì„¸ë¶€ ì‘ì—…
1. **ë°ì´í„°ì…‹ ê²€ì¦ê¸°**
   ```python
   # app/features/datasets/validator.py
   class DatasetValidator:
       def validate_dataset(self, dataset_path: str) -> ValidationReport:
           """ë°ì´í„°ì…‹ ì „ì²´ ê²€ì¦"""
           report = ValidationReport()
           
           # í•„ìˆ˜ íŒŒì¼ ì¡´ì¬ í™•ì¸
           report.file_checks = self._check_required_files(dataset_path)
           
           # ë°ì´í„° í˜•ì‹ ê²€ì¦
           report.format_checks = self._check_data_format(dataset_path)
           
           # ë°ì´í„° ì¼ê´€ì„± ê²€ì¦
           report.consistency_checks = self._check_data_consistency(dataset_path)
           
           return report
       
       def _check_required_files(self, dataset_path: str) -> Dict[str, bool]:
           """í•„ìˆ˜ íŒŒì¼ ì¡´ì¬ í™•ì¸"""
           required_files = ['queries.jsonl', 'ground_truth.jsonl', 'metadata.json']
           return {
               file: os.path.exists(os.path.join(dataset_path, file))
               for file in required_files
           }
       
       def _check_data_format(self, dataset_path: str) -> Dict[str, List[str]]:
           """ë°ì´í„° í˜•ì‹ ì˜¤ë¥˜ ê²€ì‚¬"""
           errors = []
           
           # queries.jsonl ê²€ì¦
           queries_errors = self._validate_queries_file(
               os.path.join(dataset_path, 'queries.jsonl')
           )
           errors.extend(queries_errors)
           
           # ground_truth.jsonl ê²€ì¦
           gt_errors = self._validate_ground_truth_file(
               os.path.join(dataset_path, 'ground_truth.jsonl')
           )
           errors.extend(gt_errors)
           
           return {'errors': errors}
   ```

2. **ë°ì´í„°ì…‹ í’ˆì§ˆ API**
   ```python
   # app/features/datasets/router.py (í™•ì¥)
   @router.get("/{dataset_id}/validate", response_model=ValidationReport)
   async def validate_dataset(dataset_id: str):
       """ë°ì´í„°ì…‹ í’ˆì§ˆ ê²€ì¦"""
       pass
   
   @router.get("/{dataset_id}/stats", response_model=DatasetStats)
   async def get_dataset_stats(dataset_id: str):
       """ë°ì´í„°ì…‹ í†µê³„ ì •ë³´"""
       pass
   ```

#### ì™„ë£Œ ì¡°ê±´
- [ ] ë°ì´í„°ì…‹ ê²€ì¦ ë¡œì§ êµ¬í˜„
- [ ] í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„± ê¸°ëŠ¥
- [ ] ë¬¸ì œ ë°ì´í„° ì‹ë³„ ì •í™•ì„±

## ğŸ¯ Phase 2 ì™„ë£Œ ì¡°ê±´

### ê¸°ëŠ¥ì  ìš”êµ¬ì‚¬í•­
- [ ] NDCG ë©”íŠ¸ë¦­ì´ ì •í™•í•˜ê²Œ ê³„ì‚°ë¨
- [ ] ë‹¤ì¤‘ ë°ì´í„°ì…‹ ë°°ì¹˜ í‰ê°€ ê°€ëŠ¥
- [ ] ë² ì´ìŠ¤ë¼ì¸ ë“±ë¡ ë° ë¹„êµ ê¸°ëŠ¥ ë™ì‘
- [ ] í‰ê°€ ê²°ê³¼ ìƒì„¸ í†µê³„ ì œê³µ
- [ ] ë°ì´í„°ì…‹ í’ˆì§ˆ ê²€ì¦ ê°€ëŠ¥

### ê¸°ìˆ ì  ìš”êµ¬ì‚¬í•­
- [ ] ëª¨ë“  ìƒˆ ê¸°ëŠ¥ì— ëŒ€í•œ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
- [ ] í†µí•© í…ŒìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
- [ ] API ë¬¸ì„œ ì—…ë°ì´íŠ¸
- [ ] ë² ì´ìŠ¤ë¼ì¸ ë¹„êµ ì •í™•ì„± ê²€ì¦

### ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­
- [ ] ë°°ì¹˜ í‰ê°€ ì‹œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
- [ ] NDCG ê³„ì‚° ì„±ëŠ¥ ìµœì í™”
- [ ] ë‹¤ì¤‘ ë°ì´í„°ì…‹ ë³‘ë ¬ ì²˜ë¦¬

## ğŸ“… ì˜ˆìƒ ì¼ì •
**ì´ ì†Œìš” ì‹œê°„**: 17ì‹œê°„ (ì•½ 2ì¼)

1. **Day 1**: Task 2.1, 2.2 (NDCG, ë‹¤ì¤‘ ë°ì´í„°ì…‹)
2. **Day 2**: Task 2.3, 2.4, 2.5 (ë² ì´ìŠ¤ë¼ì¸, í†µê³„, ê²€ì¦) 