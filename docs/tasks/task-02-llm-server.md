# Task 02: LLM Server êµ¬í˜„

## ğŸ¯ ëª©í‘œ
vLLM í˜¸í™˜ API ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•˜ë©´ì„œ ë‚´ë¶€ì ìœ¼ë¡œëŠ” OpenAI APIë¥¼ ì‚¬ìš©í•˜ëŠ” LLM ì„œë¹„ìŠ¤ë¥¼ TDD ë°©ì‹ìœ¼ë¡œ êµ¬í˜„í•©ë‹ˆë‹¤.

## ğŸ“‹ MVP ë²”ìœ„
- vLLM í˜¸í™˜ API ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„
- OpenAI API í”„ë¡ì‹œ ê¸°ëŠ¥
- gpt-4o-mini ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
- ì±„íŒ… ì™„ì„± ë° í…ìŠ¤íŠ¸ ìƒì„± API
- Docker ì»¨í…Œì´ë„ˆí™”

## ğŸ—ï¸ ê¸°ìˆ  ìŠ¤íƒ
- **ì›¹ í”„ë ˆì„ì›Œí¬**: FastAPI
- **LLM ì„œë¹„ìŠ¤**: OpenAI API (gpt-4o-mini)
- **ë¼ì´ë¸ŒëŸ¬ë¦¬**: openai, pydantic, httpx
- **ì»¨í…Œì´ë„ˆ**: Docker
- **í…ŒìŠ¤íŠ¸**: pytest, httpx, pytest-mock

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
llm-server/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â””â”€â”€ llm/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ router.py        â† vLLM í˜¸í™˜ API ì—”ë“œí¬ì¸íŠ¸
â”‚   â”‚       â”œâ”€â”€ service.py       â† LLM ì„œë¹„ìŠ¤ ë¡œì§
â”‚   â”‚       â”œâ”€â”€ schema.py        â† vLLM í˜¸í™˜ ìŠ¤í‚¤ë§ˆ
â”‚   â”‚       â””â”€â”€ client.py        â† OpenAI API í´ë¼ì´ì–¸íŠ¸
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py           â† ì„¤ì • ê´€ë¦¬
â”‚   â”‚   â””â”€â”€ dependencies.py     â† ì˜ì¡´ì„± ì£¼ì…
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ main.py                 â† FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â””â”€â”€ features/
â”‚   â”‚       â””â”€â”€ llm/
â”‚   â”‚           â”œâ”€â”€ test_router.py
â”‚   â”‚           â”œâ”€â”€ test_service.py
â”‚   â”‚           â”œâ”€â”€ test_schema.py
â”‚   â”‚           â””â”€â”€ test_client.py
â”‚   â”œâ”€â”€ integration/
â”‚   â”‚   â””â”€â”€ test_llm_api.py
â”‚   â””â”€â”€ conftest.py
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ .dockerignore
```

## ğŸ§ª TDD êµ¬í˜„ ìˆœì„œ

### 1ë‹¨ê³„: OpenAI í´ë¼ì´ì–¸íŠ¸ í´ë˜ìŠ¤ (TDD)

**ğŸ”´ í…ŒìŠ¤íŠ¸ ì‘ì„±**
```python
# tests/unit/features/llm/test_client.py
import pytest
from unittest.mock import Mock, patch, AsyncMock
from app.features.llm.client import OpenAIClient
from app.features.llm.schema import ChatCompletionRequest

@pytest.fixture
def mock_openai_client():
    mock_client = Mock()
    mock_client.chat = Mock()
    mock_client.chat.completions = Mock()
    mock_client.chat.completions.create = AsyncMock()
    return mock_client

@pytest.mark.asyncio
async def test_openai_client_should_create_chat_completion(mock_openai_client):
    """OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì±„íŒ… ì™„ì„±ì„ ìƒì„±í•´ì•¼ í•¨"""
    # Given
    client = OpenAIClient("test-api-key")
    client._client = mock_openai_client
    
    mock_response = Mock()
    mock_response.choices = [Mock()]
    mock_response.choices[0].message.content = "í…ŒìŠ¤íŠ¸ ì‘ë‹µ"
    mock_response.model = "gpt-4o-mini"
    mock_response.usage.total_tokens = 100
    mock_openai_client.chat.completions.create.return_value = mock_response
    
    request = ChatCompletionRequest(
        messages=[{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"}],
        model="gpt-4o-mini"
    )
    
    # When
    response = await client.create_chat_completion(request)
    
    # Then
    assert response.choices[0].message.content == "í…ŒìŠ¤íŠ¸ ì‘ë‹µ"
    assert response.model == "gpt-4o-mini"
    mock_openai_client.chat.completions.create.assert_called_once()

@pytest.mark.asyncio
async def test_openai_client_should_handle_api_error():
    """OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ API ì˜¤ë¥˜ë¥¼ ì²˜ë¦¬í•´ì•¼ í•¨"""
    # Given
    client = OpenAIClient("test-api-key")
    client._client = mock_openai_client
    
    mock_openai_client.chat.completions.create.side_effect = Exception("API ì˜¤ë¥˜")
    
    request = ChatCompletionRequest(
        messages=[{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"}],
        model="gpt-4o-mini"
    )
    
    # When & Then
    with pytest.raises(Exception) as exc_info:
        await client.create_chat_completion(request)
    
    assert "API ì˜¤ë¥˜" in str(exc_info.value)
```

**ğŸŸ¢ ìµœì†Œ êµ¬í˜„**
```python
# app/features/llm/client.py
import openai
from typing import Optional
from .schema import ChatCompletionRequest, ChatCompletionResponse

class OpenAIClient:
    def __init__(self, api_key: str, base_url: Optional[str] = None):
        self.api_key = api_key
        self._client = openai.AsyncOpenAI(
            api_key=api_key,
            base_url=base_url
        )
    
    async def create_chat_completion(self, request: ChatCompletionRequest) -> ChatCompletionResponse:
        """ì±„íŒ… ì™„ì„± ìƒì„±"""
        try:
            response = await self._client.chat.completions.create(
                model=request.model,
                messages=request.messages,
                temperature=request.temperature,
                max_tokens=request.max_tokens,
                stream=request.stream
            )
            
            return ChatCompletionResponse(
                id=response.id,
                object=response.object,
                created=response.created,
                model=response.model,
                choices=response.choices,
                usage=response.usage
            )
        except Exception as e:
            raise Exception(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
```

### 2ë‹¨ê³„: LLM ì„œë¹„ìŠ¤ í´ë˜ìŠ¤ (TDD)

**ğŸ”´ í…ŒìŠ¤íŠ¸ ì‘ì„±**
```python
# tests/unit/features/llm/test_service.py
import pytest
from unittest.mock import Mock, AsyncMock
from app.features.llm.service import LLMService
from app.features.llm.schema import ChatCompletionRequest, ChatCompletionResponse

@pytest.fixture
def mock_openai_client():
    mock_client = Mock()
    mock_client.create_chat_completion = AsyncMock()
    return mock_client

@pytest.mark.asyncio
async def test_llm_service_should_create_chat_completion(mock_openai_client):
    """LLM ì„œë¹„ìŠ¤ê°€ ì±„íŒ… ì™„ì„±ì„ ìƒì„±í•´ì•¼ í•¨"""
    # Given
    service = LLMService(mock_openai_client)
    mock_response = ChatCompletionResponse(
        id="test-id",
        object="chat.completion",
        created=1234567890,
        model="gpt-4o-mini",
        choices=[],
        usage=None
    )
    mock_openai_client.create_chat_completion.return_value = mock_response
    
    request = ChatCompletionRequest(
        messages=[{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"}],
        model="gpt-4o-mini"
    )
    
    # When
    response = await service.create_chat_completion(request)
    
    # Then
    assert response.model == "gpt-4o-mini"
    mock_openai_client.create_chat_completion.assert_called_once_with(request)

@pytest.mark.asyncio
async def test_llm_service_should_use_default_model_when_not_specified():
    """LLM ì„œë¹„ìŠ¤ê°€ ëª¨ë¸ì´ ì§€ì •ë˜ì§€ ì•Šì•˜ì„ ë•Œ ê¸°ë³¸ ëª¨ë¸ì„ ì‚¬ìš©í•´ì•¼ í•¨"""
    # Given
    service = LLMService(mock_openai_client)
    request = ChatCompletionRequest(
        messages=[{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"}]
    )
    
    # When
    await service.create_chat_completion(request)
    
    # Then
    called_request = mock_openai_client.create_chat_completion.call_args[0][0]
    assert called_request.model == "gpt-4o-mini"
```

**ğŸŸ¢ ìµœì†Œ êµ¬í˜„**
```python
# app/features/llm/service.py
from typing import Optional
from .client import OpenAIClient
from .schema import ChatCompletionRequest, ChatCompletionResponse

class LLMService:
    def __init__(self, openai_client: OpenAIClient):
        self.openai_client = openai_client
        self.default_model = "gpt-4o-mini"
    
    async def create_chat_completion(self, request: ChatCompletionRequest) -> ChatCompletionResponse:
        """ì±„íŒ… ì™„ì„± ìƒì„±"""
        # ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
        if not request.model:
            request.model = self.default_model
        
        return await self.openai_client.create_chat_completion(request)
    
    async def get_models(self) -> dict:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜ (vLLM í˜¸í™˜)"""
        return {
            "object": "list",
            "data": [
                {
                    "id": "gpt-4o-mini",
                    "object": "model",
                    "created": 1234567890,
                    "owned_by": "openai-proxy"
                }
            ]
        }
```

### 3ë‹¨ê³„: vLLM í˜¸í™˜ ìŠ¤í‚¤ë§ˆ ì •ì˜ (TDD)

**ğŸ”´ í…ŒìŠ¤íŠ¸ ì‘ì„±**
```python
# tests/unit/features/llm/test_schema.py
import pytest
from pydantic import ValidationError
from app.features.llm.schema import (
    ChatCompletionRequest, ChatCompletionResponse,
    ChatMessage, Choice, Usage
)

def test_chat_completion_request_should_validate_messages():
    """ì±„íŒ… ì™„ì„± ìš”ì²­ ìŠ¤í‚¤ë§ˆê°€ ë©”ì‹œì§€ë¥¼ ì •ìƒ ê²€ì¦í•´ì•¼ í•¨"""
    # Given & When
    request = ChatCompletionRequest(
        messages=[
            {"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"},
            {"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?"}
        ]
    )
    
    # Then
    assert len(request.messages) == 2
    assert request.messages[0]["role"] == "user"
    assert request.model == "gpt-4o-mini"  # ê¸°ë³¸ê°’

def test_chat_completion_request_should_reject_empty_messages():
    """ì±„íŒ… ì™„ì„± ìš”ì²­ ìŠ¤í‚¤ë§ˆê°€ ë¹ˆ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ê±°ë¶€í•´ì•¼ í•¨"""
    # Given & When & Then
    with pytest.raises(ValidationError):
        ChatCompletionRequest(messages=[])

def test_chat_completion_request_should_validate_temperature_range():
    """ì±„íŒ… ì™„ì„± ìš”ì²­ ìŠ¤í‚¤ë§ˆê°€ temperature ë²”ìœ„ë¥¼ ê²€ì¦í•´ì•¼ í•¨"""
    # Given & When & Then
    with pytest.raises(ValidationError):
        ChatCompletionRequest(
            messages=[{"role": "user", "content": "í…ŒìŠ¤íŠ¸"}],
            temperature=2.5  # ìœ íš¨ ë²”ìœ„ ì´ˆê³¼
        )

def test_chat_message_should_validate_role():
    """ì±„íŒ… ë©”ì‹œì§€ ìŠ¤í‚¤ë§ˆê°€ ì—­í• ì„ ì •ìƒ ê²€ì¦í•´ì•¼ í•¨"""
    # Given & When
    message = ChatMessage(role="user", content="í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€")
    
    # Then
    assert message.role == "user"
    assert message.content == "í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€"
```

**ğŸŸ¢ ìµœì†Œ êµ¬í˜„**
```python
# app/features/llm/schema.py
from pydantic import BaseModel, Field, validator
from typing import List, Optional, Union, Dict, Any
from enum import Enum

class MessageRole(str, Enum):
    USER = "user"
    ASSISTANT = "assistant"
    SYSTEM = "system"

class ChatMessage(BaseModel):
    role: MessageRole
    content: str

class ChatCompletionRequest(BaseModel):
    messages: List[Union[ChatMessage, Dict[str, str]]] = Field(..., min_items=1)
    model: str = Field(default="gpt-4o-mini")
    temperature: float = Field(default=0.7, ge=0.0, le=2.0)
    max_tokens: Optional[int] = Field(default=None, gt=0)
    stream: bool = Field(default=False)
    
    @validator('messages')
    def validate_messages(cls, v):
        if not v:
            raise ValueError('ë©”ì‹œì§€ëŠ” ìµœì†Œ 1ê°œ ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤')
        return v

class Usage(BaseModel):
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int

class Choice(BaseModel):
    index: int
    message: ChatMessage
    finish_reason: Optional[str] = None

class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[Choice]
    usage: Optional[Usage] = None

class ModelInfo(BaseModel):
    id: str
    object: str = "model"
    created: int
    owned_by: str

class ModelsResponse(BaseModel):
    object: str = "list"
    data: List[ModelInfo]
```

### 4ë‹¨ê³„: vLLM í˜¸í™˜ API ë¼ìš°í„° êµ¬í˜„ (TDD)

**ğŸ”´ í…ŒìŠ¤íŠ¸ ì‘ì„±**
```python
# tests/integration/test_llm_api.py
import pytest
from fastapi.testclient import TestClient
from unittest.mock import patch, AsyncMock
from app.main import app

client = TestClient(app)

@patch('app.features.llm.client.OpenAIClient')
def test_chat_completions_endpoint_should_return_completion(mock_openai_client):
    """ì±„íŒ… ì™„ì„± ì—”ë“œí¬ì¸íŠ¸ê°€ ì™„ì„±ì„ ë°˜í™˜í•´ì•¼ í•¨"""
    # Given
    mock_instance = AsyncMock()
    mock_openai_client.return_value = mock_instance
    
    mock_response = {
        "id": "test-id",
        "object": "chat.completion",
        "created": 1234567890,
        "model": "gpt-4o-mini",
        "choices": [
            {
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": "ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?"
                },
                "finish_reason": "stop"
            }
        ],
        "usage": {
            "prompt_tokens": 10,
            "completion_tokens": 20,
            "total_tokens": 30
        }
    }
    mock_instance.create_chat_completion.return_value = mock_response
    
    request_data = {
        "messages": [
            {"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"}
        ],
        "model": "gpt-4o-mini"
    }
    
    # When
    response = client.post("/v1/chat/completions", json=request_data)
    
    # Then
    assert response.status_code == 200
    data = response.json()
    assert data["model"] == "gpt-4o-mini"
    assert len(data["choices"]) == 1
    assert data["choices"][0]["message"]["content"] == "ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?"

def test_models_endpoint_should_return_available_models():
    """ëª¨ë¸ ëª©ë¡ ì—”ë“œí¬ì¸íŠ¸ê°€ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ ë°˜í™˜í•´ì•¼ í•¨"""
    # When
    response = client.get("/v1/models")
    
    # Then
    assert response.status_code == 200
    data = response.json()
    assert data["object"] == "list"
    assert len(data["data"]) >= 1
    assert data["data"][0]["id"] == "gpt-4o-mini"
```

**ğŸŸ¢ ìµœì†Œ êµ¬í˜„**
```python
# app/features/llm/router.py
from fastapi import APIRouter, Depends, HTTPException
from .service import LLMService
from .schema import (
    ChatCompletionRequest, ChatCompletionResponse,
    ModelsResponse
)
from app.core.dependencies import get_llm_service

router = APIRouter()

@router.post("/chat/completions", response_model=ChatCompletionResponse)
async def create_chat_completion(
    request: ChatCompletionRequest,
    service: LLMService = Depends(get_llm_service)
) -> ChatCompletionResponse:
    """ì±„íŒ… ì™„ì„± ìƒì„± (vLLM í˜¸í™˜)"""
    try:
        return await service.create_chat_completion(request)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/models", response_model=ModelsResponse)
async def list_models(
    service: LLMService = Depends(get_llm_service)
) -> ModelsResponse:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
    return await service.get_models()

@router.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {"status": "healthy", "service": "llm-server"}
```

## ğŸ³ Docker êµ¬ì„±

### Dockerfile
```dockerfile
FROM python:3.9-slim

WORKDIR /app

# ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ë³µì‚¬
COPY app/ app/

# í¬íŠ¸ ë…¸ì¶œ
EXPOSE 8002

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8002", "--reload"]
```

### requirements.txt
```txt
fastapi==0.104.1
uvicorn[standard]==0.24.0
openai==1.3.0
pydantic==2.5.0
httpx==0.25.2
pytest==7.4.3
pytest-asyncio==0.21.1
pytest-mock==3.12.0
```

## ğŸ”§ ì„¤ì • ê´€ë¦¬

### app/core/config.py
```python
from pydantic import BaseSettings
from typing import Optional

class Settings(BaseSettings):
    openai_api_key: str
    openai_base_url: Optional[str] = None
    default_model: str = "gpt-4o-mini"
    
    class Config:
        env_file = ".env"

settings = Settings()
```

### app/core/dependencies.py
```python
from functools import lru_cache
from .config import settings
from ..features.llm.client import OpenAIClient
from ..features.llm.service import LLMService

@lru_cache()
def get_openai_client() -> OpenAIClient:
    return OpenAIClient(
        api_key=settings.openai_api_key,
        base_url=settings.openai_base_url
    )

def get_llm_service() -> LLMService:
    return LLMService(get_openai_client())
```

## âœ… ì„±ê³µ ê¸°ì¤€

### ê¸°ëŠ¥ì  ìš”êµ¬ì‚¬í•­
- [ ] vLLM í˜¸í™˜ ì±„íŒ… ì™„ì„± API êµ¬í˜„
- [ ] ëª¨ë¸ ëª©ë¡ API êµ¬í˜„
- [ ] OpenAI API í”„ë¡ì‹œ ê¸°ëŠ¥ ë™ì‘
- [ ] gpt-4o-mini ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
- [ ] ì ì ˆí•œ ì—ëŸ¬ ì²˜ë¦¬ ë° ì‘ë‹µ

### ë¹„ê¸°ëŠ¥ì  ìš”êµ¬ì‚¬í•­
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ 90% ì´ìƒ
- [ ] í†µí•© í…ŒìŠ¤íŠ¸ í†µê³¼
- [ ] Docker ì»¨í…Œì´ë„ˆ ì •ìƒ ì‹¤í–‰
- [ ] API ì‘ë‹µ ì‹œê°„ < 10ì´ˆ (ì¼ë°˜ ìš”ì²­)

### vLLM í˜¸í™˜ì„±
- [ ] vLLM API ìŠ¤í‚¤ë§ˆì™€ í˜¸í™˜
- [ ] í‘œì¤€ ì—”ë“œí¬ì¸íŠ¸ ê²½ë¡œ ì‚¬ìš© (/v1/chat/completions)
- [ ] í‘œì¤€ ì‘ë‹µ í˜•ì‹ ì¤€ìˆ˜

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### ë¡œì»¬ ê°œë°œ
```bash
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export OPENAI_API_KEY="your-api-key-here"

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytest tests/ -v --cov=app

# ì„œë²„ ì‹¤í–‰
uvicorn app.main:app --reload --port 8002
```

### Docker ì‹¤í–‰
```bash
# ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t llm-server .

# ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run -p 8002:8002 -v $(pwd):/app \
  -e OPENAI_API_KEY="your-api-key-here" \
  llm-server
```

## ğŸ“š API ë¬¸ì„œ
ì„œë²„ ì‹¤í–‰ í›„ http://localhost:8002/docs ì—ì„œ Swagger UI í™•ì¸

## ğŸ”— vLLM í˜¸í™˜ì„± í™•ì¸
```bash
# vLLM í´ë¼ì´ì–¸íŠ¸ë¡œ í…ŒìŠ¤íŠ¸
curl -X POST "http://localhost:8002/v1/chat/completions" \
     -H "Content-Type: application/json" \
     -d '{
       "model": "gpt-4o-mini",
       "messages": [{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"}]
     }'
```

## ğŸ§ª Integration í…ŒìŠ¤íŠ¸ ë‹¨ê³„

### LLM ì„œë¹„ìŠ¤ í†µí•© í…ŒìŠ¤íŠ¸

**ëª©í‘œ**: LLM ì„œë¹„ìŠ¤ì˜ OpenAI API í˜¸í™˜ì„± ë° ì‹¤ì œ í™˜ê²½ ë™ì‘ ê²€ì¦

```python
# tests/integration/test_llm_integration.py
import pytest
import httpx
import asyncio
import time

@pytest.mark.asyncio
class TestLLMIntegration:
    """LLM ì„œë¹„ìŠ¤ í†µí•© í…ŒìŠ¤íŠ¸"""
    
    @pytest.fixture(autouse=True)
    async def setup(self):
        """Docker Compose í™˜ê²½ì—ì„œ LLM ì„œë¹„ìŠ¤ ì‹œì‘ í™•ì¸"""
        async with httpx.AsyncClient() as client:
            # ì„œë¹„ìŠ¤ ì¤€ë¹„ ëŒ€ê¸° (ìµœëŒ€ 120ì´ˆ - LLM ëª¨ë¸ ë¡œë”© ì‹œê°„ ê³ ë ¤)
            for _ in range(120):
                try:
                    response = await client.get("http://localhost:8002/health")
                    if response.status_code == 200:
                        break
                    await asyncio.sleep(1)
                except:
                    await asyncio.sleep(1)
            else:
                pytest.fail("LLM ì„œë¹„ìŠ¤ê°€ ì‹œì‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    async def test_openai_api_compatibility(self):
        """OpenAI API í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸"""
        async with httpx.AsyncClient() as client:
            payload = {
                "model": "gpt-4o-mini",
                "messages": [
                    {"role": "user", "content": "Hello, how are you?"}
                ],
                "max_tokens": 100,
                "temperature": 0.7
            }
            
            response = await client.post(
                "http://localhost:8002/v1/chat/completions",
                json=payload,
                timeout=60.0
            )
            
            assert response.status_code == 200
            result = response.json()
            
            # OpenAI API í˜•ì‹ ê²€ì¦
            assert "choices" in result
            assert "message" in result["choices"][0]
            assert "content" in result["choices"][0]["message"]
            assert "usage" in result
            
            print(f"âœ… OpenAI API í˜¸í™˜ì„± ê²€ì¦ ì™„ë£Œ")
            print(f"ì‘ë‹µ: {result['choices'][0]['message']['content'][:50]}...")
    
    async def test_streaming_response(self):
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í…ŒìŠ¤íŠ¸"""
        async with httpx.AsyncClient() as client:
            payload = {
                "model": "gpt-4o-mini",
                "messages": [
                    {"role": "user", "content": "Count from 1 to 5"}
                ],
                "max_tokens": 50,
                "stream": True
            }
            
            chunks_received = 0
            async with client.stream(
                "POST",
                "http://localhost:8002/v1/chat/completions",
                json=payload,
                timeout=60.0
            ) as response:
                assert response.status_code == 200
                
                async for chunk in response.aiter_text():
                    if chunk.strip():
                        chunks_received += 1
                        if chunks_received >= 5:  # ì¶©ë¶„í•œ ì²­í¬ ìˆ˜ì‹  í™•ì¸
                            break
            
            assert chunks_received >= 5
            print(f"âœ… ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {chunks_received}ê°œ ì²­í¬ ìˆ˜ì‹ ")
    
    async def test_concurrent_llm_requests(self):
        """ë™ì‹œ LLM ìš”ì²­ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        async with httpx.AsyncClient() as client:
            tasks = []
            for i in range(5):  # LLMì€ ì²˜ë¦¬ ì‹œê°„ì´ ê¸¸ì–´ 5ê°œë§Œ í…ŒìŠ¤íŠ¸
                payload = {
                    "model": "gpt-4o-mini",
                    "messages": [
                        {"role": "user", "content": f"Task {i}: Say hello"}
                    ],
                    "max_tokens": 20
                }
                task = client.post(
                    "http://localhost:8002/v1/chat/completions",
                    json=payload,
                    timeout=120.0
                )
                tasks.append(task)
            
            responses = await asyncio.gather(*tasks, return_exceptions=True)
            
            success_count = 0
            for response in responses:
                if not isinstance(response, Exception) and response.status_code == 200:
                    success_count += 1
            
            assert success_count >= 3, f"ë™ì‹œ ìš”ì²­ ì‹¤íŒ¨: {success_count}/5"
            print(f"âœ… ë™ì‹œ ìš”ì²­ ì„±ê³µ: {success_count}/5")
    
    async def test_error_handling(self):
        """ì—ëŸ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        async with httpx.AsyncClient() as client:
            # ì˜ëª»ëœ ëª¨ë¸ëª…
            payload = {
                "model": "invalid-model",
                "messages": [{"role": "user", "content": "test"}]
            }
            
            response = await client.post(
                "http://localhost:8002/v1/chat/completions",
                json=payload,
                timeout=30.0
            )
            
            assert response.status_code == 400
            error_result = response.json()
            assert "error" in error_result
            print(f"âœ… ì—ëŸ¬ ì²˜ë¦¬ ê²€ì¦ ì™„ë£Œ")
    
    async def test_response_time_performance(self):
        """ì‘ë‹µ ì‹œê°„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        response_times = []
        
        async with httpx.AsyncClient() as client:
            for i in range(3):  # 3íšŒ ì¸¡ì •
                start_time = time.time()
                
                payload = {
                    "model": "gpt-4o-mini",
                    "messages": [
                        {"role": "user", "content": "Write a short hello message"}
                    ],
                    "max_tokens": 30
                }
                
                response = await client.post(
                    "http://localhost:8002/v1/chat/completions",
                    json=payload,
                    timeout=120.0
                )
                
                response_time = time.time() - start_time
                response_times.append(response_time)
                
                assert response.status_code == 200
        
        avg_response_time = sum(response_times) / len(response_times)
        assert avg_response_time < 30.0  # í‰ê·  ì‘ë‹µì‹œê°„ 30ì´ˆ ì´ë‚´
        
        print(f"âœ… ì‘ë‹µ ì‹œê°„ ì„±ëŠ¥: í‰ê·  {avg_response_time:.2f}ì´ˆ")
        print(f"ê°œë³„ ì¸¡ì •: {[f'{t:.2f}s' for t in response_times]}")
```

**ì‹¤í–‰ ë°©ë²•**:
```bash
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export OPENAI_API_KEY="your-api-key-here"

# Docker Compose í™˜ê²½ ì‹œì‘
docker-compose up -d llm-server

# Integration í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytest tests/integration/test_llm_integration.py -v

# ì„±ê³µ ê¸°ì¤€
# - OpenAI API í˜¸í™˜ì„± ê²€ì¦ í†µê³¼
# - ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì •ìƒ ë™ì‘
# - ë™ì‹œ ìš”ì²­ ì„±ê³µë¥  > 60%
# - í‰ê·  ì‘ë‹µ ì‹œê°„ < 30ì´ˆ
# - ì—ëŸ¬ ì²˜ë¦¬ ì ì ˆ
```

## ğŸ”„ ë‹¤ìŒ ë‹¨ê³„
Task 03: Vector DB êµ¬ì„± 